{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hello! \ud83d\udc4b \u00b6 I build AI that helps someone somewhere everyday.","title":"Hello! \ud83d\udc4b"},{"location":"#hello","text":"I build AI that helps someone somewhere everyday.","title":"Hello! \ud83d\udc4b"},{"location":"contact/","text":"I am, Somewhere out there. Some wonderful place on our planet earth. Wandering and Wondering. You can write to me at, hetulvp@gmail.com Or connect via the digital universe. GitHub LinkedIn Instagram","title":"Contact"},{"location":"education/","text":"Nirma University Aug, 2014 - May, 2018 Institute of technology, Nirma University, Ahmedabad, India. Bachelor of Technology in Computer Science and Engineering CGPA: 8.12 / 10.0 Gujarat Secondary and Higher Secondary Education Board April, 2014 Utkarsh vidyalaya, Vadodara, India. Higher Secondary School Certificate Percentile: 99.87 Gujarat Secondary and Higher Secondary Education Board March, 2012 Bright School, Vadodara, India. Secondary School Certificate Percentile: 99.98","title":"Education"},{"location":"experience/","text":"Machine Learning Engineer - II Feb, 2021 - Present InFoCusp Innovations Private Limited , Ahmedabad, India. Client: Google-X, the Moonshot Factory (Confidential Research Project) Neural Machine Translation Computer vision The goal was to create a single virtualized view of the electricity system through an aerial view imaginary using an image segmentation model. I explored various state-of-the-art image segmentation models such as PSP-NET, ViT, etc., and combined the best of all in a single model to achieve production-grade results. I also developed a novel metric for the evaluation of the model's performance as traditional segmentation metrics failed to provide helpful indications of failure cases. While the new metric helped filter failure cases easily which was then improved through data augmentation techniques. GRAD-CAM and its variant were used to locate features in the image when the model failed to produce the correct output. This analysis helped improve the quality of the training and evaluation dataset. Client: Innovyze, An Autodesk company. At Innovyze, I helped Process Engineers to optimize chemical consumption in the water treatment plant by analyzing historical sensor data and developing predictive models to automate the processes. My primary tasks were Data analysis, Feature engineering, and Modelling. Software Development Engineer - II (AI/ML) May, 2018 - Feb, 2021 Matrix Comsec R&D , Vadodara, India. Matrix ComSec is a leader in Security and Telecom solutions for modern businesses and enterprises with 1M+ customers in 50+ countries. I was responsible for developing and delivering DL algorithms in SDK form. My major contributions are listed below, Face Recognition (FR) and Face Detection (FD) Developed FD algorithm to detect multiple faces in the image with a minimum face size of 30px. Developed FR algorithm with 99.85 % accuracy on the LFW benchmark. Improved existing FR algorithm to identify people wearing a mask on the face with only 5% accuracy drop wrt to full-face model. It helped employees to mark attendance without removing their masks during the coronavirus pandemic Developed a Face Mask Detection model to identify whether employees are wearing masks or not when marking attendance to improve their safety. Developed CNN for a single RGB image-based Passive Face Antispoofing model to prevent fake attendance marking using a mobile phone or printed photo. Delivered all these features in a single FR SDK for RPi3/4, CPU, GPU, and Android devices with a maximum latency of 800 ms on the slowest hardware Automated License Plate Recognition (ALPR) Developed a real-time Licence Plate Detection model which achieved 99%+ recall and 93%+ precision on the internal Indian vehicle test dataset Contributed to the development of the CNN model for Licence Plate Recognition using CTC Loss. We achieved 87% accuracy on challenging the Indian test datasets outperforming other commercial solutions by at least 15% and 98% accuracy on the OpenALPR benchmark Developed semi-supervised data annotation tool which helped us collect a labeled dataset of 30K+ images and incrementally improved the model's accuracy Designed ALPR SDK architecture for improving throughput by batching simultaneous requests into single inference. A myriad of inference engines such as TensorFlow, TensorRT, OpenVINO, TensorflowLite, etc. can be integrated based on hardware without modifying 95% of the codebase. Seven Segment Display Number Recognition To handle a variety of color digital displays, a self-calibration algorithm was designed to run once to extract display-specific properties using KNN and an inference algorithm was designed to use these properties for recognizing digits in real-time using SVM. The algorithm was integrated into existing weighbridge vehicle management software for automating data entry of the vehicle's weight from the digital display through the camera. It reduced manual human intervention by 95% and introduced transparency in the whole process. Deep Learning Research Intern Jan, 2018 - May, 2018 Space Applications Centre (SAC) - ISRO, Ahmedabad, Gujarat, India SAC being the major R&D center of ISRO designs and develops the optical and microwave sensors for the satellites, signal and image processing software, GIS software, and many applications for the Earth Observation (EO) program of ISRO. As a Research Intern, I developed DL techniques for accurate crop classification using Hyper Spectral Satellite images (425 channels per pixel). My primary contributions were as listed below Developed ANN-based dimensionality reduction technique which retained useful features and worked better than PCA. Developed FR algorithm with 99.85 % accuracy on the LFW benchmark. Developed virtual data augmentation technique for overcoming the issue of insufficient ground truth data of different crops for training. Provided detailed comparative analysis of ANN, CNN, and SVM. Designed highly accurate Parallel-CNNs architecture for classifying nearly inseparable classes based on interclass separability analysis. Developed Python library based on my work so that other scientists can use these techniques on a variety of other satellite images. Machine Learning Summer Intern May, 2017 - June, 2017 Wolfsoft Pvt. Ltd., Vadodara, Gujarat, India. During my internship I worked on a food review app similar to Zomato and my contributions were as listed below, Developed a Food Recommendation API based on a Collaborative Filtering algorithm. Developed a Food Search API. Normalized and stored posting list of Indian food names in the Trie data structure for faster and more accurate search. Added spelling correction using the Levenshtein distance algorithm which achieved higher recall compared to SQL's search.","title":"Experience"},{"location":"projects/clothes_retrieval/","text":"Clothes Retrieval \u00b6 Searching for exact cloth image accurately from massive collections of clothes' images based on a query image. For example, here the left most image is the query image and other images are matched images retrieved from a huge collection of clothes using a computer vision algorithm. Dataset Details \u00b6 Dataset used : Deep Fashion 2 Problem statement : Consumer-to-shop Clothes Retrieval Problem description : Matching consumer-taken photos with their shop counterparts Code \u00b6 Google Colab Building the intuition \u00b6 Sorted in the order of how I reached the solution 1. Usecase understanding \u00b6 Consumer can upload a photo of clothing item System should retrieve similar looking items from shopping catalog. 2. Dataset understanding \u00b6 In the given task, we have two different sources of data . Consumer captured images : These are low quality images captured using phone's camera (front or back). Images have variation in lighting, orientation, occlusion, filters etc. Shop captured images : Shop images include good quality photo shoot quality images. It also includes images from online shopping carts where background is removed etc. 3. Real world challenges \u00b6 Labeling the dataset is major challenge Shopping image collection including online and offline stores can be huge Thousands of unique clothing items Manually pairing each user captured image to a similar shop image costs lots of human effort When the product is new, we might not have huge collection of user captured images. In that case the system cannot work better for new styles Solving the problem \u00b6 Considering the scarcity of labeled consumer to shop pairs, I decided build two stage pipeline . First learning task agnostic attributes from raw data then use them to reduce consumer to shop domain gap using fewer labeled images. Step 1 : Learning directly from raw images without labeling \u00b6 Observations Humans are good at pattern matching . We recognize many things from their attributes . For e.g we recognize a vehicle with wheels, seats, glasses, horn etc. We also use this attributes to distinguish between two different vehicles We learn about these attributes by comparing between many instances unconsciously Inspirations Deep learning model can also learn similar attributes by just observing across different images In many image retrieval problems we consider output of pre-fc layer as embedding We use metric learning approaches such as triplet loss to lean discriminative embedding We can consider each dimension in embedding vector as one attribute This attributes make the embedding vector discriminative Difference from metric learning Metric learning approaches try to increase distance between embeddings as a whole . For that we need positive and negative pairs . Instead we can increase distance between attributes of embedding without labels . Here, We do not try to teach which attribute represents what ? We try to teach that no two attributes should represent same concept . Mathematical Formulation We often calculate similarity of two vectors using cosine similarity measure. Cosine similarity has an interpretation as the cosine of the angle between the two vectors Cosine similarity is not invariant to shifts . If x was shifted to x+1, the cosine similarity would change. Unlike the cosine, the correlation is invariant to both scale and location changes of x and y. Correlation is the cosine similarity between centered versions of x and y In a batch of images, attribute vector represents values of particular dimension across all images For e.g if batch size = 512 and embedding dim = 2048, then we have 2048 attribute vectors each of length 512 Now consider another batch of slightly different version of the same images in first batch. Our goal is to Maximize cosine similarity of same attributes in both batches Minimize cosine similarity of different attributes in both batches Here, I used correlation as a proxy loss as both have same output range [-1,1] I divided loss function in two parts Correlation of attribute vectors at same index should be 1 . To make model generalize better I decreased the predicted correlation by margin m. For e.g if correlation is 0.9 we make it 0.6 so model tries to make it higher. Correlation of attribute vectors at different index should be 0 . Since there are N^2-N pairs of different attribute vectors I assigned lower weight compared to positive attribute pair. Code for attribute loss def attribute_loss(emb_1, emb_2, alpha=5e-4, margin=0.3): # per feature-dimension normalisation standardize = lambda x : (x - tf.reduce_mean(x, axis=0)) / tf.math.reduce_std(x, axis=0) emb_1_std = standardize(emb_1) # BxE emb_2_std = standardize(emb_2) # BxE # similarity of pairwise feature-dimension bsize = tf.cast(tf.shape(emb_1)[0], tf.float32) cos_t = tf.matmul(emb_1_std, emb_2_std, transpose_a=True) / bsize # Embed x Embed acos_t = tf.acos(tf.clip_by_value(cos_t, -1.0, 1.0)) # diagonal values rep. expected similarity of same feature-dim same_dim_mask = tf.eye(emb_1.shape[1]) # Embed x Embed # non-diagonal values rep. expected similarity of different features-dim diff_dim_mask = 1.0 - same_dim_mask # Embed x Embed # increase angle betweem same feature-dims : Cos(angle + m)(i==j) cosine_same = tf.cos(acos_t + margin) same_dim_loss = tf.square(same_dim_mask - cosine_same) * same_dim_mask # decrease angle between different feature-dims : Cos(angle - m)(i!=j) # cosine_diff = tf.cos(acos_t - margin) diff_dim_loss = tf.square(same_dim_mask - cos_t) * diff_dim_mask # final weighted loss weighted_loss = tf.reduce_sum(same_dim_loss + diff_dim_loss * alpha) return weighted_loss Step 2 : Reducing the domain gap using pinch of supervision \u00b6 Model trained with attribute loss can learn diverse set of attributes for each image. Although consumer and shop images have inherent biases due to different source of data generation This is very well known as domain gap where two similar objects can perceived differently by model due to pixel level differences I used combination of attribute loss and instance loss to reduce this domain gap with only 3 more epochs for finetuning 1. Attribute loss : In stage one, attribute loss was calculated between two different versions of same image as we did not have any other information Here we calculate it between attributes of same cloth type but one image from consumer and other from shop using label information 2. Instance loss : Attribute loss encourages model to learn features which are consistent in consumer and shop domain Instance loss is normal classification loss which uses these attributes to group similar cloths in tight clusters and increase distance between other groups. I used arcface as an instance loss. 3. Using weights from attribute model to Initialize backbone Initialize weights of newly added classification layer . Arcface tries to reduce cosine similarity of weights vector and instance vector. I used average of all embeddings produced by unsupervised model for each class as the initial weight vector of that class. Code for classifier weights calculation # Get classifier weights def classifier_weights(dataset, model, num_class): # calculate avg embedding for each class as w_init for fc out_layer = params.model.class_layer weights = np.zeros(shape=(num_class, model.outputs[out_layer].shape[-1]), dtype=np.float32) # (n_class, emb_dim) # (n_class,) extra 1 for ignoring zero div class_cnt = np.ones(shape=(num_class,), dtype=np.float32) for data in tqdm(dataset): user, shop, class_id = data all_images = tf.concat([user, shop], axis=0) class_ids = tf.concat([class_id, class_id], axis=0) embeds = model(all_images, training=False)[out_layer].numpy() weights[class_ids] += embeds class_cnt[class_ids] += 1 weights = np.divide(weights, class_cnt[:, np.newaxis]) return weights Experiments \u00b6 Model details \u00b6 Model Resnet50 Batch Size 512 Input resolution 96 Embedding dimension 2048 For training details refer code notebook Results \u00b6 Final Comments \u00b6 Although unsupervised loss has lower accuracy when source domains are different , it performs significantly better in mix domain (Exp 1 Last Block). Which suggests that attribute loss can work very well in task agnostic manner. Both attribute loss and fc7 weights initialization using avg class embedding improves results Supervised training from scratch performed significantly worse when training from scratch for 3 epoch Demo on validation dataset \u00b6 No image was used during training Results are in descending order of recall First column is consumer query image Other columns are retrieved shop images Green box is True Positive , Red box is False positive Model : Unsupervised Learning + Attribute Loss (Exp 1) \u00b6 Model : Attribute Loss + Instance Loss + fc7 weights init (Exp-2) \u00b6","title":"Clothes Retrieval"},{"location":"projects/clothes_retrieval/#clothes-retrieval","text":"Searching for exact cloth image accurately from massive collections of clothes' images based on a query image. For example, here the left most image is the query image and other images are matched images retrieved from a huge collection of clothes using a computer vision algorithm.","title":"Clothes Retrieval"},{"location":"projects/clothes_retrieval/#dataset-details","text":"Dataset used : Deep Fashion 2 Problem statement : Consumer-to-shop Clothes Retrieval Problem description : Matching consumer-taken photos with their shop counterparts","title":"Dataset Details"},{"location":"projects/clothes_retrieval/#code","text":"Google Colab","title":"Code"},{"location":"projects/clothes_retrieval/#building-the-intuition","text":"Sorted in the order of how I reached the solution","title":"Building the intuition"},{"location":"projects/clothes_retrieval/#1-usecase-understanding","text":"Consumer can upload a photo of clothing item System should retrieve similar looking items from shopping catalog.","title":"1. Usecase understanding"},{"location":"projects/clothes_retrieval/#2-dataset-understanding","text":"In the given task, we have two different sources of data . Consumer captured images : These are low quality images captured using phone's camera (front or back). Images have variation in lighting, orientation, occlusion, filters etc. Shop captured images : Shop images include good quality photo shoot quality images. It also includes images from online shopping carts where background is removed etc.","title":"2. Dataset understanding"},{"location":"projects/clothes_retrieval/#3-real-world-challenges","text":"Labeling the dataset is major challenge Shopping image collection including online and offline stores can be huge Thousands of unique clothing items Manually pairing each user captured image to a similar shop image costs lots of human effort When the product is new, we might not have huge collection of user captured images. In that case the system cannot work better for new styles","title":"3. Real world challenges"},{"location":"projects/clothes_retrieval/#solving-the-problem","text":"Considering the scarcity of labeled consumer to shop pairs, I decided build two stage pipeline . First learning task agnostic attributes from raw data then use them to reduce consumer to shop domain gap using fewer labeled images.","title":"Solving the problem"},{"location":"projects/clothes_retrieval/#step-1-learning-directly-from-raw-images-without-labeling","text":"Observations Humans are good at pattern matching . We recognize many things from their attributes . For e.g we recognize a vehicle with wheels, seats, glasses, horn etc. We also use this attributes to distinguish between two different vehicles We learn about these attributes by comparing between many instances unconsciously Inspirations Deep learning model can also learn similar attributes by just observing across different images In many image retrieval problems we consider output of pre-fc layer as embedding We use metric learning approaches such as triplet loss to lean discriminative embedding We can consider each dimension in embedding vector as one attribute This attributes make the embedding vector discriminative Difference from metric learning Metric learning approaches try to increase distance between embeddings as a whole . For that we need positive and negative pairs . Instead we can increase distance between attributes of embedding without labels . Here, We do not try to teach which attribute represents what ? We try to teach that no two attributes should represent same concept . Mathematical Formulation We often calculate similarity of two vectors using cosine similarity measure. Cosine similarity has an interpretation as the cosine of the angle between the two vectors Cosine similarity is not invariant to shifts . If x was shifted to x+1, the cosine similarity would change. Unlike the cosine, the correlation is invariant to both scale and location changes of x and y. Correlation is the cosine similarity between centered versions of x and y In a batch of images, attribute vector represents values of particular dimension across all images For e.g if batch size = 512 and embedding dim = 2048, then we have 2048 attribute vectors each of length 512 Now consider another batch of slightly different version of the same images in first batch. Our goal is to Maximize cosine similarity of same attributes in both batches Minimize cosine similarity of different attributes in both batches Here, I used correlation as a proxy loss as both have same output range [-1,1] I divided loss function in two parts Correlation of attribute vectors at same index should be 1 . To make model generalize better I decreased the predicted correlation by margin m. For e.g if correlation is 0.9 we make it 0.6 so model tries to make it higher. Correlation of attribute vectors at different index should be 0 . Since there are N^2-N pairs of different attribute vectors I assigned lower weight compared to positive attribute pair. Code for attribute loss def attribute_loss(emb_1, emb_2, alpha=5e-4, margin=0.3): # per feature-dimension normalisation standardize = lambda x : (x - tf.reduce_mean(x, axis=0)) / tf.math.reduce_std(x, axis=0) emb_1_std = standardize(emb_1) # BxE emb_2_std = standardize(emb_2) # BxE # similarity of pairwise feature-dimension bsize = tf.cast(tf.shape(emb_1)[0], tf.float32) cos_t = tf.matmul(emb_1_std, emb_2_std, transpose_a=True) / bsize # Embed x Embed acos_t = tf.acos(tf.clip_by_value(cos_t, -1.0, 1.0)) # diagonal values rep. expected similarity of same feature-dim same_dim_mask = tf.eye(emb_1.shape[1]) # Embed x Embed # non-diagonal values rep. expected similarity of different features-dim diff_dim_mask = 1.0 - same_dim_mask # Embed x Embed # increase angle betweem same feature-dims : Cos(angle + m)(i==j) cosine_same = tf.cos(acos_t + margin) same_dim_loss = tf.square(same_dim_mask - cosine_same) * same_dim_mask # decrease angle between different feature-dims : Cos(angle - m)(i!=j) # cosine_diff = tf.cos(acos_t - margin) diff_dim_loss = tf.square(same_dim_mask - cos_t) * diff_dim_mask # final weighted loss weighted_loss = tf.reduce_sum(same_dim_loss + diff_dim_loss * alpha) return weighted_loss","title":"Step 1 : Learning directly from raw images without labeling"},{"location":"projects/clothes_retrieval/#step-2-reducing-the-domain-gap-using-pinch-of-supervision","text":"Model trained with attribute loss can learn diverse set of attributes for each image. Although consumer and shop images have inherent biases due to different source of data generation This is very well known as domain gap where two similar objects can perceived differently by model due to pixel level differences I used combination of attribute loss and instance loss to reduce this domain gap with only 3 more epochs for finetuning 1. Attribute loss : In stage one, attribute loss was calculated between two different versions of same image as we did not have any other information Here we calculate it between attributes of same cloth type but one image from consumer and other from shop using label information 2. Instance loss : Attribute loss encourages model to learn features which are consistent in consumer and shop domain Instance loss is normal classification loss which uses these attributes to group similar cloths in tight clusters and increase distance between other groups. I used arcface as an instance loss. 3. Using weights from attribute model to Initialize backbone Initialize weights of newly added classification layer . Arcface tries to reduce cosine similarity of weights vector and instance vector. I used average of all embeddings produced by unsupervised model for each class as the initial weight vector of that class. Code for classifier weights calculation # Get classifier weights def classifier_weights(dataset, model, num_class): # calculate avg embedding for each class as w_init for fc out_layer = params.model.class_layer weights = np.zeros(shape=(num_class, model.outputs[out_layer].shape[-1]), dtype=np.float32) # (n_class, emb_dim) # (n_class,) extra 1 for ignoring zero div class_cnt = np.ones(shape=(num_class,), dtype=np.float32) for data in tqdm(dataset): user, shop, class_id = data all_images = tf.concat([user, shop], axis=0) class_ids = tf.concat([class_id, class_id], axis=0) embeds = model(all_images, training=False)[out_layer].numpy() weights[class_ids] += embeds class_cnt[class_ids] += 1 weights = np.divide(weights, class_cnt[:, np.newaxis]) return weights","title":"Step 2 : Reducing the domain gap using pinch of supervision"},{"location":"projects/clothes_retrieval/#experiments","text":"","title":"Experiments"},{"location":"projects/clothes_retrieval/#model-details","text":"Model Resnet50 Batch Size 512 Input resolution 96 Embedding dimension 2048 For training details refer code notebook","title":"Model details"},{"location":"projects/clothes_retrieval/#results","text":"","title":"Results"},{"location":"projects/clothes_retrieval/#final-comments","text":"Although unsupervised loss has lower accuracy when source domains are different , it performs significantly better in mix domain (Exp 1 Last Block). Which suggests that attribute loss can work very well in task agnostic manner. Both attribute loss and fc7 weights initialization using avg class embedding improves results Supervised training from scratch performed significantly worse when training from scratch for 3 epoch","title":"Final Comments"},{"location":"projects/clothes_retrieval/#demo-on-validation-dataset","text":"No image was used during training Results are in descending order of recall First column is consumer query image Other columns are retrieved shop images Green box is True Positive , Red box is False positive","title":"Demo on validation dataset"},{"location":"projects/clothes_retrieval/#model-unsupervised-learning-attribute-loss-exp-1","text":"","title":"Model :  Unsupervised Learning + Attribute Loss (Exp 1)"},{"location":"projects/clothes_retrieval/#model-attribute-loss-instance-loss-fc7-weights-init-exp-2","text":"","title":"Model :  Attribute Loss + Instance Loss + fc7 weights init (Exp-2)"}]}