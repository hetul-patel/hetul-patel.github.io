{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello! \ud83d\udc4b","text":"I build      AI  that helps someone somewhere everyday."},{"location":"contact/","title":"Contact","text":"<p>I am, Somewhere out there. Some wonderful place on our planet earth. Wandering and Wondering.</p> <p>You can write to me at, hetulvp@gmail.com</p> <p>Or connect via the digital universe.</p> <ul> <li>GitHub</li> <li>LinkedIn</li> <li>Instagram</li> </ul>"},{"location":"education/","title":"Education","text":"Nirma University               Aug, 2014 - May, 2018               Institute of technology, Nirma University, Ahmedabad, India.      <p>     Bachelor of Technology in Computer Science and Engineering      CGPA: 8.12 / 10.0  </p>          Gujarat Secondary and Higher Secondary Education Board               April, 2014               Utkarsh vidyalaya, Vadodara, India.      <p>     Higher Secondary School Certificate      Percentile: 99.87  </p>          Gujarat Secondary and Higher Secondary Education Board               March, 2012               Bright School, Vadodara, India.      <p>     Secondary School Certificate      Percentile: 99.98  </p>"},{"location":"experience/","title":"Experience","text":"Machine Learning Engineer - III               Feb, 2021 - Present      InFoCusp Innovations Private Limited, Ahmedabad, India.      <p>     Client: (Confidential Research Project)  </p> <ul> <li>Large Language Models for Code Synthesis</li> <ul> <li>Developing transformer-based models for program representations and using them for code completion, code repair, and code translation.</li> <li>Developed fully automated and hybrid ML pipelines for data engineering, modeling, evaluation, and deployment that can work on both client's private infra and GCP. Some of the frameworks I used were TensorFlow, Kubeflow, KFserving, and Apache Beam.</li> </ul> <li>Computer vision</li> <ul> <li>The goal was to create a single virtualized view of the electricity system through an aerial view imaginary using an image segmentation model.</li> <li>I explored various state-of-the-art image segmentation models such as PSP-NET, ViT, etc., and combined the best of all in a single model to achieve production-grade results.</li> <li>I also developed a novel metric for the evaluation of the model's performance as traditional segmentation metrics failed to provide helpful indications of failure cases. While the new metric helped filter failure cases easily which was then improved through data augmentation techniques.</li> <li>GRAD-CAM and its variant were used to locate features in the image when the model failed to produce the correct output. This analysis helped improve the quality of the training and evaluation dataset.</li> </ul> </ul> <p>     Client: Innovyze, An Autodesk company. </p> <ul> <li> At Innovyze, I helped Process Engineers to optimize chemical consumption in the water treatment plant by analyzing historical sensor data and developing predictive models to automate the processes. My primary tasks were Data analysis, Feature engineering, and Modelling.</li> </ul>          Software Development Engineer - II (AI/ML)               May, 2018 - Feb, 2021      Matrix Comsec R&amp;D, Vadodara, India.      <p>Matrix ComSec is a leader in Security and Telecom solutions for modern businesses and enterprises with 1M+ customers in 50+ countries. I was responsible for developing and delivering DL algorithms in SDK form. My major contributions are listed below,</p> <ul> <li>Face Recognition (FR) and Face Detection (FD)</li> <ul> <li>Developed FD algorithm to detect multiple faces in the image with a minimum face size of 30px.</li> <li>Developed FR algorithm with 99.85 % accuracy on the LFW benchmark.</li> <li>Improved existing FR algorithm to identify people wearing a mask on the face with only 5% accuracy drop wrt to full-face model. It helped employees to mark attendance without removing their masks during the coronavirus pandemic</li> <li>Developed a Face Mask Detection model to identify whether employees are wearing masks or not when marking attendance to improve their safety.</li> <li>Developed CNN for a single RGB image-based Passive Face Antispoofing model to prevent fake attendance marking using a mobile phone or printed photo.</li> <li>Delivered all these features in a single FR SDK for RPi3/4, CPU, GPU, and Android devices with a maximum latency of 800 ms on the slowest hardware</li> </ul> </ul> <ul> <li>Automated License Plate Recognition (ALPR)</li> <ul> <li>Developed a real-time Licence Plate Detection model which achieved 99%+ recall and 93%+ precision on the internal Indian vehicle test dataset</li> <li>Contributed to the development of the CNN model for Licence Plate Recognition using CTC Loss. We achieved 87% accuracy on challenging the Indian test datasets outperforming other commercial solutions by at least 15% and 98% accuracy on the OpenALPR benchmark</li> <li>Developed semi-supervised data annotation tool which helped us collect a labeled dataset of 30K+ images and incrementally improved the model's accuracy</li> <li>Designed ALPR SDK architecture for improving throughput by batching simultaneous requests into single inference. A myriad of inference engines such as TensorFlow, TensorRT, OpenVINO, TensorflowLite, etc. can be integrated based on hardware without modifying 95% of the codebase.</li> </ul> </ul> <ul> <li>Seven Segment Display Number Recognition</li> <ul> <li>To handle a variety of color digital displays, a self-calibration algorithm was designed to run once to extract display-specific properties using KNN and an inference algorithm was designed to use these properties for recognizing digits in real-time using SVM.</li> <li>The algorithm was integrated into existing weighbridge vehicle management software for automating data entry of the vehicle's weight from the digital display through the camera. It reduced manual human intervention by 95% and introduced transparency in the whole process.</li> </ul> </ul>          Deep Learning Research Intern               Jan, 2018 - May, 2018               Space Applications Centre (SAC) - ISRO, Ahmedabad, Gujarat, India      <p>SAC being the major R&amp;D center of ISRO designs and develops the optical and microwave sensors for the satellites, signal and image processing software, GIS software, and many applications for the Earth Observation (EO) program of ISRO. As a Research Intern, I developed DL techniques for accurate crop classification using Hyper Spectral Satellite images (425 channels per pixel). My primary contributions were as listed below</p> <ul> <li>Developed ANN-based dimensionality reduction technique which retained useful features and worked better than PCA.</li> <li>Developed virtual data augmentation technique for overcoming the issue of insufficient ground truth data of different crops for training.</li> <li>Provided detailed comparative analysis of ANN, CNN, and SVM. Designed highly accurate Parallel-CNNs architecture for classifying nearly inseparable classes based on interclass separability analysis.     <li>Developed Python library based on my work so that other scientists can use these techniques on a variety of other satellite images.</li>         Machine Learning Summer Intern               May, 2017 - June, 2017               Wolfsoft Pvt. Ltd., Vadodara, Gujarat, India.      <p>During my internship I worked on a food review app similar to Zomato and my contributions were as listed below,</p> <ul> <li>Developed a Food Recommendation API based on a Collaborative Filtering algorithm.</li> <li>Developed a Food Search API. Normalized and stored posting list of Indian food names in the Trie data structure for faster and more accurate search. Added spelling correction using the Levenshtein distance algorithm which achieved higher recall compared to SQL's search.</li> </ul>"},{"location":"publications/","title":"Publications","text":"<ol> <li>Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques <pre><code>@INPROCEEDINGS{8897897,\n  author={Patel, Hetul and Bhagia, Nita and Vyas, Tarjni and Bhattacharya, Bimal and Dave, Kinjal},\n  booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, \n  title={Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques}, \n  year={2019},\n  volume={},\n  number={},\n  pages={3728-3731},\n  doi={10.1109/IGARSS.2019.8897897}}\n</code></pre></li> </ol>"},{"location":"talks/","title":"Talks/Seminars","text":"<p>I love sharing my learnings from the ML industry with others through invited lectures or talks. All the slides of my talks/sessions can be found below.</p> <p></p> <ul> <li> <p>Conducted a full day workshop on Software Engineering Essentials for more than 100 students from sophomore and final year at Marwadi University, Rajkot\", 5 Oct, 2023.</p> </li> <li> <p>Delivered an expert talk on Hands-on on Language Modeling using Coventional and Deep Learning Approaches at ISTE Approved STTP on \"Deep Learning: Concepts to Deployment\" Organized by CSE Department, Institute of Technology, Nirma University \u2013 Internal Quality Assurance Cell (IQAC), 7 July, 2023.</p> </li> <li> <p>Conducted expert session on NLP using Deep Learning for students of Computer Science at Nirma University, Ahmedabad, 20 March, 2023.</p> </li> <li> <p>Spoke on \"Introduction Accelerators and NVIDIA RAPIDS for Datascience on GPU\" where I explained concepts such as working of GPUs, TPUs, and hands-on on cuDF and cuML libraries at  Machine Learning Ahmedabad Meetup, Infocusp, Ahmedabad, 24 Dec, 2022.</p> </li> <li> <p>Spoke on \"Introduction Accelerators and NVIDIA RAPIDS for Datascience on GPU\" where I explained concepts such as working of GPUs, TPUs, and hands-on on cuDF and cuML libraries at  Machine Learning Ahmedabad Meetup, Infocusp, Ahmedabad, 24 Dec, 2022.</p> </li> <li> <p>Spoke on \"Introduction to Deep Learning with ANN\" where I explained concepts such as MLP, backpropagation, gradient descent with hands-on implementation from scratch in NumPy at Machine Learning Summer School'22, DAIICT, Gandhinagar, 18 June, 2022.</p> </li> <li> <p>Spoke on \"Advanced application of deep learning for computer vision\" where I explained working of object detection, semantic segmentation etc. Also explained how to design models for different data modalities such as text, sound and vision etc. at Machine Learning Summer School'22, DAIICT, Gandhinagar, 19 June, 2022.</p> </li> <li> <p>Expert session on \"Transformers for NLP with hands on applications\" at Nirma University, Ahmedabad, 25 March, 2022.</p> </li> <li> <p>Expert session on Industry expectations from students in the field of deep learning at Nirma University, Ahmedabad, 07 March, 2022.</p> </li> <li> <p>Expert session on Importance of Maths for Machine Learning at Nirma University, Ahmedabad, 20 November, 2021.</p> </li> </ul>"},{"location":"blogs/software_development_essentials/","title":"Software Development Essentials","text":"<p>This is a comprehensive guide that covers a range of important topics and tools for professional software developers. It includes best practices for code profiling, debugging, contributing code, testing, code auto-formatting, spell checking, documentation, and automation using precommit hooks. </p> <p>Note</p> <p>Python is used as an example in this blog, but you can apply these concepts to any widely used programming language for development.</p>"},{"location":"blogs/software_development_essentials/#1-remote-first-developmengrt","title":"1. Remote First Developmengrt","text":"<p>Tip</p> <p>Say goodbye to running code locally \ud83d\udc4b !! </p> Remote Development Local Development Accessible from anywhere with an internet connection Limited to local machine Facilitates collaboration and teamwork with built-in features Collaboration requires manual coordination Provides a consistent development environment across team members Environment may vary across different machines Isolated from local machine, reducing conflicts and dependencies Prone to conflicts and dependency issues Offers scalability with adjustable resources Limited by local machine's resources Seamlessly integrates with version control systems like Git Requires separate setup for version control <p>For example, you can open any GitHub repository in GitHub Codespaces by just clicking \u201cCreate codespace on master\u201d as shown below.</p> <p></p> <p>Here is the list of recommended remote development platforms.</p> <ul> <li>GitHub Codespaces</li> <li>AWS Cloud9</li> <li>Project IDX</li> </ul>"},{"location":"blogs/software_development_essentials/#2-contributing-to-the-code","title":"2. Contributing to the Code","text":"<p>Tip</p> <p>Fork and Pull is all you need !!</p> <p></p> <p>It's an ideal way to follow a \"fork and pull request\" workflow when you are contributing to a collaborative project. For example, you can follow these steps to contribute to any open-source project on GitHub.</p> <ul> <li>Fork the project on GitHub.</li> <li>Clone your forked repository to your local machine.<ul> <li><code>git clone [https://github.com/](https://github.com/)&lt;your-username&gt;/&lt;repository&gt;.git</code></li> </ul> </li> <li>Create a new branch for your feature or bug fix.<ul> <li><code>git checkout -b &lt;branch-name&gt;</code></li> </ul> </li> <li>Make and commit your changes.<ul> <li><code>git add .</code> and <code>git commit -m \"Your commit message here\"</code></li> </ul> </li> <li>Push your changes to your fork on GitHub.<ul> <li><code>git push origin &lt;branch-name&gt;</code></li> </ul> </li> <li>Create a pull request from your branch to the original project.</li> </ul> <p>While fork and pull request model is specific to GitHub, many managed version control systems uses similar workflows for collaborative development. Here are some additional platforms that support collaborative development workflows similar to the \"fork and pull request\" model:</p> <ul> <li>GitLab</li> <li>Bitbucket</li> </ul>"},{"location":"blogs/software_development_essentials/#3-managing-dependencies","title":"3. Managing Dependencies","text":"<p>Tip</p> <p>If the code works for you, Nice \ud83d\udc4d But If the code works for everyone, Great \ud83d\udc4f!!</p> <p></p> <p>Managing dependencies of a project is crucial because it ensures that the project's external libraries and components are correctly integrated, up-to-date, and compatible with each other. Package managers, such as npm for JavaScript or pip for Python, simplify this process by automating the installation, version control, and resolution of dependencies.</p>"},{"location":"blogs/software_development_essentials/#start-simple","title":"Start simple !!","text":"<p>Just create a <code>requirements.txt</code> file in the root folder of your python project and list down all the dependencies as shown here. Then run <code>pip install -r requirements.txt</code> to install all of them in any machines. That\u2019s cool \ud83d\ude0e!! </p> <pre><code>Flask==2.3.3\ntensorflow==2.13.0\n</code></pre>"},{"location":"blogs/software_development_essentials/#keep-up-with-the-trend","title":"Keep up with the trend !!","text":"<p>Pip serves as a basic tool for managing Python package dependencies, while Poetry offers a modern and comprehensive solution that streamlines the entire Python project management process, including dependency management, packaging, and version control. Poetry's popularity is growing, and it simplifies dependency installation with a single command (<code>poetry install</code>) from the project's root directory.</p>"},{"location":"blogs/software_development_essentials/#4-testing","title":"4. Testing","text":"<p>Tip</p> <p>\u201cWriting tests before the code \ud83d\udcaf\u201d : Mantra for Test Driven Development (TDD).</p>"},{"location":"blogs/software_development_essentials/#unit-tests","title":"Unit Tests","text":"<p>Unit tests cover modular logic that does not require call to outside APIs. If you add a new logic, please add a unit test. We can use python\u2019s inbuilt <code>unittest</code> library to write unit tests as shown below. </p> basic_algebra.py<pre><code>\"\"\"File containing basic algebraic operations.\"\"\"\n\ndef add(a, b):\n    \"\"\"Adds two numbers.\"\"\"\n    return a + b\n\ndef subtract(a, b):\n    \"\"\"Subtracts two numbers.\"\"\"\n    return a + b  # See the bug? Check coverage section. \n</code></pre> basic_algebra_test.py<pre><code>\"\"\"Unit tests for addition function.\"\"\"\nimport unittest\nimport basic_algebra\n\nclass TestAddition(unittest.TestCase):\n    def test_add_positive_numbers(self):\n        result = basic_algebra.add(2, 3)\n        self.assertEqual(result, 5)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre> <p>You can directly run unit tests using this command: </p> <pre><code>python basic_algebra_test.py\n</code></pre> <p>Sample Output:</p> <pre><code># Output\n# .\n# ----------------------------------------------------------------------\n# Ran 1 test in 0.000s\n\n# OK\n</code></pre>"},{"location":"blogs/software_development_essentials/#automated-testing","title":"Automated testing","text":"<p>In a real world project, we have lots of test files and we certainly don\u2019t  want to run them individually.  Make sure the files end with <code>test.py</code>  and method starts with <code>test_</code>.</p> <p>Simply run this command from the project\u2019s root directory. </p> <pre><code>pytest .\n</code></pre> <p>See how it also runs test files from the sub directories. </p> <pre><code>============ test session starts ===================================\nplatform linux -- Python 3.10.8, pytest-7.4.2, pluggy-1.3.0\nrootdir: \nplugins: anyio-4.0.0, cov-4.1.0\ncollected 2 items                                                   \n\nbasic_algebra_test.py .                                       [ 50%]\nnested_dir/nested_test.py .                                   [100%]\n\n============= 2 passed in 0.02s ====================================\n</code></pre>"},{"location":"blogs/software_development_essentials/#coverage","title":"Coverage","text":"<p>Code coverage (i.e. the amount of code that is covered by unit tests) helps identify areas of the code that are potentially more or less brittle.</p> <p>After installing <code>pytest-cov</code>, run tests with coverage:</p> <pre><code>pytest --cov --cov-report term-missing\n</code></pre> <p>You can see that the coverage report tells us that we missed writing a test for the <code>subtract</code> method at line number 9.</p> <pre><code>======================== test session starts ========================\nplatform linux -- Python 3.10.8, pytest-7.4.2, pluggy-1.3.0\nrootdir: \nplugins: anyio-4.0.0, cov-4.1.0\ncollected 1 item\n\nbasic_algebra_test.py .                                        [100%]\n\n---------- coverage: platform linux, python 3.10.8-final-0 ----------\nName                                    Stmts   Miss  Cover   Missing\n---------------------------------------------------------------------\nbasic_algebra.py                            4      1    75%   9\nbasic_algebra_test.py                       8      1    88%   11\n---------------------------------------------------------------------\nTOTAL                                     112     97    13%\n\n========================== 1 passed in 0.03s ========================\n</code></pre>"},{"location":"blogs/software_development_essentials/#5-code-profiling","title":"5. Code Profiling","text":"<p>Tip</p> <p>Put your code on turbo booster \ud83d\ude80!!</p> <p>Let's say you are writing a code, and it seems to run slower than expected or consumes more resources than it should. In such cases, code profiling becomes crucial as it allows you to pinpoint exactly which parts of your code are causing performance issues, helping you make informed optimizations to deliver a more efficient and responsive application.</p>"},{"location":"blogs/software_development_essentials/#time-benchmark-short-code-snippets","title":"Time: Benchmark Short Code Snippets","text":"<p>In Python, the most basic form of profiling involves measuring the code execution time by calling one of the\u00a0timer functions\u00a0from the\u00a0time\u00a0module:</p> my_program.py<pre><code>import time\n\ndef slow_func():\n    print(\"Slow function running!!\")\n    my_code = 5+5\n    time.sleep(6) # Sleep for 6 seconds\n\ndef fast_func():\n    print(\"Fast function running!!\")\n    my_code = 5+5\n    time.sleep(2) # Sleep for 2 seconds\n\ndef main():\n    start = time.time()\n    fast_func()\n    slow_func()\n    print(f\"{(time.time() - start)} seconds\")\n\nif __name__ == '__main__':\n   main()\n</code></pre> <p>Sample Output:</p> <pre><code>Fast function running!!\nSlow function running!!\n8.000357627868652 seconds\n</code></pre>"},{"location":"blogs/software_development_essentials/#cprofile-collect-detailed-runtime-statistics","title":"cProfile: Collect Detailed Runtime Statistics","text":"<p>cProfile is a built-in Python module used for performance profiling. It allows developers to analyze the execution time of individual functions and method calls within their code, helping identify bottlenecks and areas for optimization, making it a valuable tool for optimizing Python applications.</p> <p>To profile a specific file using <code>cProfile</code> run below command which generates a program.prof output file.</p> <pre><code>python -m cProfile -o program.prof my_program.py\n</code></pre> <p>We visualize the output of cProfile suing snakeviz by running below command.</p> <pre><code>snakeviz program.prof\n</code></pre> <p>Sample Output:</p> <p></p>"},{"location":"blogs/software_development_essentials/#6-code-debugging","title":"6. Code Debugging","text":"<p>Tip</p> <p>Finding a needle in a haystack \ud83e\udd2f</p> <p>Often times, we encounter unexpected issues, errors, or undesirable behavior while developing software. Debugging is the crucial process of systematically investigating and rectifying these problems, ensuring that our code functions as intended and meets quality standards.</p>"},{"location":"blogs/software_development_essentials/#python-debugger","title":"Python Debugger","text":"<p>pdb  is a built-in interactive debugging tool for Python that allows developers to step through code, inspect variables, and diagnose issues in real-time, aiding in the debugging process.</p> <p>For example when this run file, it will pause the execution just after calculating the result. </p> <pre><code>import pdb\n\ndef divide(x, y):\n  result = x / y\n  pdb.set_trace() # Start debugging from here\n  return result\n\nresult = divide(10, 2)\n</code></pre> <p>Sample input and outputs.</p> <pre><code>(Pdb) x\n10\n(Pdb) y\n2\n(Pdb) result\n5.0\n</code></pre>"},{"location":"blogs/software_development_essentials/#7-following-a-consistent-code-style","title":"7. Following a consistent code style","text":"<p>Tip</p> <p>Wear your own style \ud83d\ude0e</p> <p>Programming style, also known as\u00a0code style, is a set of rules or guidelines used when writing the\u00a0source code. It generally helps other programmers\u00a0read and understand your source code conforming to the style, and help to avoid introducing errors.  To start with you can follow Google Python Style Guide. </p> <p>Let\u2019s see an example of formatting the imports. Imports should be on separate lines; there are\u00a0exceptions for\u00a0<code>typing</code>\u00a0 and \u00a0<code>collections.abc</code>\u00a0 imports.</p> <pre><code>Yes: from collections.abc import Mapping, Sequence\n     import os\n     import sys\n     from typing import Any, NewType\n</code></pre> <pre><code>No:  import os, sys\n</code></pre>"},{"location":"blogs/software_development_essentials/#8-linting","title":"8. Linting","text":"<p>Tip</p> <p>Meet your personal code-proofreader \ud83e\uddd0 !!</p> <p>While it's a good idea to follow a consistent coding style, it's not usual for the human brain to remember all the rules while writing the code. Don't worry; a linter is a program that automates the process of checking your code against coding style guidelines.</p> <p>To check all the styling issues, run the <code>pylint</code> command.</p> <pre><code>pylint .\n</code></pre> <p>Sample Input</p> app.py<pre><code>import os, sys\n</code></pre> <p>Sample Output</p> <pre><code>************* Module app\napp.py:1:0: C0304: Final newline missing (missing-final-newline)\napp.py:1:0: C0114: Missing module docstring (missing-module-docstring)\napp.py:1:0: C0410: Multiple imports on one line (os, sys) (multiple-imports)\napp.py:1:0: W0611: Unused import os (unused-import)\napp.py:1:0: W0611: Unused import sys (unused-import)\n\n-----------------------------------\nYour code has been rated at 0.00/10\n</code></pre> <p>Tip</p> <p>Oh I can fix that easily \ud83d\udc81\u200d\u2642\ufe0f !!</p> <p>While linters are good at finding code style-related mistakes in the code, formatters go a step further by automatically fixing those style issues, ensuring code consistency and reducing manual effort in code maintenance.</p> <p>We can use YAPF to automatically fix styling errors by running below command.</p> <pre><code>yapf -i my_code.py\n</code></pre> <p>An example of the type of formatting that YAPF can do, it will take this ugly code:</p> my_code.py<pre><code>x = {  'a':37,'b':42,\n\n'c':927}\n\ny = 'hello ''world'\nz = 'hello '+'world'\na = 'hello {}'.format('world')\nclass foo  (     object  ):\n  def f    (self   ):\n    return       37*-+2\n  def g(self, x,y=42):\n      return y\ndef f  (   a ) :\n  return      37+-+a[42-x :  y**3]\n</code></pre> <p>and reformat it into:</p> my_code.py<pre><code>x = {'a': 37, 'b': 42, 'c': 927}\n\ny = 'hello ' 'world'\nz = 'hello ' + 'world'\na = 'hello {}'.format('world')\n\nclass foo(object):\n    def f(self):\n        return 37 * -+2\n\n    def g(self, x, y=42):\n        return y\n\ndef f(a):\n    return 37 + -+a[42 - x:y**3]\n</code></pre>"},{"location":"blogs/software_development_essentials/#9-code-auto-formatting","title":"9. Code Auto-Formatting","text":""},{"location":"blogs/software_development_essentials/#10-documentation","title":"10. Documentation","text":"<p>Tip</p> <p>Please have some empathy  \ud83d\ude4f!!</p> <p>Maintaining good documentation of the code is essential for enhancing code understandability, facilitating collaboration, and ensuring long-term maintainability of the software.</p>"},{"location":"blogs/software_development_essentials/#general-documentation","title":"General Documentation","text":"<p>We can utilize Docusaurus 2.0 to create comprehensive project documentation. </p> <p>Docusaurus streamlines the process of creating and maintaining documentation websites by automatically generating static sites from the Markdown content we provide. Furthermore, it effortlessly infers the sitemap from directory structures, enhancing the efficiency of the documentation development process.</p> <p>Look at this example how Docusaurus generates a static site from markdown files.</p> <p></p>"},{"location":"blogs/software_development_essentials/#auto-generated-api-docs","title":"Auto-Generated API Docs","text":"<p>Use Sphinx to generate API documentation from docstrings in your code:</p> <pre><code>sphinx-quickstart  # Follow the setup prompts\nsphinx-apidoc -o docs maths/  # Generate API documentation\nPYTHONPATH=. sphinx-build -b html docs/ docs/_build  # Build HTML documentation\n</code></pre> <p>These sample inputs and outputs provide a clear picture of how these tools and best practices can be applied and what results to expect.</p> <p>Sample input:</p> <pre><code>\"\"\"Addition of two numbers.\"\"\"\n\ndef add_nums(num1, num2):\n    \"\"\"This method will be used to add two numbers.\n\n    Args:\n        num1: The first number\n        num2: The second number\n\n    Returns:\n        The sum of two numbers\n    \"\"\"\n    answer = num1 + num2\n    return answer\n</code></pre> <p>Sample Output:</p> <p></p>"},{"location":"blogs/software_development_essentials/#11-automation-before-committing-pre-commit-hook","title":"11. Automation Before Committing: Pre-commit Hook","text":"<p>Tip</p> <p>Let me do that all for you \u2705</p> <p>No need to run all these commands manually. One powerful way to enforce coding standards, run tests, and perform various checks before committing your code is by using a precommit hook.</p> <p>A precommit hook is a script or set of scripts that automatically run before a commit is allowed. Here's how you can set it up:</p> <ol> <li> <p>In your project directory, create a configuration file named <code>.pre-commit-config.yaml</code>. This file defines the hooks and checks that should run before each commit.</p> <pre><code>default_stages: [commit, push]\nrepos:\n- repo: local\n hooks:\n - id: pytest\n   name: pytest\n   entry: pytest\n   args: ['--cov-report=term-missing', '--cov=search', '--cov-fail-under=50']\n   language: system\n   always_run: true\n   pass_filenames: false\n</code></pre> </li> <li> <p>Run the following command to install the precommit hook in your local Git repository:</p> <pre><code>pre-commit install\n</code></pre> </li> </ol> <p>Now, before each commit, the configured hooks will automatically check your code. If any checks fail, the commit will be rejected, ensuring that only code that passes these automated checks is committed to the repository.</p>"},{"location":"blogs/software_development_essentials/#conclusion","title":"Conclusion","text":"<p>In conclusion, this guide highlights key practices and tools vital for software development, encompassing remote-first collaboration, dependency management, effective testing, code optimization, debugging, code style adherence, automation, and robust documentation. By embracing these practices and tools, developers can elevate their coding proficiency and streamline their workflows across programming languages, fostering more efficient software development processes.</p>"},{"location":"projects/3d_password/","title":"3D Password","text":""},{"location":"projects/3d_password/#introduction","title":"Introduction","text":"<p>3D password is an advancement in the field of user authentication techniques. 3d password can have large number of possible combinations as integration of textual, graphical and biometric. So it is technically very hard to crack the one. 3d password is setup in any computer screen which includes a virtual environment where user can set different objects as password. </p> <p></p> <p>Below is the list object which can be used as 3D password,</p> <ol> <li>An Iris Scanner</li> <li>A Fingerprint Scanner</li> <li>A Virtual ATM machine that requires real ATM card</li> <li>A text input device which take input</li> <li>Photo album as pictorial password</li> </ol> <p>So any real world object can be integrated in the 3D environment and any new authentication technique can also be included if it is invented after.</p>"},{"location":"projects/3d_password/#working-of-3d-environment","title":"Working of 3d environment","text":"<p>Every object in a 3D environment has its unique <code>(x,y,z)</code> coordinates. If the size of the 3D world is <code>M x M x M</code> then the entire virtual world can be represented by <code>[1,2,3......, M] x [1,2,3......, M] x [1,2,3......, M]</code>. User can move in the 3d environment using mouse and keyboard. Also biometric scanners, stylus pen and ATM card reader can be attached for interactions with virtual models of it. The coordinates are stored when any interaction is performed with the object. E.g. user opens a door located at <code>(10,15,60)</code>, He closes the door and enters the office. Take a marker located at <code>(25,20,70)</code> and write <code>\u201cHello\u201d</code> on the white board located at <code>(25,52,80)</code>. This interaction is stored as 3D password as follows:</p> <ol> <li>(10,15,60) Object: door Action: Opening the door,</li> <li>(10,15,60) Object: door Action: Closing the door,</li> <li>(25,20,70) Object: marker Action: Take the marker,</li> <li>(25,52,80) Object: white board Action: Writing \u201cH\u201d,</li> <li>(25,52,80) Object: white board Action: Writing \u201cE\u201d,</li> <li>(25,52,80) Object: white board Action: Writing \u201cL\u201d,</li> <li>(25,52,80) Object: white board Action: Writing \u201cL\u201d,</li> <li>(25,52,80) Object: white board Action: Writing \u201cO\u201d.</li> </ol>"},{"location":"projects/3d_password/#implementing-3d-password","title":"Implementing 3D Password","text":"<p>3D password implementation requires the integration of software and hardware. 3D objects can be designed in 3D modelling software such as Adobe Maya, Blender etc. This 3D objects are imported in a 3D world which can be created in gave development software like UNITY 3D. The Serial Input and Output can be driven through USB port. I have implemented small 3D Password using UNITY 3D. The steps are as follows:</p>"},{"location":"projects/3d_password/#step-1-read-all-the-instructions-carefully","title":"Step 1: Read all the instructions carefully.","text":"<p>Figure 4.1 shows the instruction to be followed to travel in the 3D world. Use Keyboard direction keys to move in linear direction and mouse to move in angular direction.</p>"},{"location":"projects/3d_password/#step-2-open-the-right-or-left-gate-to-enter-into-the-nirmas-gate","title":"Step 2: Open the Right or Left gate to enter into the Nirma\u2019s Gate.","text":"<p>As shown in figure 4.2 You will see the entrance gate of Nirma. User has to go Security guard and use the mouse to click on LEFT or RIGHT button to open the left or right gate respectively. As The password is predefined here I\u2019ll open the LEFT gate as shown in fig 4.3 and enter into the college.</p>"},{"location":"projects/3d_password/#step-3-go-to-correct-colony-and-choose-correct-photo-frame","title":"Step 3: Go to correct colony and choose correct photo frame.","text":"<p>As shown in figure 4.4, You will see 3 identical colonies with same photo frames hanged on the walls. The difference is that each colony has different number of trees near it. We will go first the colony which has 3 trees near it</p> <p></p> <p>As shown in figure 4.5 there are 4 photo frames hanged on the wall from the left</p> <ol> <li>Steve jobs</li> <li>Mahatama Gandhi</li> <li>Karsanbhai Patel</li> <li>Bill Gates</li> </ol> <p>I will select the Mahatama Gandhi photo frame by clicking on it. The photo will come forward as selected as shown in figure 4.6</p> <p></p> <p></p> <p>Then I\u2019ll go to the another colony with two trees. And select the Karsanbhai patel\u2019s photo frame as it is my predefined password.</p>"},{"location":"projects/3d_password/#step-4-click-the-login-button-to-get-the-access","title":"Step 4: Click the login button to get the access.","text":"<p>After performing the correct sequence of interactions, I will go to the last stage of my password where I click the login button as shown in figure 4.8. Now If all the actions are correctly performed, I will be logged into the Nirma University System as show in figure 4.9. Else it will show me an Invalid password error as shown in figure 4.10.</p> <p></p> <p></p> <p></p>"},{"location":"projects/alpr/","title":"Automatic License Plate Recognition","text":""},{"location":"projects/alpr/#introduction","title":"Introduction","text":"<p>ALPR/ANPR is an algorithm to recognize the digits of a vehicle number plate using its image. Even though this problem seems a simple optical character recognition task, many traditional solutions fail to achieve good results in real-world conditions as shown below. I developed a custom Deep Learning based solution that not only works with these cases but is also fast enough to deploy on edge devices. I broke down this task into two subtasks, license plate detection, and recognition.</p>"},{"location":"projects/alpr/#license-plate-detection","title":"License Plate Detection","text":"<p>In the first phase, we need an object detection model which can retrieve the bounding box coordinates of a license plate in the input image. </p> <p></p>"},{"location":"projects/alpr/#dataset-collection","title":"Dataset Collection","text":"<p>Since the challenge was to build this model for real-world images, I needed a dataset that has variations in vehicle type, plate type, location of plate, size, shape, etc. I collected a large number of vehicle images and hand-annotated bounding boxed for license plates. Some of the sources I used for collecting data are listed below,</p> <ul> <li> <p>OpenSource Datasets: I merged all available open-source plate detection datasets which I found such as Cars, CCPD etc. into a large collection of the labeled dataset.</p> </li> <li> <p>Active learning: I trained my initial model on all the readily available annotated images. Then used this model to generate labels for unknown images and corrected the annotations manually. This way I was able to label a large number of unlabelled images quickly. For active learning, I developed a custom annotation tool using javascript to adjust bounding boxes.</p> <p></p> </li> </ul> <p>Some of the unlabelled datasets used for active learning are as follows,</p> <ul> <li> <p>Google images: Vehicle images with visible license plates from google search.</p> </li> <li> <p>Youtube videos: A large number of channels live stream traffic or street view cameras 24/7 on YouTube. I collected a large number of frames containing vehicles by running a vehicle detection model on it.</p> <p></p> </li> <li> <p>India Driving Dataset: Around 10k Indian street view images captured from a front-facing camera attached to a car.</p> <p></p> </li> </ul>"},{"location":"projects/alpr/#modeling","title":"Modeling","text":"<p>I used tensorflow-yolov3 repo to train my custom yolov3 model. YoloV3 is a single-shot detector that can detect objects of multiple scales in a single pass. I made a couple of modifications as per my use-case as below,</p> <ul> <li>Since I only needed to predict a single class, I reduced the number of channels for each layer by at least 8x. This way I was able to fit my model on edge devices.</li> <li>I used multiscale training where instead of training the model on a fixed input resolution, we choose a random input resolution for each batch. e.g I trained my model on batches varying in input resolutions from 128x128, 256x256, 512x512, and 768x768 pixels. This way I could use the same model for different input sizes based on latency requirements as higher resolution takes more inference time.</li> <li>For the prediction layer, I used the encoding method proposed by the SSD paper since the TensorFlow-lite library has an inbuilt Op called <code>TFLite_Detection_PostProcess</code> for SSD style decoding and NMS, and thus it was easy to convert the entire model to tflite format later for edge deployment.</li> <li>For bounding box regression I used Generelized IOU loss instead of standard box regression and for classification, I used the Focal Loss to tackle the class imbalance problem.</li> <li>For data augmentation, I used imgaug library which preserves the location of the bounding box after applying affine transformations.</li> <li>Apart from standard augmentations, I designed a scale-oriented augmentation in which we choose a random bounding box from the image and randomly crop 10-90% image around it. This way we make sure our model sees all the different scales of the license plate. </li> </ul>"},{"location":"projects/alpr/#polygon-detection","title":"Polygon detection","text":"<p>Our license plate detection model only predicts rectangular bounding boxes. For recognition, we need to correct issues like rotation and skewness using a perspective transformation as shown below. For this, we need the coordinates of the 4 corners. </p> <p></p>"},{"location":"projects/alpr/#dataset-collection_1","title":"Dataset Collection","text":"<ul> <li>I used CCPD dataset as it contains polygon annotations for license plates.</li> <li>The CCPD only contained blue color Chinese number plates. To generalize better, I replaced the original plate images with other country number plates using OpenCV's perspective transform.</li> </ul>"},{"location":"projects/alpr/#modeling_1","title":"Modeling","text":"<ul> <li>I trained a very small CNN model which directly regresses these 8 values (x and y coordinates of all the 4 points) given a crop of license plates. </li> <li>Additionally, I predicted the probability of the license plate filtering out false positives. </li> <li>I used wing loss instead of standard MSE loss to weigh hard examples more compared to the easier ones.</li> </ul>"},{"location":"projects/alpr/#license-plate-recognition","title":"License Plate Recognition","text":"<p>After detecting and correcting the angle and skewness of the license plate, in the third stage, we need to extract the license plate number from the crop. The license plates can be single lines or multi-line. Also, the variability in lighting conditions, background noise, fonts and font size, etc. results in very poor recognition accuracy for standard image processing-based approaches. So I trained a model which can predict both single and multiline license plates directly from the RGB image crop.</p>"},{"location":"projects/alpr/#dataset-collection_2","title":"Dataset collection","text":"<ul> <li>Similar to the detection task, I started with CCPD and other open-source datasets which contained string-level annotations for plate images.</li> <li>Then used an active learning technique to iteratively predict and refine labels for unlabelled images to increase the dataset size. With active learning, I was able to collect around 30k labeled license plate crops.</li> <li>The problem with this data was the class imbalance at the character level. Since the model does not see all the characters equally, it misclassified low-frequency characters often. E.g. 0&lt;-&gt;8, 1&lt;-&gt;7 etc. To overcome this issue, I synthetically generated license plate images with different backgrounds and styles keeping the total frequency of each character equal across the entire dataset. I used this data to train the model from scratch and then finetuned it with real data which increased the overall accuracy by more than 20%.</li> <li> <p>I also designed a 3D version of an Indian license plate in a blender similar to as shown below and rendered different views of it by changing characters for every render using python and blender integration. This way I was able to improve the model's accuracy on country-specific plates.</p> <p></p> </li> </ul>"},{"location":"projects/alpr/#modeling_2","title":"Modeling","text":"<ul> <li>I designed a custom CNN + CTC decoder based model for recognition.</li> <li>The input of the model was semi-rectangular (width 128 and height 256) such that we can fit both single and multiline number plates in the input placeholder padded by zeros.</li> <li>The model extracts features from the upper and lower image and then concatenates both in a single row such that we have an output of shape (H:1, W:64, C:37). Channels refer to the number of possible characters for each position and 64 is the maximum string length the model can predict.</li> <li>CTC loss was used to directly train the model on unaligned image-and-text pairs. The model learns to output one character for every 64 positions such that the final string matches our desired string by predicting other characters as blank. During inference, the CTC beam search decoder was used for better accuracy.</li> <li>I implemented my own CTC decoder in C++ with a python wrapper as frameworks like TensorflowLite and OpenVINO did not support the beam search decoding op for edge deployment.</li> </ul>"},{"location":"projects/alpr/#evaluation","title":"Evaluation","text":"<p>To test the model's performance with existing solutions I compared my model to two openly available commercial solutions on Indian and mix country number plates.</p> <p></p> <ol> <li>Plate Recognizer: The model was able to achieve higher accuracy for both detection and recognition tasks compared to results extracted from the plate recognizer's online demo. late recognizer solution.</li> <li>OpenALPR Benchmark: This company published a benchmark of 100 challenging images on which the model was able to achieve 98% accuracy.  </li> </ol>"},{"location":"projects/alpr/#deployment","title":"Deployment","text":"<p>I used different frameworks to deploy the same model based on hardware specifications. Some of the frameworks I used are listed below. The challenge was to use only supported ops and maintain accuracy after conversion from TensorFlow to custom frameworks. </p> <ul> <li>NVIDIA TensorRT: For GPU-enabled server deployment, I converted my model to TensorRT plan files. This reduced float32 to float16 and significantly improved inference speed.</li> <li>Intel OpenVino: For intel's CPU-based server deployment, I converted my model to OpenVino's intermediate format. This reduced float32 to int8 using inbuilt quantization with a fallback to float32.</li> <li>TensorFlow Lite: For edge devices such as raspberry-pi and smartphones, I used TensorFlow lite to deploy my model.</li> </ul> <p>I used C++ to write a modular backend where image reading, preprocessing and post-processing functions were common. While the actual inference class was an abstract class implemented for every hardware using framework-specific methods. It is easy to compile and distribute models in form of a C++ SDK and then to write separate code for each hardware. </p>"},{"location":"projects/auto_adaptive/","title":"Auto Adaptive Image Classifier","text":"<p>Platform similar to google photos which can group similar looking images into albums and can predict labels for the unknown images. Starting with zero known labels, the AI can learn to recognize many different classes on the fly as in when new labelled examples are added.</p>"},{"location":"projects/auto_adaptive/#functionalities","title":"Functionalities","text":""},{"location":"projects/auto_adaptive/#homepage","title":"HomePage","text":"<ul> <li>GridLayout which shows different classes</li> </ul>"},{"location":"projects/auto_adaptive/#add-zip-for-one-class","title":"Add Zip for one class","text":"<ul> <li>Defines popup which takes zip from user and class label to add multiple images with labeled class in database</li> </ul>"},{"location":"projects/auto_adaptive/#view-individual-class-images","title":"View Individual class Images","text":"<ul> <li>Click on class card to view images for that class</li> <li>We can also update the name of class from here</li> </ul>"},{"location":"projects/auto_adaptive/#predict-image","title":"Predict Image","text":"<ul> <li>Defines popup which ask image input from user (only png and jpg allowed) and predict class from existing classes then user can give choice of add predicted label to given image or not</li> </ul> <p>E.g. We have 5 car images in the database and we will test on unknown car image.</p> <p></p> <p>Model predicted below out of training image correctly as a car.</p> <p> </p>"},{"location":"projects/auto_adaptive/#add-unknown-class","title":"Add unknown class","text":"<ul> <li>We can add a new class without any label. These can be out of distribution images,</li> </ul>"},{"location":"projects/auto_adaptive/#recluster","title":"Recluster","text":"<ul> <li>When we click on recluster button, all unknown images are grouped together by their visual similarity.</li> </ul>"},{"location":"projects/auto_adaptive/#add-group-to-known-classes","title":"Add group to known classes","text":"<ul> <li>We can add goup_0 to human verified classes by updating its name.</li> </ul>"},{"location":"projects/auto_adaptive/#train-on-new-images","title":"Train on new images","text":"<ul> <li>To automatically adapt newly added classes we can click the train button.</li> <li>Auto adapts new verified classes.</li> </ul> <p>We can see two pandas images are correctly classified which are not part of training.</p> <p> </p>"},{"location":"projects/auto_adaptive/#predict-out-of-distribution-classes","title":"Predict out of distribution classes","text":"<ul> <li>Classes which the model has never seen will be predicted as unknown.</li> <li>We have set a threshold for detecting unknown classes high so that precision can be higher compared to recall.</li> <li>Try with a few good quality images for known classes if unknown is suggested.</li> </ul>"},{"location":"projects/clothes_retrieval/","title":"Clothes Retrieval","text":"<p>Searching for exact cloth image accurately from massive collections of clothes' images based on a query image.</p> <p>For example, here the left most image is the query image and other images are matched images retrieved from a huge collection of clothes using a computer vision algorithm.</p> <p></p>"},{"location":"projects/clothes_retrieval/#dataset-details","title":"Dataset Details","text":"<ul> <li>Dataset used : Deep Fashion 2</li> <li>Problem statement : Consumer-to-shop Clothes Retrieval</li> <li>Problem description: Matching consumer-taken photos with their shop counterparts</li> </ul>"},{"location":"projects/clothes_retrieval/#code","title":"Code","text":"<p>Google Colab</p>"},{"location":"projects/clothes_retrieval/#building-the-intuition","title":"Building the intuition","text":"<p>Sorted in the order of how I reached the solution</p>"},{"location":"projects/clothes_retrieval/#1-usecase-understanding","title":"1. Usecase understanding","text":"<ul> <li>Consumer can upload a photo of clothing item </li> <li>System should retrieve similar looking items from shopping catalog.</li> </ul>"},{"location":"projects/clothes_retrieval/#2-dataset-understanding","title":"2. Dataset understanding","text":"<p>In the given task, we have two different sources of data. </p> <ul> <li>Consumer captured images : </li> </ul> <p>These are low quality images captured using phone's camera (front or back). Images have variation in lighting, orientation, occlusion, filters etc.</p> <ul> <li>Shop captured images :</li> </ul> <p>Shop images include good quality photo shoot quality images. It also includes images from online shopping carts where background is removed etc.</p>"},{"location":"projects/clothes_retrieval/#3-real-world-challenges","title":"3. Real world challenges","text":"<ul> <li>Labeling the dataset is major challenge</li> <li>Shopping image collection including online and offline stores can be huge</li> <li>Thousands of unique clothing items</li> <li>Manually pairing each user captured image to a similar shop image costs lots of human effort</li> <li>When the product is new, we might not have huge collection of user captured images. In that case the system cannot work better for new styles</li> </ul>"},{"location":"projects/clothes_retrieval/#solving-the-problem","title":"Solving the problem","text":"<p>Considering the scarcity of labeled consumer to shop pairs, I decided build two stage pipeline. First learning task agnostic attributes from raw data then use them to reduce consumer to shop domain gap using fewer labeled images.</p>"},{"location":"projects/clothes_retrieval/#step-1-learning-directly-from-raw-images-without-labeling","title":"Step 1 : Learning directly from raw images without labeling","text":"<p>Observations</p> <ul> <li>Humans are good at pattern matching. </li> <li>We recognize many things from their attributes. For e.g we recognize a vehicle with wheels, seats, glasses, horn etc.</li> <li>We also use this attributes to distinguish between two different vehicles</li> <li>We learn about these attributes by comparing between many instances unconsciously</li> </ul> <p>Inspirations</p> <ul> <li>Deep learning model can also learn similar attributes by just observing across different images</li> <li>In many image retrieval problems we consider output of pre-fc layer as embedding</li> <li>We use metric learning approaches such as triplet loss to lean discriminative embedding</li> <li>We can consider each dimension in embedding vector as one attribute</li> <li>This attributes make the embedding vector discriminative</li> </ul> <p>Difference from metric learning</p> <ul> <li>Metric learning approaches try to increase distance between embeddings as a whole. For that we need positive and negative pairs.</li> </ul> <p></p> <ul> <li>Instead we can increase distance between attributes of embedding without labels.</li> <li>Here, We do not try to teach which attribute represents what?</li> <li>We try to teach that no two attributes should represent same concept.</li> </ul> <p></p> <p>Mathematical Formulation</p> <ul> <li>We often calculate similarity of two vectors using cosine similarity measure.</li> <li>Cosine similarity has an interpretation as the cosine of the angle between the two vectors</li> <li>Cosine similarity is not invariant to shifts. If x was shifted to x+1, the cosine similarity would change.</li> </ul> <p></p> <ul> <li>Unlike the cosine, the correlation is invariant to both scale and location changes of x and y.</li> </ul> <p></p> <ul> <li>Correlation is the cosine similarity between centered versions of x and y</li> </ul> <p></p> <ul> <li>In a batch of images, attribute vector represents values of particular dimension across all images</li> <li>For e.g if batch size = 512 and embedding dim = 2048, then we have 2048 attribute vectors each of length 512</li> <li>Now consider another batch of slightly different version of the same images in first batch.</li> </ul> <p>Our goal is to</p> <ol> <li>Maximize cosine similarity of same attributes in both batches</li> <li> <p>Minimize cosine similarity of different attributes in both batches</p> </li> <li> <p>Here, I used correlation as a proxy loss as both have same output range [-1,1]</p> </li> </ol> <p>I divided loss function in two parts</p> <ol> <li>Correlation of attribute vectors at same index should be 1. To make model generalize better I decreased the predicted correlation by margin m. For e.g if correlation is 0.9 we make it 0.6 so model tries to make it higher.</li> <li>Correlation of attribute vectors at different index should be 0.</li> <li>Since there are N^2-N pairs of different attribute vectors I assigned lower weight compared to positive attribute pair. </li> </ol> <p>Code for attribute loss</p> <pre><code>def attribute_loss(emb_1, emb_2, alpha=5e-4, margin=0.3):\n\n    # per feature-dimension normalisation\n    standardize = lambda x : (x - tf.reduce_mean(x, axis=0)) / tf.math.reduce_std(x, axis=0)\n    emb_1_std = standardize(emb_1) # BxE\n    emb_2_std = standardize(emb_2) # BxE\n\n    # similarity of pairwise feature-dimension\n    bsize = tf.cast(tf.shape(emb_1)[0], tf.float32)\n    cos_t = tf.matmul(emb_1_std, emb_2_std, transpose_a=True) / bsize # Embed x Embed\n    acos_t = tf.acos(tf.clip_by_value(cos_t, -1.0, 1.0))\n\n    # diagonal values rep. expected similarity of same feature-dim\n    same_dim_mask = tf.eye(emb_1.shape[1]) # Embed x Embed\n\n    # non-diagonal values rep. expected similarity of different features-dim\n    diff_dim_mask = 1.0 - same_dim_mask # Embed x Embed\n\n    # increase angle betweem same feature-dims : Cos(angle + m)(i==j)\n    cosine_same     = tf.cos(acos_t + margin)\n    same_dim_loss   = tf.square(same_dim_mask - cosine_same) * same_dim_mask\n\n    # decrease angle between different feature-dims : Cos(angle - m)(i!=j)\n    # cosine_diff     = tf.cos(acos_t - margin)\n    diff_dim_loss   = tf.square(same_dim_mask - cos_t) * diff_dim_mask\n\n    # final weighted loss\n    weighted_loss = tf.reduce_sum(same_dim_loss + diff_dim_loss * alpha)\n    return weighted_loss\n</code></pre>"},{"location":"projects/clothes_retrieval/#step-2-reducing-the-domain-gap-using-pinch-of-supervision","title":"Step 2 : Reducing the domain gap using pinch of supervision","text":"<ul> <li>Model trained with attribute loss can learn diverse set of attributes for each image.</li> <li>Although consumer and shop images have inherent biases due to different source of data generation</li> <li>This is very well known as domain gap where two similar objects can perceived differently by model due to pixel level differences</li> <li>I used combination of attribute loss and instance loss to reduce this domain gap with only 3 more epochs for finetuning</li> </ul> <p>1. Attribute loss:</p> <ul> <li>In stage one, attribute loss was calculated between two different versions of same image as we did not have any other information</li> <li>Here we calculate it between attributes of same cloth type but one image from consumer and other from shop using label information</li> </ul> <p>2. Instance loss:</p> <ul> <li>Attribute loss encourages model to learn features which are consistent in consumer and shop domain</li> <li>Instance loss is normal classification loss which uses these attributes to group similar cloths in tight clusters and increase distance between other groups.</li> <li>I used arcface as an instance loss.</li> </ul> <p>3. Using weights from attribute model to</p> <ol> <li>Initialize backbone</li> <li>Initialize weights of newly added classification layer.</li> <li>Arcface tries to reduce cosine similarity of weights vector and instance vector.</li> <li>I used average of all embeddings produced by unsupervised model for each class as the initial weight vector of that class.</li> </ol> <p></p> <p>Code for classifier weights calculation</p> <pre><code># Get classifier weights\ndef classifier_weights(dataset, model, num_class):\n    # calculate avg embedding for each class as w_init for fc\n    out_layer   = params.model.class_layer\n    weights     = np.zeros(shape=(num_class, model.outputs[out_layer].shape[-1]),\n                           dtype=np.float32) # (n_class, emb_dim)\n    # (n_class,) extra 1 for ignoring zero div\n    class_cnt   = np.ones(shape=(num_class,), dtype=np.float32) \n    for data in tqdm(dataset):\n        user, shop, class_id = data\n        all_images  = tf.concat([user, shop], axis=0)\n        class_ids   = tf.concat([class_id, class_id], axis=0)\n        embeds      = model(all_images, training=False)[out_layer].numpy()\n        weights[class_ids] += embeds\n        class_cnt[class_ids] += 1\n    weights = np.divide(weights, class_cnt[:, np.newaxis])\n    return weights\n</code></pre>"},{"location":"projects/clothes_retrieval/#experiments","title":"Experiments","text":""},{"location":"projects/clothes_retrieval/#model-details","title":"Model details","text":"<ul> <li>Model Resnet50</li> <li>Batch Size 512</li> <li>Input resolution 96</li> <li>Embedding dimension 2048</li> </ul> <p>For training details refer code notebook</p>"},{"location":"projects/clothes_retrieval/#results","title":"Results","text":""},{"location":"projects/clothes_retrieval/#final-comments","title":"Final Comments","text":"<ul> <li>Although unsupervised loss has lower accuracy when source domains are different, it performs significantly better in mix domain (Exp 1 Last Block).</li> <li>Which suggests that attribute loss can work very well in task agnostic manner.</li> <li>Both attribute loss and fc7 weights initialization using avg class embedding improves results</li> <li>Supervised training from scratch performed significantly worse when training from scratch for 3 epoch</li> </ul>"},{"location":"projects/clothes_retrieval/#demo-on-validation-dataset","title":"Demo on validation dataset","text":"<ul> <li>No image was used during training</li> <li>Results are in descending order of recall</li> <li>First column is consumer query image</li> <li>Other columns are retrieved shop images</li> <li>Green box is True Positive, Red box is False positive</li> </ul>"},{"location":"projects/clothes_retrieval/#model-unsupervised-learning-attribute-loss-exp-1","title":"Model :  Unsupervised Learning + Attribute Loss (Exp 1)","text":""},{"location":"projects/clothes_retrieval/#model-attribute-loss-instance-loss-fc7-weights-init-exp-2","title":"Model :  Attribute Loss + Instance Loss + fc7 weights init (Exp-2)","text":""},{"location":"projects/deep_face_recognition/","title":"Deep Face Recognition","text":""},{"location":"projects/deep_face_recognition/#introduction","title":"Introduction","text":"<p>Deep face recognition is the technique to identify the identity of a person using only facial images. Generally, a Deep Conv Neural Network (DCNN) is used for transforming an image into fix length high dimensional vector. We usually precompute the embeddings for all the people in our database using a few facial images per person and use it for matching it against an embedding of an unknown face image using distance metrics such as cosine similarity.</p> <p>The accuracy of our model depends upon how well it can maximize the distance between two different person's face embeddings and minimize the distance between two face embeddings of the same person.</p> <p>A standard approach is to train our model using triplet loss where for each image we prepare a positive image that belongs to the same person and a negative image that belongs to a different person. Although there is a combinatorial explosion in the number of face triplets especially for large-scale datasets, leading to a significant increase in the number of iteration steps. Technique such as Hard Negative Mining is used very often with triplet loss to generalize it for challenging face images. Though sampling is quite a hard problem.</p> <p></p>"},{"location":"projects/deep_face_recognition/#arcface-loss","title":"Arcface Loss","text":"<p>Instead of sampling triplets, I used the Arcface loss which required no sampling at all. Arcface modifies the logit for the target class before calculating softmax cross-entropy loss, such that the model learns to form very close clusters for embeddings of the same class. Specifically, the dot product between the DCNN feature and the last fully connected layer is equal to the cosine distance after feature and center normalization. Arcface utilizes the arc-cosine function to calculate the angle between the current feature and the target center. Afterward, it introduces an additive angular margin to the target angle, and we get the target logit back again by the cosine function. Then, it re-scales all logits by a fixed feature norm, and the subsequent steps are the same as in the softmax loss. Due to the exact correspondence between the angle and arc in the normalized hypersphere, this method can directly optimize the geodesic distance margin, thus it is called as ArcFace.</p> <p></p>"},{"location":"projects/deep_face_recognition/#datasets","title":"Datasets","text":""},{"location":"projects/deep_face_recognition/#training-dataset","title":"Training dataset","text":"<p>For the training, I mixed two datasets by removing overlapping identities using a pretrained Resnet100 model.</p> <ol> <li>MS-Celeb-1M - Around 5.1M images of 93K identities</li> <li>DeepGlint - Around 6.75M images of 181K identities</li> </ol>"},{"location":"projects/deep_face_recognition/#test-dataset","title":"Test dataset","text":"<p>For testing, I used two datasets as below,</p> <ol> <li> <p>LFW Benchmark dataset: LFW is a public benchmark for face verification, also known as pair matching. Here we have 50% positive pairs and 50% negative pairs. We calculate PR curve using cosine similarity as a threshold. Although it is just a verification benchmark that only checks whether pair of images are of the same person or not. It is very difficult to extrapolate from performance on verification to performance on 1:N recognition. Many groups are not well represented in LFW. For example, there are very few children, no babies, very few people over the age of 80, and a relatively small proportion of women. In addition, many ethnicities have very minor representation or none at all.</p> </li> <li> <p>I created a custom evaluation dataset balanced using age, gender, ethnicity, image quality, and lighting. Here I tested both the 1:1 face verification and 1:N face recognition performance of my model for three different image quality pairs.</p> <ul> <li> <p>High vs High: High-quality query images and High-quality database images. It is used for testing best-case scenarios when both enrolled and query images are captured using a high-quality camera.</p> </li> <li> <p>Low vs High: Low-quality query images and High-quality database images. This tests the real-world scenario where enrolled images are mostly captured using good-quality cameras and in the controlled environment in which query images are often captured using low-quality surveillance cameras. This specifically tests the recall of the model as the chances of rejection are high for a low-quality image.</p> </li> <li> <p>Low vs Low: Low-quality query images and Low-quality database images. This specifically tests the precision of the model when we enroll low-quality images in the database to decrease the rejection rate. Chances of false-match are high when image quality is not good for both query and database images.</p> </li> </ul> </li> </ol>"},{"location":"projects/deep_face_recognition/#data-normalization","title":"Data normalization","text":"<p>Empirically it is observed that normalizing face images such that the location of eyes, nose, and lips ends are always almost fixed for the training and evaluation images significantly increases the model's accuracy. For this task, I used a small landmark detection model trained with wing loss based on standard CNN with an input resolution of 96x96 px on the face crop extracted by a face detection model.</p> <p></p>"},{"location":"projects/deep_face_recognition/#data-augmentation","title":"Data augmentation","text":"<p>I used two types of data augmentation techniques for better generalization</p> <ol> <li> <p>Offline data augmentation for pose diversity</p> <ul> <li>It is very hard to collect a wide range of facial poses in the real world for each identity. I used PRNet to generate a 3D depth map from a single-face image and generate different poses for the same as shown below. The final dataset was balanced by poses. </li> </ul> <p> Depth estimation</p> <p> Reconstruct face images with different poses.</p> </li> <li> <p>Online data augmentation for lighting and image quality.</p> <ul> <li> <p>I used imgaug library which provides a range of image augmentation functions that can be randomly applied to an image during training. Some of the transforms I used were,</p> <ul> <li>Fliplr</li> <li>JpegCompression</li> <li>blur</li> <li>AddToHue</li> <li>AddToBrightness</li> </ul> <p> Example of online augmentations. </p> </li> </ul> </li> </ol>"},{"location":"projects/deep_face_recognition/#faster-inference","title":"Faster inference","text":"<p>As I wanted to deploy my model on the edge devices such as raspberry-pi or smartphone SOCs, I used two techniques to increase speed and reduce model size.</p> <ol> <li> <p>Knowledge distillation</p> <p>The model I used for deployment was a custom ResNet50. Training a smaller model from scratch on a large amount of data could lead to an underfitting problem. I designed a knowledge distillation routine in which I transferred the learnings of the pretrained ResNet100 model to the ResNet50. The steps were as shown below.</p> <ol> <li>Train ResNet100 from scratch on the whole dataset</li> <li>Use ResNet100 to precompute outputs of final embedding and intermediate Resnet blocks for the left and right flips of the image.</li> <li>Copy weights of the last fully connected layer from pretrained ResNet100 to the initial version of ResNet50.</li> <li>Train ResNet50 only on the precomputed dataset and add loss for embedding reconstruction and intermediate features reconstruction loss to the arcface loss.</li> </ol> <p>This way the embedding generated using ResNet50 is very similar to ResNet100 and it significantly outperforms vanilla ResNet50 trained from scratch.</p> </li> <li> <p>Full integer quantization using TensorFlow lite.</p> <p>I used TensorFlow lite's post-training quantization module to convert my float32 model to a full int8 model. For the representative dataset, I balanced images by age, gender, pose, ethnicity, and image quality. This reduced model's size by 4x and inference speed by 5x.</p> </li> </ol> <p>With these two techniques, I was able to infer my model within a second on the slowest hardware such as raspberry pi 3 (1.2 GHz quad-core ARM Cortex-A53) by keeping similar accuracy of a full-fledged ResNet100 deployment on GPU servers.</p>"},{"location":"projects/deep_face_recognition/#deployment","title":"Deployment","text":"<p>I used different frameworks to deploy the same model based on hardware specifications. Some of the frameworks I used are listed below. The challenge was to use only supported ops and maintain accuracy after conversion from TensorFlow to custom frameworks. </p> <ul> <li>NVIDIA TensorRT: For GPU-enabled server deployment, I converted my model to TensorRT plan files. This reduced float32 to float16 and significantly improved inference speed.</li> <li>Intel OpenVino: For intel's CPU-based server deployment, I converted my model to OpenVino's intermediate format. This reduced float32 to int8 using inbuilt quantization with a fallback to float32.</li> <li>TensorFlow Lite: For edge devices such as raspberry-pi and smartphones, I used TensorFlow lite to deploy my model.</li> </ul> <p>I used C++ to write a modular backend where image reading, preprocessing and post-processing functions were common. While the actual inference class was an abstract class implemented for every hardware using framework-specific methods. It is easy to compile and distribute models in form of a C++ SDK and then to write separate code for each hardware. </p>"},{"location":"projects/hyperspectral_image_classification/","title":"Hyper Spectral Image Classification","text":""},{"location":"projects/hyperspectral_image_classification/#code","title":"Code","text":"<ul> <li>Hyspeclib Library</li> <li>Code Manual</li> <li>Thesis</li> </ul>"},{"location":"projects/hyperspectral_image_classification/#introduction","title":"Introduction","text":"<p>Hyperspectral imaging which is also known as imaging spectroscopy, detects radiation of earth surface features in narrow contiguous spectral regions of the electromagnetic spectrum. The Airborne Visible Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) is an airborne hyperspectral sensor of NASA\u2019s Jet Propulsion Laboratory (JPL) with 425 spectral bands ranging from 380 nm to 2510 nm with a bandwidth of 5 nm and spatial resolution of 4-6 m. </p> <p></p> <p>This study aims at pixel-wise identification and discrimination of crop types using AVIRIS-NG hyperspectral images, with novel Parallel Convolutional Neural Networks architecture.</p> <p></p> <p></p> <p>To tackle the challenge posed by a large number of correlated bands, we compare two band selection techniques using Principal Component Analysis (PCA) and back traversal of pre-trained Artificial Neural Network (ANN).</p> <ul> <li> <p>Bands selected by PCA method  </p> </li> <li> <p>Bands selected by ANN method </p> </li> </ul> <p>We also propose an automated technique for augmentation of training dataset with a large number of pixels from unlabelled parts of an image, based on Euclidian distance. Experiments show that bands selected by ANN achieve higher accuracy compare to PCA selected bands with automated data augmentation.</p> <p></p>"},{"location":"projects/hyperspectral_image_classification/#paper","title":"Paper","text":"<p>Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques <pre><code>@INPROCEEDINGS{8897897,\n  author={Patel, Hetul and Bhagia, Nita and Vyas, Tarjni and Bhattacharya, Bimal and Dave, Kinjal},\n  booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, \n  title={Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques}, \n  year={2019},\n  volume={},\n  number={},\n  pages={3728-3731},\n  doi={10.1109/IGARSS.2019.8897897}}\n</code></pre></p>"},{"location":"projects/lecture_notes_saving_app/","title":"Fellow (Lecture notes app)","text":"<p>Fellow is an Android app that I built to quickly save my lecture notes into google drive to refer them easily later. Students can quickly click a picture of daily lecture notes for each course. Notes are automatically compiled into a single pdf per course. Some of its feautres are as mentioned below.</p>"},{"location":"projects/lecture_notes_saving_app/#code","title":"Code","text":"<ul> <li>Fellow App</li> </ul>"},{"location":"projects/lecture_notes_saving_app/#login-with-google-drive-account","title":"Login with google drive account","text":"<p>User can login with google account so that all the data is backed-up.</p> <p></p>"},{"location":"projects/lecture_notes_saving_app/#add-semester-wise-courses","title":"Add semester wise courses","text":"<p>Students can add list of courses enrolled in current semester so that notes can be easily organized and accessed quickly in pdf form by clicking on the subject name.</p> <p> </p>"},{"location":"projects/lecture_notes_saving_app/#customise-home-screen","title":"Customise home screen","text":"<p>Students can customize the homescreen by adding the weekly schedule of lectures. The home screen will then only show courses planned for the given day. User can quickly add photos of lecture notes for the given course by clicking on the upload button. </p> <p> </p>"},{"location":"projects/lecture_notes_saving_app/#navigate-easily","title":"Navigate easily","text":"<p>Access all the features from the stylish drawer navigation.</p> <p></p>"},{"location":"projects/mind_controlled_home_automation/","title":"Mind Controlled Home Automation","text":""},{"location":"projects/mind_controlled_home_automation/#introduction","title":"Introduction","text":"<p>Mind Controlled Home Automation System with self-learning capability \u2013 an approach to enter the era of virtual reality, an era where everything is possible. That\u2019s why we named it as SAMBHAV. The main aim of our project is to control home appliances with human brain acting as an interface. This means that the signals generated by our brain will now control electrical appliances \u2013 termed as Home Automation. For achieving the same, we have used MindWave headset that calculates our concentration(attention) and meditation(relaxation) and even detects the blink. The combination of these human activities will result into switching ON/OFF devices. </p>"},{"location":"projects/mind_controlled_home_automation/#about-headset","title":"About Headset","text":"<p>A closer look at the headset:</p> <p></p>"},{"location":"projects/mind_controlled_home_automation/#basic-functionalities-of-headset","title":"Basic functionalities of headset:","text":"<ul> <li> <p>Power Switch \u2013 To power ON/OFF the headset. If the switch is held past the ON position for 3 seconds, the headset will enter the Bluetooth Pairing Mode and if held past the ON position for 6 seconds, the headset\u2019s pairing memory will be cleared.</p> </li> <li> <p>Ear Clip \u2013 Acts as the reference point and is clipped at the earlobe (lower part of the ear).</p> </li> <li> <p>Sensor Arm \u2013 Core of the headset. It contains the sensor that should be in direct contact with the forehead. Besides sensor, it also includes the on-board chip that processes all of the data.</p> </li> </ul>"},{"location":"projects/mind_controlled_home_automation/#working","title":"Working","text":"<p>The biosensor placed inside the headset measures the frequencies of the electrical signals emitted by the nerve cells. Although neurons are not so good conductor of electricity, yet they generate electrical signals based on the flow of ions across their plasma membranes. Basically, neurons generate a negative potential, that is measured by recording the voltage between the inside and outside of nerve cells. The sensor chip does this work with ear clip acting as the reference point and the electrical signals measured are commonly referred to as brainwaves. The sensor chip then amplifies the brainwave signals and removes the unnecessary noise and muscle movement. These analog signals are then converted into digital signals by the on-board chip and is being processed to the device.</p> <p>The table below shows some of the frequencies that tend to be generated by different types of activity in the brain:</p> Brainwave Type Frequency range Mental states and conditions Delta 0.1Hz to 3Hz Deep, dreamless sleep, non-REM sleep, unconscious Theta 4Hz to 7Hz Intuitive, creative, recall, fantasy, imaginary, dream Alpha 8Hz to 12Hz Relaxed (but not drowsy) tranquil, conscious Low Beta 12Hz to 15Hz Formerly SMR, relaxed yet focused, integrated Midrange Beta 12Hz to 15Hz Thinking, aware of self &amp; surroundings High Beta 21Hz to 30Hz Alertness, agitation"},{"location":"projects/mind_controlled_home_automation/#android-app-for-analysis","title":"Android App for analysis","text":"<p>We created an android app for analyzing various signals generated by our brain on pursuing different activities. App is used to display different EEG band power values such as Delta, Theta, Alpha and Gamma etc.</p> <p></p> <p>At the top of the screen, Graph is displayed based on this values and a sudden change in amplitude is detected on blinking. The Attention and Meditation values are also displayed. This app is connected to mindwave headset via Bluetooth. When user go beyond the meditation value of 85, Flash light of the mobile phone automatically gets turned on.</p>"},{"location":"projects/mind_controlled_home_automation/#controlling-light-by-mind-circuit-diagram","title":"Controlling Light by Mind : Circuit Diagram","text":""},{"location":"projects/mind_controlled_home_automation/#main-components-of-the-circuit-are","title":"Main components of the circuit are:","text":"<ol> <li>Arduino Uno R3</li> <li>Bluetooth Module HC \u2013 05</li> <li>4 Channel Relay Module</li> <li>Bulb</li> <li>Power Supply</li> </ol>"},{"location":"projects/mind_controlled_home_automation/#hc-05-module-has-3-pins-in-use","title":"HC-05 Module has 3 pins in use:","text":"<ol> <li>VCC \u2013 Connected to Digital Pin 5 of Arduino (5 Volt Supply)</li> <li>GND \u2013 Connected to Ground Pin of Arduino</li> <li>Tx \u2013 Transmitter Pin is connected to RX pin of Arduino</li> </ol>"},{"location":"projects/mind_controlled_home_automation/#relay-module-has-5-pins-in-use","title":"Relay module has 5 pins in use:","text":"<ol> <li>VCC \u2013 Connected to VCC of Arduino (5 Volt Supply)</li> <li>GND- Connected to Ground Pin of Arduino</li> <li>IN2 \u2013 As an input pin for relay 2 since we have connected bulb to relay 2. This pin is connected to Digital Pin no. 2 of Arduino.</li> <li>C \u2013 It is common pin of relay connected to Hot line of 250 V Ac supply.</li> <li>NO- Normally open pin is connected to Positive terminal of bulb.</li> </ol> <p>Negative terminal of Bulb is connected to Neutral line of power supply.</p> <p>Mind wave Head set is wirelessly connected to Arduino via Bluetooth connection.</p>"},{"location":"projects/mind_controlled_home_automation/#setting-up-bluetooth-module","title":"Setting up Bluetooth module","text":"<p>HC-05 by default can be paired to any device. But for our project we specifically need to pair the module with mind wave headset. For that We first turned on our module in COMMAND MODE by holding a push button provided on top of HC-05. </p> <p>To send AT commands to HC-05, we coded the Arduino as shown below and connected the module to it.</p> <p></p>"},{"location":"projects/mind_controlled_home_automation/#at-commands-for-setting-up-module-are-listed-below","title":"AT commands for setting up module are listed below:","text":"<ol> <li> <p>AT+NAME=SAMBHAV: Sets name of Bluetooth device to \u201cSambhav\u201d</p> </li> <li> <p>AT+UART=57600,0,0: Sets Baud rate to 57600 as mindwave sends data at this rate.</p> </li> <li> <p>AT+PSWD=0000: \u201c0000\u201d is default password for pairing up with mindwave head set</p> </li> <li> <p>AT+BIND=MMM,YY,NNNNN: We mentioned the Unique Identifier Number of Our Headset here which we get from device properties of headset.</p> </li> </ol>"},{"location":"projects/mind_controlled_home_automation/#algorithm","title":"Algorithm","text":"<p>Once the connection is established, mind wave sends the data in 32-byte packet. We need to retrieve the data from packet. Based on the attention and meditation value of mind we can send signal to relay for controlling light.</p> <p>Packet is described here.</p> Byte Value Explanation 0 0xAA [SYNC] Synchronisation bit 1 0xAA [SYNC] Synchronisation bit 2 0x08 (payload length) of 8 bytes 3 0x02 POOR_SIGNAL Quality 4 0x20 Some poor signal detected (32/255) 5 0x01 BATTERY Level 6 0x7E Almost full 3V of battery (126/127) 7 0x04 ATTENTION 8 0x12 Attention level of 18% 9 0x05 MEDITATION 10 0x60 Meditation level of 96% 11 0xE3 [CHKSUM] (1's comp inverse of 8-bit Payload sum of 0x1C)"},{"location":"projects/mind_controlled_home_automation/#parsing-a-packet","title":"Parsing a Packet","text":"<ol> <li> <p>Keep reading bytes from the stream until a [SYNC] byte (0xAA) is encountered.</p> </li> <li> <p>Read the next byte and ensure it is also a [SYNC] byte</p> <ul> <li>If not a [SYNC] byte, return to step 1.</li> <li>Otherwise, continue to step 3.</li> </ul> </li> <li> <p>Read the next byte from the stream as the [PLENGTH].</p> <ul> <li>If [PLENGTH] is 170 ([SYNC]), then repeat step 3.</li> <li>If [PLENGTH] is greater than 170, then return to step 1 (PLENGTH TOO LARGE).</li> <li>Otherwise, continue to step 4.</li> </ul> </li> <li> <p>Read the next [PLENGTH] bytes of the [PAYLOAD\u2026] from the stream, saving them into a storage area (such as an unsigned char payload [256] array). Sum up each byte as it is read by incrementing a checksum accumulator (checksum += byte).</p> </li> <li> <p>Take the lowest 8 bits of the checksum accumulator and invert them. Here is the C code:</p> </li> </ol> <pre><code>checksum &amp;= 0xFF;\nchecksum = ~checksum &amp; 0xFF; \n</code></pre> <ol> <li> <p>Read the next byte from the stream as the [CHKSUM] byte.</p> <ul> <li>If the [CHKSUM] does not match your calculated chksum (CHKSUM FAILED).</li> <li>Otherwise, you may now parse the contents of the Payload into DataRows to obtain the Data Values, as described below.</li> <li>In either case, return to step 1. </li> </ul> </li> </ol>"},{"location":"projects/mind_controlled_home_automation/#parsing-data-rows-in-a-packet-payload","title":"Parsing Data Rows in a Packet Payload","text":"<p>Repeat the following steps for parsing a DataRow until all bytes in the payload[] array ([PLENGTH] bytes) have been considered and parsed:</p> <ol> <li>Parse and count the number of [EXCODE] (0x55) bytes that may be at the beginning of the current DataRow.</li> <li>Parse the [CODE] byte for the current DataRow.</li> <li>If [CODE] &gt;= 0x80, parse the next byte as the [VLENGTH] byte for the current DataRow.</li> <li>Parse and handle the [VALUE\u2026] byte(s) of the current DataRow, based on the DataRow's [EXCODE] level, [CODE], and [VLENGTH].</li> <li>If not all bytes have been parsed from the payload[] array, return to step 1. to continue parsing the next DataRow.</li> <li>Get Attention Value from [EXCODE] 0x04</li> <li>Get Meditation Value from [EXCODE] 0x05</li> <li>If Attention is greater than threshold, then turn on light else go to step 9.</li> <li>If Meditation is greater than threshold, then turn off light else go to step 1.</li> </ol>"},{"location":"projects/mind_controlled_home_automation/#references","title":"References","text":"<ol> <li>Documentation and algorithm for mindwave headset</li> </ol>"},{"location":"projects/on_device_classification/","title":"On-Device Object Classification","text":""},{"location":"projects/on_device_classification/#transfer-learning","title":"Transfer Learning","text":"<p>Transfer learning is the technique for image classification where we use a pre-trained model trained on large-scale datasets (such as ImageNet) and adapt it for our small number of classes by only retraining the fully connected layer of the model from scratch. </p> <p>Generally, training models require powerful CPUs or GPUs. But since the transfer learning technique only requires a small amount of computing to retrain a single layer, we can utilize CPUs/GPUs of modern smartphone devices. That means users can train image classification models on their smartphones on the fly with only as few as 10 images per class. </p> <p></p>"},{"location":"projects/on_device_classification/#inspiration","title":"Inspiration","text":"<p>I found this interesting blog in Example on-device model personalization with TensorFlow Lite  which does something similar. The app uses transfer learning on a quantized MobileNetV2 model pre-trained on ImageNet with the last few layers replaced by a trainable softmax classifier. You can train the last layers to recognize any four new classes.</p> <p></p> <p>Since the app was entirely written in java, I redesigned the backend of the app such that the core logic is wrapped inside in a C++ SDK. This SDK is portable such that it can be used on any hardware and os which supports c++ program execution.</p>"},{"location":"projects/on_device_classification/#workflow","title":"Workflow","text":"<p>The below image explains the workflow of how this app works.</p> <p></p>"},{"location":"projects/on_device_classification/#apis","title":"APIs","text":"<p>The C++ library exposes three functions that any app can use for building on-device classification apps.</p>"},{"location":"projects/on_device_classification/#1-add-an-image","title":"1. Add an image","text":"<p>Use can add an image and label to the training set. Internally we store the output of the bottleneck layer generated from the pretrained model.</p>"},{"location":"projects/on_device_classification/#2-train-model","title":"2. Train model","text":"<p>Randomly shuffles all the embeddings and re-trains the last fully connected layer.</p>"},{"location":"projects/on_device_classification/#3-predict-an-image","title":"3. Predict an image","text":"<p>Generates embedding using a pre-trained model and multiplies it with weights of the finetuned fc layer and outputs the final result. </p>"},{"location":"projects/on_device_classification/#models","title":"Models","text":"<p>There are five different TensorflowLite models to execute the entire flow.</p>"},{"location":"projects/on_device_classification/#1-bottlenecktflite-base-model","title":"1. bottleneck.tflite (Base model)","text":"<p>Pretrained mobilenet model without the last fully connected layer. It is used for extracting learned features from the input image. For mobile CPUs, we use the fully quantized int8 model and for mobile GPUs, we use the fp16 model.</p> <p></p>"},{"location":"projects/on_device_classification/#3-initializetflite","title":"3. initialize.tflite","text":"<p>To train our model on custom datasets, the library maintains a weights and biases array in the memory. The <code>initialize.tflite</code> model outputs these two matrices initialized with zeros.</p> <p></p>"},{"location":"projects/on_device_classification/#2-train_headtflite","title":"2. train_head.tflite","text":"<p>During the training phase, <code>train_head.tflite</code> model takes the output of the base model, current weights, current biases, and the ground truth labels. It then calculates the loss and returns gradients of the weights and biases w.r.t the loss. Those gradients are passed to the optimizer model to apply updates. </p> <p></p>"},{"location":"projects/on_device_classification/#4-optimizertflite","title":"4. optimizer.tflite","text":"<p>The optimizer model takes current weights and biases and gradients of weights and biases to apply a gradient descent algorithm (<code>new_weights = old_weights - learning_rate * gradients</code>) to calculate new weights and biases. We perform this step after each invocation of the <code>train_head.tflite</code> model during the training phase. The library replaces current parameters with the updated parameters in the memory.</p> <p></p>"},{"location":"projects/on_device_classification/#5-inferencetflite-head-model","title":"5. inference.tflite (Head model)","text":"<p>The inference model takes three inputs, the output of the bottleneck model, the trained weights matrix, and the trained bias values. It calculates the final probability using the <code>softmax(w*x + b)</code> formula.</p> <p></p>"},{"location":"projects/on_device_classification/#extending-to-object-detection","title":"Extending to object detection","text":"<p>I extended this idea to object detection by using a bottleneck layer of mobilenet-ssd model trained on the COCO dataset and finetuning the bounding box and class prediction layers on the device itself. The primary results were good if there is very change in the test image compared to the training image, but it required more images for the model to produce good results when the test image is taken under different conditions. Though this can be a good future work.  </p>"},{"location":"projects/spry/","title":"Hospital Management System","text":"<p>Spry was a hobby project I created for my family doctor. It provides some helpful features for the doctors to manage their patients' data through this web app.</p>"},{"location":"projects/spry/#responsive-dashboard","title":"Responsive Dashboard","text":""},{"location":"projects/spry/#list-of-features","title":"List of features","text":""},{"location":"projects/spry/#1-add-symptoms-and-treatments","title":"1. Add symptoms and treatments","text":""},{"location":"projects/spry/#2-track-patients-blood-pressure-history","title":"2. Track patient's blood pressure history.","text":""},{"location":"projects/spry/#3-track-paid-and-unpaid-amount","title":"3. Track paid and unpaid amount.","text":""},{"location":"projects/spry/#4-create-unique-patient-profile","title":"4. Create unique patient profile","text":""},{"location":"projects/spry/#5-check-patients-medical-history","title":"5. Check patient's medical history.","text":""},{"location":"projects/spry/#6-search-patients-by-multiple-fields","title":"6. Search patients by multiple fields.","text":""},{"location":"projects/spry/#7-real-time-sync-with-cloud","title":"7. Real time sync with cloud","text":""}]}