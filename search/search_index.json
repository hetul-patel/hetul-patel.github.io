{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hello! \ud83d\udc4b \u00b6 I build AI that helps someone somewhere everyday.","title":"Hello! \ud83d\udc4b"},{"location":"#hello","text":"I build AI that helps someone somewhere everyday.","title":"Hello! \ud83d\udc4b"},{"location":"contact/","text":"I am, Somewhere out there. Some wonderful place on our planet earth. Wandering and Wondering. You can write to me at, hetulvp@gmail.com Or connect via the digital universe. GitHub LinkedIn Instagram","title":"Contact"},{"location":"education/","text":"Nirma University Aug, 2014 - May, 2018 Institute of technology, Nirma University, Ahmedabad, India. Bachelor of Technology in Computer Science and Engineering CGPA: 8.12 / 10.0 Gujarat Secondary and Higher Secondary Education Board April, 2014 Utkarsh vidyalaya, Vadodara, India. Higher Secondary School Certificate Percentile: 99.87 Gujarat Secondary and Higher Secondary Education Board March, 2012 Bright School, Vadodara, India. Secondary School Certificate Percentile: 99.98","title":"Education"},{"location":"experience/","text":"Machine Learning Engineer - III Feb, 2021 - Present InFoCusp Innovations Private Limited , Ahmedabad, India. Client: Google-X, the Moonshot Factory (Confidential Research Project) Neural Machine Translation Developing transformer-based models for program representations and using them for code completion, code repair, and code translation. Developed fully automated and hybrid ML pipelines for data engineering, modeling, evaluation, and deployment that can work on both google's internal infra and google cloud platforms. Some of the frameworks I used were TensorFlow, Kubeflow, KFserving, and Apache Beam. Computer vision The goal was to create a single virtualized view of the electricity system through an aerial view imaginary using an image segmentation model. I explored various state-of-the-art image segmentation models such as PSP-NET, ViT, etc., and combined the best of all in a single model to achieve production-grade results. I also developed a novel metric for the evaluation of the model's performance as traditional segmentation metrics failed to provide helpful indications of failure cases. While the new metric helped filter failure cases easily which was then improved through data augmentation techniques. GRAD-CAM and its variant were used to locate features in the image when the model failed to produce the correct output. This analysis helped improve the quality of the training and evaluation dataset. Client: Innovyze, An Autodesk company. At Innovyze, I helped Process Engineers to optimize chemical consumption in the water treatment plant by analyzing historical sensor data and developing predictive models to automate the processes. My primary tasks were Data analysis, Feature engineering, and Modelling. Software Development Engineer - II (AI/ML) May, 2018 - Feb, 2021 Matrix Comsec R&D , Vadodara, India. Matrix ComSec is a leader in Security and Telecom solutions for modern businesses and enterprises with 1M+ customers in 50+ countries. I was responsible for developing and delivering DL algorithms in SDK form. My major contributions are listed below, Face Recognition (FR) and Face Detection (FD) Developed FD algorithm to detect multiple faces in the image with a minimum face size of 30px. Developed FR algorithm with 99.85 % accuracy on the LFW benchmark. Improved existing FR algorithm to identify people wearing a mask on the face with only 5% accuracy drop wrt to full-face model. It helped employees to mark attendance without removing their masks during the coronavirus pandemic Developed a Face Mask Detection model to identify whether employees are wearing masks or not when marking attendance to improve their safety. Developed CNN for a single RGB image-based Passive Face Antispoofing model to prevent fake attendance marking using a mobile phone or printed photo. Delivered all these features in a single FR SDK for RPi3/4, CPU, GPU, and Android devices with a maximum latency of 800 ms on the slowest hardware Automated License Plate Recognition (ALPR) Developed a real-time Licence Plate Detection model which achieved 99%+ recall and 93%+ precision on the internal Indian vehicle test dataset Contributed to the development of the CNN model for Licence Plate Recognition using CTC Loss. We achieved 87% accuracy on challenging the Indian test datasets outperforming other commercial solutions by at least 15% and 98% accuracy on the OpenALPR benchmark Developed semi-supervised data annotation tool which helped us collect a labeled dataset of 30K+ images and incrementally improved the model's accuracy Designed ALPR SDK architecture for improving throughput by batching simultaneous requests into single inference. A myriad of inference engines such as TensorFlow, TensorRT, OpenVINO, TensorflowLite, etc. can be integrated based on hardware without modifying 95% of the codebase. Seven Segment Display Number Recognition To handle a variety of color digital displays, a self-calibration algorithm was designed to run once to extract display-specific properties using KNN and an inference algorithm was designed to use these properties for recognizing digits in real-time using SVM. The algorithm was integrated into existing weighbridge vehicle management software for automating data entry of the vehicle's weight from the digital display through the camera. It reduced manual human intervention by 95% and introduced transparency in the whole process. Deep Learning Research Intern Jan, 2018 - May, 2018 Space Applications Centre (SAC) - ISRO, Ahmedabad, Gujarat, India SAC being the major R&D center of ISRO designs and develops the optical and microwave sensors for the satellites, signal and image processing software, GIS software, and many applications for the Earth Observation (EO) program of ISRO. As a Research Intern, I developed DL techniques for accurate crop classification using Hyper Spectral Satellite images (425 channels per pixel). My primary contributions were as listed below Developed ANN-based dimensionality reduction technique which retained useful features and worked better than PCA. Developed FR algorithm with 99.85 % accuracy on the LFW benchmark. Developed virtual data augmentation technique for overcoming the issue of insufficient ground truth data of different crops for training. Provided detailed comparative analysis of ANN, CNN, and SVM. Designed highly accurate Parallel-CNNs architecture for classifying nearly inseparable classes based on interclass separability analysis. Developed Python library based on my work so that other scientists can use these techniques on a variety of other satellite images. Machine Learning Summer Intern May, 2017 - June, 2017 Wolfsoft Pvt. Ltd., Vadodara, Gujarat, India. During my internship I worked on a food review app similar to Zomato and my contributions were as listed below, Developed a Food Recommendation API based on a Collaborative Filtering algorithm. Developed a Food Search API. Normalized and stored posting list of Indian food names in the Trie data structure for faster and more accurate search. Added spelling correction using the Levenshtein distance algorithm which achieved higher recall compared to SQL's search.","title":"Experience"},{"location":"publications/","text":"Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques @INPROCEEDINGS{8897897, author={Patel, Hetul and Bhagia, Nita and Vyas, Tarjni and Bhattacharya, Bimal and Dave, Kinjal}, booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, title={Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques}, year={2019}, volume={}, number={}, pages={3728-3731}, doi={10.1109/IGARSS.2019.8897897}}","title":"Publications"},{"location":"talks/","text":"Talks/Seminars \u00b6 I love sharing my learnings from the ML industry with others through invited lectures or talks. All the slides of my talks/sessions can be found below. Spoke on \"Introduction Accelerators and NVIDIA RAPIDS for Datascience on GPU\" where I explained concepts such as working of GPUs, TPUs, and hands-on on cuDF and cuML libraries at Machine Learning Ahmedabad Meetup, Infocusp, Ahmedabad , 24 Dec, 2022. Spoke on \"Introduction to Deep Learning with ANN\" where I explained concepts such as MLP, backpropagation, gradient descent with hands-on implementation from scratch in NumPy at Machine Learning Summer School'22, DAIICT, Gandhinagar , 18 June, 2022. Spoke on \"Advance application of deep learning for computer vision\" where I explained working of object detection , semantic segmentation etc. Also explained how to design models for different data modalities such as text, sound and vision etc. at Machine Learning Summer School'22, DAIICT, Gandhinagar , 19 June, 2022. Expert session on \"Transformers for NLP with hands on applications\" at Nirma University, Ahmedabad , 25 March, 2022. Expert session on Industry expectations from students in the field of deep learning at Nirma University, Ahmedabad , 07 March, 2022. Expert session on Importance of Maths for Machine Learning at Nirma University, Ahmedabad , 20 November, 2021.","title":"Talks/Seminars"},{"location":"talks/#talksseminars","text":"I love sharing my learnings from the ML industry with others through invited lectures or talks. All the slides of my talks/sessions can be found below. Spoke on \"Introduction Accelerators and NVIDIA RAPIDS for Datascience on GPU\" where I explained concepts such as working of GPUs, TPUs, and hands-on on cuDF and cuML libraries at Machine Learning Ahmedabad Meetup, Infocusp, Ahmedabad , 24 Dec, 2022. Spoke on \"Introduction to Deep Learning with ANN\" where I explained concepts such as MLP, backpropagation, gradient descent with hands-on implementation from scratch in NumPy at Machine Learning Summer School'22, DAIICT, Gandhinagar , 18 June, 2022. Spoke on \"Advance application of deep learning for computer vision\" where I explained working of object detection , semantic segmentation etc. Also explained how to design models for different data modalities such as text, sound and vision etc. at Machine Learning Summer School'22, DAIICT, Gandhinagar , 19 June, 2022. Expert session on \"Transformers for NLP with hands on applications\" at Nirma University, Ahmedabad , 25 March, 2022. Expert session on Industry expectations from students in the field of deep learning at Nirma University, Ahmedabad , 07 March, 2022. Expert session on Importance of Maths for Machine Learning at Nirma University, Ahmedabad , 20 November, 2021.","title":"Talks/Seminars"},{"location":"projects/3d_password/","text":"3D Password \u00b6 Introduction \u00b6 3D password is an advancement in the field of user authentication techniques. 3d password can have large number of possible combinations as integration of textual, graphical and biometric. So it is technically very hard to crack the one. 3d password is setup in any computer screen which includes a virtual environment where user can set different objects as password. Below is the list object which can be used as 3D password, An Iris Scanner A Fingerprint Scanner A Virtual ATM machine that requires real ATM card A text input device which take input Photo album as pictorial password So any real world object can be integrated in the 3D environment and any new authentication technique can also be included if it is invented after. Working of 3d environment \u00b6 Every object in a 3D environment has its unique (x,y,z) coordinates. If the size of the 3D world is M x M x M then the entire virtual world can be represented by [1,2,3......, M] x [1,2,3......, M] x [1,2,3......, M] . User can move in the 3d environment using mouse and keyboard. Also biometric scanners, stylus pen and ATM card reader can be attached for interactions with virtual models of it. The coordinates are stored when any interaction is performed with the object. E.g. user opens a door located at (10,15,60) , He closes the door and enters the office. Take a marker located at (25,20,70) and write \u201cHello\u201d on the white board located at (25,52,80) . This interaction is stored as 3D password as follows: (10,15,60) Object: door Action: Opening the door, (10,15,60) Object: door Action: Closing the door, (25,20,70) Object: marker Action: Take the marker, (25,52,80) Object: white board Action: Writing \u201cH\u201d, (25,52,80) Object: white board Action: Writing \u201cE\u201d, (25,52,80) Object: white board Action: Writing \u201cL\u201d, (25,52,80) Object: white board Action: Writing \u201cL\u201d, (25,52,80) Object: white board Action: Writing \u201cO\u201d. Implementing 3D Password \u00b6 3D password implementation requires the integration of software and hardware. 3D objects can be designed in 3D modelling software such as Adobe Maya, Blender etc. This 3D objects are imported in a 3D world which can be created in gave development software like UNITY 3D. The Serial Input and Output can be driven through USB port. I have implemented small 3D Password using UNITY 3D. The steps are as follows: Step 1: Read all the instructions carefully. \u00b6 Figure 4.1 shows the instruction to be followed to travel in the 3D world. Use Keyboard direction keys to move in linear direction and mouse to move in angular direction. Step 2: Open the Right or Left gate to enter into the Nirma\u2019s Gate. \u00b6 As shown in figure 4.2 You will see the entrance gate of Nirma. User has to go Security guard and use the mouse to click on LEFT or RIGHT button to open the left or right gate respectively. As The password is predefined here I\u2019ll open the LEFT gate as shown in fig 4.3 and enter into the college. Step 3: Go to correct colony and choose correct photo frame. \u00b6 As shown in figure 4.4, You will see 3 identical colonies with same photo frames hanged on the walls. The difference is that each colony has different number of trees near it. We will go first the colony which has 3 trees near it As shown in figure 4.5 there are 4 photo frames hanged on the wall from the left Steve jobs Mahatama Gandhi Karsanbhai Patel Bill Gates I will select the Mahatama Gandhi photo frame by clicking on it. The photo will come forward as selected as shown in figure 4.6 Then I\u2019ll go to the another colony with two trees. And select the Karsanbhai patel\u2019s photo frame as it is my predefined password. Step 4: Click the login button to get the access. \u00b6 After performing the correct sequence of interactions, I will go to the last stage of my password where I click the login button as shown in figure 4.8. Now If all the actions are correctly performed, I will be logged into the Nirma University System as show in figure 4.9. Else it will show me an Invalid password error as shown in figure 4.10.","title":"3D Password"},{"location":"projects/3d_password/#3d-password","text":"","title":"3D Password"},{"location":"projects/3d_password/#introduction","text":"3D password is an advancement in the field of user authentication techniques. 3d password can have large number of possible combinations as integration of textual, graphical and biometric. So it is technically very hard to crack the one. 3d password is setup in any computer screen which includes a virtual environment where user can set different objects as password. Below is the list object which can be used as 3D password, An Iris Scanner A Fingerprint Scanner A Virtual ATM machine that requires real ATM card A text input device which take input Photo album as pictorial password So any real world object can be integrated in the 3D environment and any new authentication technique can also be included if it is invented after.","title":"Introduction"},{"location":"projects/3d_password/#working-of-3d-environment","text":"Every object in a 3D environment has its unique (x,y,z) coordinates. If the size of the 3D world is M x M x M then the entire virtual world can be represented by [1,2,3......, M] x [1,2,3......, M] x [1,2,3......, M] . User can move in the 3d environment using mouse and keyboard. Also biometric scanners, stylus pen and ATM card reader can be attached for interactions with virtual models of it. The coordinates are stored when any interaction is performed with the object. E.g. user opens a door located at (10,15,60) , He closes the door and enters the office. Take a marker located at (25,20,70) and write \u201cHello\u201d on the white board located at (25,52,80) . This interaction is stored as 3D password as follows: (10,15,60) Object: door Action: Opening the door, (10,15,60) Object: door Action: Closing the door, (25,20,70) Object: marker Action: Take the marker, (25,52,80) Object: white board Action: Writing \u201cH\u201d, (25,52,80) Object: white board Action: Writing \u201cE\u201d, (25,52,80) Object: white board Action: Writing \u201cL\u201d, (25,52,80) Object: white board Action: Writing \u201cL\u201d, (25,52,80) Object: white board Action: Writing \u201cO\u201d.","title":"Working of 3d environment"},{"location":"projects/3d_password/#implementing-3d-password","text":"3D password implementation requires the integration of software and hardware. 3D objects can be designed in 3D modelling software such as Adobe Maya, Blender etc. This 3D objects are imported in a 3D world which can be created in gave development software like UNITY 3D. The Serial Input and Output can be driven through USB port. I have implemented small 3D Password using UNITY 3D. The steps are as follows:","title":"Implementing 3D Password"},{"location":"projects/3d_password/#step-1-read-all-the-instructions-carefully","text":"Figure 4.1 shows the instruction to be followed to travel in the 3D world. Use Keyboard direction keys to move in linear direction and mouse to move in angular direction.","title":"Step 1: Read all the instructions carefully."},{"location":"projects/3d_password/#step-2-open-the-right-or-left-gate-to-enter-into-the-nirmas-gate","text":"As shown in figure 4.2 You will see the entrance gate of Nirma. User has to go Security guard and use the mouse to click on LEFT or RIGHT button to open the left or right gate respectively. As The password is predefined here I\u2019ll open the LEFT gate as shown in fig 4.3 and enter into the college.","title":"Step 2: Open the Right or Left gate to enter into the Nirma\u2019s Gate."},{"location":"projects/3d_password/#step-3-go-to-correct-colony-and-choose-correct-photo-frame","text":"As shown in figure 4.4, You will see 3 identical colonies with same photo frames hanged on the walls. The difference is that each colony has different number of trees near it. We will go first the colony which has 3 trees near it As shown in figure 4.5 there are 4 photo frames hanged on the wall from the left Steve jobs Mahatama Gandhi Karsanbhai Patel Bill Gates I will select the Mahatama Gandhi photo frame by clicking on it. The photo will come forward as selected as shown in figure 4.6 Then I\u2019ll go to the another colony with two trees. And select the Karsanbhai patel\u2019s photo frame as it is my predefined password.","title":"Step 3: Go to correct colony and choose correct photo frame."},{"location":"projects/3d_password/#step-4-click-the-login-button-to-get-the-access","text":"After performing the correct sequence of interactions, I will go to the last stage of my password where I click the login button as shown in figure 4.8. Now If all the actions are correctly performed, I will be logged into the Nirma University System as show in figure 4.9. Else it will show me an Invalid password error as shown in figure 4.10.","title":"Step 4: Click the login button to get the access."},{"location":"projects/alpr/","text":"Automatic License Plate Recognition \u00b6 Introduction \u00b6 ALPR/ANPR is an algorithm to recognize the digits of a vehicle number plate using its image. Even though this problem seems a simple optical character recognition task, many traditional solutions fail to achieve good results in real-world conditions as shown below. I developed a custom Deep Learning based solution that not only works with these cases but is also fast enough to deploy on edge devices. I broke down this task into two subtasks, license plate detection, and recognition. License Plate Detection \u00b6 In the first phase, we need an object detection model which can retrieve the bounding box coordinates of a license plate in the input image. Dataset Collection \u00b6 Since the challenge was to build this model for real-world images, I needed a dataset that has variations in vehicle type, plate type, location of plate, size, shape, etc. I collected a large number of vehicle images and hand-annotated bounding boxed for license plates. Some of the sources I used for collecting data are listed below, OpenSource Datasets : I merged all available open-source plate detection datasets which I found such as Cars , CCPD etc. into a large collection of the labeled dataset. Active learning : I trained my initial model on all the readily available annotated images. Then used this model to generate labels for unknown images and corrected the annotations manually. This way I was able to label a large number of unlabelled images quickly. For active learning, I developed a custom annotation tool using javascript to adjust bounding boxes. Some of the unlabelled datasets used for active learning are as follows, Google images : Vehicle images with visible license plates from google search. Youtube videos : A large number of channels live stream traffic or street view cameras 24/7 on YouTube. I collected a large number of frames containing vehicles by running a vehicle detection model on it. India Driving Dataset : Around 10k Indian street view images captured from a front-facing camera attached to a car. Modeling \u00b6 I used tensorflow-yolov3 repo to train my custom yolov3 model. YoloV3 is a single-shot detector that can detect objects of multiple scales in a single pass. I made a couple of modifications as per my use-case as below, Since I only needed to predict a single class, I reduced the number of channels for each layer by at least 8x. This way I was able to fit my model on edge devices. I used multiscale training where instead of training the model on a fixed input resolution, we choose a random input resolution for each batch. e.g I trained my model on batches varying in input resolutions from 128x128, 256x256, 512x512, and 768x768 pixels. This way I could use the same model for different input sizes based on latency requirements as higher resolution takes more inference time. For the prediction layer, I used the encoding method proposed by the SSD paper since the TensorFlow-lite library has an inbuilt Op called TFLite_Detection_PostProcess for SSD style decoding and NMS, and thus it was easy to convert the entire model to tflite format later for edge deployment. For bounding box regression I used Generelized IOU loss instead of standard box regression and for classification, I used the Focal Loss to tackle the class imbalance problem. For data augmentation, I used imgaug library which preserves the location of the bounding box after applying affine transformations. Apart from standard augmentations, I designed a scale-oriented augmentation in which we choose a random bounding box from the image and randomly crop 10-90% image around it. This way we make sure our model sees all the different scales of the license plate. Polygon detection \u00b6 Our license plate detection model only predicts rectangular bounding boxes. For recognition, we need to correct issues like rotation and skewness using a perspective transformation as shown below. For this, we need the coordinates of the 4 corners. Dataset Collection \u00b6 I used CCPD dataset as it contains polygon annotations for license plates. The CCPD only contained blue color Chinese number plates. To generalize better, I replaced the original plate images with other country number plates using OpenCV's perspective transform . Modeling \u00b6 I trained a very small CNN model which directly regresses these 8 values (x and y coordinates of all the 4 points) given a crop of license plates. Additionally, I predicted the probability of the license plate filtering out false positives. I used wing loss instead of standard MSE loss to weigh hard examples more compared to the easier ones. License Plate Recognition \u00b6 After detecting and correcting the angle and skewness of the license plate, in the third stage, we need to extract the license plate number from the crop. The license plates can be single lines or multi-line. Also, the variability in lighting conditions, background noise, fonts and font size, etc. results in very poor recognition accuracy for standard image processing-based approaches. So I trained a model which can predict both single and multiline license plates directly from the RGB image crop. Dataset collection \u00b6 Similar to the detection task, I started with CCPD and other open-source datasets which contained string-level annotations for plate images. Then used an active learning technique to iteratively predict and refine labels for unlabelled images to increase the dataset size. With active learning, I was able to collect around 30k labeled license plate crops. The problem with this data was the class imbalance at the character level. Since the model does not see all the characters equally, it misclassified low-frequency characters often. E.g. 0<->8, 1<->7 etc. To overcome this issue, I synthetically generated license plate images with different backgrounds and styles keeping the total frequency of each character equal across the entire dataset. I used this data to train the model from scratch and then finetuned it with real data which increased the overall accuracy by more than 20%. I also designed a 3D version of an Indian license plate in a blender similar to as shown below and rendered different views of it by changing characters for every render using python and blender integration. This way I was able to improve the model's accuracy on country-specific plates. Modeling \u00b6 I designed a custom CNN + CTC decoder based model for recognition. The input of the model was semi-rectangular (width 128 and height 256) such that we can fit both single and multiline number plates in the input placeholder padded by zeros. The model extracts features from the upper and lower image and then concatenates both in a single row such that we have an output of shape (H:1, W:64, C:37). Channels refer to the number of possible characters for each position and 64 is the maximum string length the model can predict. CTC loss was used to directly train the model on unaligned image-and-text pairs. The model learns to output one character for every 64 positions such that the final string matches our desired string by predicting other characters as blank. During inference, the CTC beam search decoder was used for better accuracy. I implemented my own CTC decoder in C++ with a python wrapper as frameworks like TensorflowLite and OpenVINO did not support the beam search decoding op for edge deployment. Evaluation \u00b6 To test the model's performance with existing solutions I compared my model to two openly available commercial solutions on Indian and mix country number plates. Plate Recognizer : The model was able to achieve higher accuracy for both detection and recognition tasks compared to results extracted from the plate recognizer's online demo. late recognizer solution. OpenALPR Benchmark : This company published a benchmark of 100 challenging images on which the model was able to achieve 98% accuracy. Deployment \u00b6 I used different frameworks to deploy the same model based on hardware specifications. Some of the frameworks I used are listed below. The challenge was to use only supported ops and maintain accuracy after conversion from TensorFlow to custom frameworks. NVIDIA TensorRT : For GPU-enabled server deployment, I converted my model to TensorRT plan files. This reduced float32 to float16 and significantly improved inference speed. Intel OpenVino : For intel's CPU-based server deployment, I converted my model to OpenVino's intermediate format. This reduced float32 to int8 using inbuilt quantization with a fallback to float32. TensorFlow Lite : For edge devices such as raspberry-pi and smartphones, I used TensorFlow lite to deploy my model. I used C++ to write a modular backend where image reading, preprocessing and post-processing functions were common. While the actual inference class was an abstract class implemented for every hardware using framework-specific methods. It is easy to compile and distribute models in form of a C++ SDK and then to write separate code for each hardware.","title":"Automatic License Plate Recognition"},{"location":"projects/alpr/#automatic-license-plate-recognition","text":"","title":"Automatic License Plate Recognition"},{"location":"projects/alpr/#introduction","text":"ALPR/ANPR is an algorithm to recognize the digits of a vehicle number plate using its image. Even though this problem seems a simple optical character recognition task, many traditional solutions fail to achieve good results in real-world conditions as shown below. I developed a custom Deep Learning based solution that not only works with these cases but is also fast enough to deploy on edge devices. I broke down this task into two subtasks, license plate detection, and recognition.","title":"Introduction"},{"location":"projects/alpr/#license-plate-detection","text":"In the first phase, we need an object detection model which can retrieve the bounding box coordinates of a license plate in the input image.","title":"License Plate Detection"},{"location":"projects/alpr/#dataset-collection","text":"Since the challenge was to build this model for real-world images, I needed a dataset that has variations in vehicle type, plate type, location of plate, size, shape, etc. I collected a large number of vehicle images and hand-annotated bounding boxed for license plates. Some of the sources I used for collecting data are listed below, OpenSource Datasets : I merged all available open-source plate detection datasets which I found such as Cars , CCPD etc. into a large collection of the labeled dataset. Active learning : I trained my initial model on all the readily available annotated images. Then used this model to generate labels for unknown images and corrected the annotations manually. This way I was able to label a large number of unlabelled images quickly. For active learning, I developed a custom annotation tool using javascript to adjust bounding boxes. Some of the unlabelled datasets used for active learning are as follows, Google images : Vehicle images with visible license plates from google search. Youtube videos : A large number of channels live stream traffic or street view cameras 24/7 on YouTube. I collected a large number of frames containing vehicles by running a vehicle detection model on it. India Driving Dataset : Around 10k Indian street view images captured from a front-facing camera attached to a car.","title":"Dataset Collection"},{"location":"projects/alpr/#modeling","text":"I used tensorflow-yolov3 repo to train my custom yolov3 model. YoloV3 is a single-shot detector that can detect objects of multiple scales in a single pass. I made a couple of modifications as per my use-case as below, Since I only needed to predict a single class, I reduced the number of channels for each layer by at least 8x. This way I was able to fit my model on edge devices. I used multiscale training where instead of training the model on a fixed input resolution, we choose a random input resolution for each batch. e.g I trained my model on batches varying in input resolutions from 128x128, 256x256, 512x512, and 768x768 pixels. This way I could use the same model for different input sizes based on latency requirements as higher resolution takes more inference time. For the prediction layer, I used the encoding method proposed by the SSD paper since the TensorFlow-lite library has an inbuilt Op called TFLite_Detection_PostProcess for SSD style decoding and NMS, and thus it was easy to convert the entire model to tflite format later for edge deployment. For bounding box regression I used Generelized IOU loss instead of standard box regression and for classification, I used the Focal Loss to tackle the class imbalance problem. For data augmentation, I used imgaug library which preserves the location of the bounding box after applying affine transformations. Apart from standard augmentations, I designed a scale-oriented augmentation in which we choose a random bounding box from the image and randomly crop 10-90% image around it. This way we make sure our model sees all the different scales of the license plate.","title":"Modeling"},{"location":"projects/alpr/#polygon-detection","text":"Our license plate detection model only predicts rectangular bounding boxes. For recognition, we need to correct issues like rotation and skewness using a perspective transformation as shown below. For this, we need the coordinates of the 4 corners.","title":"Polygon detection"},{"location":"projects/alpr/#dataset-collection_1","text":"I used CCPD dataset as it contains polygon annotations for license plates. The CCPD only contained blue color Chinese number plates. To generalize better, I replaced the original plate images with other country number plates using OpenCV's perspective transform .","title":"Dataset Collection"},{"location":"projects/alpr/#modeling_1","text":"I trained a very small CNN model which directly regresses these 8 values (x and y coordinates of all the 4 points) given a crop of license plates. Additionally, I predicted the probability of the license plate filtering out false positives. I used wing loss instead of standard MSE loss to weigh hard examples more compared to the easier ones.","title":"Modeling"},{"location":"projects/alpr/#license-plate-recognition","text":"After detecting and correcting the angle and skewness of the license plate, in the third stage, we need to extract the license plate number from the crop. The license plates can be single lines or multi-line. Also, the variability in lighting conditions, background noise, fonts and font size, etc. results in very poor recognition accuracy for standard image processing-based approaches. So I trained a model which can predict both single and multiline license plates directly from the RGB image crop.","title":"License Plate Recognition"},{"location":"projects/alpr/#dataset-collection_2","text":"Similar to the detection task, I started with CCPD and other open-source datasets which contained string-level annotations for plate images. Then used an active learning technique to iteratively predict and refine labels for unlabelled images to increase the dataset size. With active learning, I was able to collect around 30k labeled license plate crops. The problem with this data was the class imbalance at the character level. Since the model does not see all the characters equally, it misclassified low-frequency characters often. E.g. 0<->8, 1<->7 etc. To overcome this issue, I synthetically generated license plate images with different backgrounds and styles keeping the total frequency of each character equal across the entire dataset. I used this data to train the model from scratch and then finetuned it with real data which increased the overall accuracy by more than 20%. I also designed a 3D version of an Indian license plate in a blender similar to as shown below and rendered different views of it by changing characters for every render using python and blender integration. This way I was able to improve the model's accuracy on country-specific plates.","title":"Dataset collection"},{"location":"projects/alpr/#modeling_2","text":"I designed a custom CNN + CTC decoder based model for recognition. The input of the model was semi-rectangular (width 128 and height 256) such that we can fit both single and multiline number plates in the input placeholder padded by zeros. The model extracts features from the upper and lower image and then concatenates both in a single row such that we have an output of shape (H:1, W:64, C:37). Channels refer to the number of possible characters for each position and 64 is the maximum string length the model can predict. CTC loss was used to directly train the model on unaligned image-and-text pairs. The model learns to output one character for every 64 positions such that the final string matches our desired string by predicting other characters as blank. During inference, the CTC beam search decoder was used for better accuracy. I implemented my own CTC decoder in C++ with a python wrapper as frameworks like TensorflowLite and OpenVINO did not support the beam search decoding op for edge deployment.","title":"Modeling"},{"location":"projects/alpr/#evaluation","text":"To test the model's performance with existing solutions I compared my model to two openly available commercial solutions on Indian and mix country number plates. Plate Recognizer : The model was able to achieve higher accuracy for both detection and recognition tasks compared to results extracted from the plate recognizer's online demo. late recognizer solution. OpenALPR Benchmark : This company published a benchmark of 100 challenging images on which the model was able to achieve 98% accuracy.","title":"Evaluation"},{"location":"projects/alpr/#deployment","text":"I used different frameworks to deploy the same model based on hardware specifications. Some of the frameworks I used are listed below. The challenge was to use only supported ops and maintain accuracy after conversion from TensorFlow to custom frameworks. NVIDIA TensorRT : For GPU-enabled server deployment, I converted my model to TensorRT plan files. This reduced float32 to float16 and significantly improved inference speed. Intel OpenVino : For intel's CPU-based server deployment, I converted my model to OpenVino's intermediate format. This reduced float32 to int8 using inbuilt quantization with a fallback to float32. TensorFlow Lite : For edge devices such as raspberry-pi and smartphones, I used TensorFlow lite to deploy my model. I used C++ to write a modular backend where image reading, preprocessing and post-processing functions were common. While the actual inference class was an abstract class implemented for every hardware using framework-specific methods. It is easy to compile and distribute models in form of a C++ SDK and then to write separate code for each hardware.","title":"Deployment"},{"location":"projects/auto_adaptive/","text":"Auto Adaptive Image Classifier \u00b6 Platform similar to google photos which can group similar looking images into albums and can predict labels for the unknown images. Starting with zero known labels, the AI can learn to recognize many different classes on the fly as in when new labelled examples are added. Functionalities \u00b6 HomePage \u00b6 GridLayout which shows different classes Add Zip for one class \u00b6 Defines popup which takes zip from user and class label to add multiple images with labeled class in database View Individual class Images \u00b6 Click on class card to view images for that class We can also update the name of class from here Predict Image \u00b6 Defines popup which ask image input from user (only png and jpg allowed) and predict class from existing classes then user can give choice of add predicted label to given image or not E.g. We have 5 car images in the database and we will test on unknown car image. Model predicted below out of training image correctly as a car. Add unknown class \u00b6 We can add a new class without any label. These can be out of distribution images, Recluster \u00b6 When we click on recluster button, all unknown images are grouped together by their visual similarity. Add group to known classes \u00b6 We can add goup_0 to human verified classes by updating its name. Train on new images \u00b6 To automatically adapt newly added classes we can click the train button. Auto adapts new verified classes. We can see two pandas images are correctly classified which are not part of training. Predict out of distribution classes \u00b6 Classes which the model has never seen will be predicted as unknown. We have set a threshold for detecting unknown classes high so that precision can be higher compared to recall. Try with a few good quality images for known classes if unknown is suggested.","title":"Auto Adaptive Image Classifier"},{"location":"projects/auto_adaptive/#auto-adaptive-image-classifier","text":"Platform similar to google photos which can group similar looking images into albums and can predict labels for the unknown images. Starting with zero known labels, the AI can learn to recognize many different classes on the fly as in when new labelled examples are added.","title":"Auto Adaptive Image Classifier"},{"location":"projects/auto_adaptive/#functionalities","text":"","title":"Functionalities"},{"location":"projects/auto_adaptive/#homepage","text":"GridLayout which shows different classes","title":"HomePage"},{"location":"projects/auto_adaptive/#add-zip-for-one-class","text":"Defines popup which takes zip from user and class label to add multiple images with labeled class in database","title":"Add Zip for one class"},{"location":"projects/auto_adaptive/#view-individual-class-images","text":"Click on class card to view images for that class We can also update the name of class from here","title":"View Individual class Images"},{"location":"projects/auto_adaptive/#predict-image","text":"Defines popup which ask image input from user (only png and jpg allowed) and predict class from existing classes then user can give choice of add predicted label to given image or not E.g. We have 5 car images in the database and we will test on unknown car image. Model predicted below out of training image correctly as a car.","title":"Predict Image"},{"location":"projects/auto_adaptive/#add-unknown-class","text":"We can add a new class without any label. These can be out of distribution images,","title":"Add unknown class"},{"location":"projects/auto_adaptive/#recluster","text":"When we click on recluster button, all unknown images are grouped together by their visual similarity.","title":"Recluster"},{"location":"projects/auto_adaptive/#add-group-to-known-classes","text":"We can add goup_0 to human verified classes by updating its name.","title":"Add group to known classes"},{"location":"projects/auto_adaptive/#train-on-new-images","text":"To automatically adapt newly added classes we can click the train button. Auto adapts new verified classes. We can see two pandas images are correctly classified which are not part of training.","title":"Train on new images"},{"location":"projects/auto_adaptive/#predict-out-of-distribution-classes","text":"Classes which the model has never seen will be predicted as unknown. We have set a threshold for detecting unknown classes high so that precision can be higher compared to recall. Try with a few good quality images for known classes if unknown is suggested.","title":"Predict out of distribution classes"},{"location":"projects/clothes_retrieval/","text":"Clothes Retrieval \u00b6 Searching for exact cloth image accurately from massive collections of clothes' images based on a query image. For example, here the left most image is the query image and other images are matched images retrieved from a huge collection of clothes using a computer vision algorithm. Dataset Details \u00b6 Dataset used : Deep Fashion 2 Problem statement : Consumer-to-shop Clothes Retrieval Problem description : Matching consumer-taken photos with their shop counterparts Code \u00b6 Google Colab Building the intuition \u00b6 Sorted in the order of how I reached the solution 1. Usecase understanding \u00b6 Consumer can upload a photo of clothing item System should retrieve similar looking items from shopping catalog. 2. Dataset understanding \u00b6 In the given task, we have two different sources of data . Consumer captured images : These are low quality images captured using phone's camera (front or back). Images have variation in lighting, orientation, occlusion, filters etc. Shop captured images : Shop images include good quality photo shoot quality images. It also includes images from online shopping carts where background is removed etc. 3. Real world challenges \u00b6 Labeling the dataset is major challenge Shopping image collection including online and offline stores can be huge Thousands of unique clothing items Manually pairing each user captured image to a similar shop image costs lots of human effort When the product is new, we might not have huge collection of user captured images. In that case the system cannot work better for new styles Solving the problem \u00b6 Considering the scarcity of labeled consumer to shop pairs, I decided build two stage pipeline . First learning task agnostic attributes from raw data then use them to reduce consumer to shop domain gap using fewer labeled images. Step 1 : Learning directly from raw images without labeling \u00b6 Observations Humans are good at pattern matching . We recognize many things from their attributes . For e.g we recognize a vehicle with wheels, seats, glasses, horn etc. We also use this attributes to distinguish between two different vehicles We learn about these attributes by comparing between many instances unconsciously Inspirations Deep learning model can also learn similar attributes by just observing across different images In many image retrieval problems we consider output of pre-fc layer as embedding We use metric learning approaches such as triplet loss to lean discriminative embedding We can consider each dimension in embedding vector as one attribute This attributes make the embedding vector discriminative Difference from metric learning Metric learning approaches try to increase distance between embeddings as a whole . For that we need positive and negative pairs . Instead we can increase distance between attributes of embedding without labels . Here, We do not try to teach which attribute represents what ? We try to teach that no two attributes should represent same concept . Mathematical Formulation We often calculate similarity of two vectors using cosine similarity measure. Cosine similarity has an interpretation as the cosine of the angle between the two vectors Cosine similarity is not invariant to shifts . If x was shifted to x+1, the cosine similarity would change. Unlike the cosine, the correlation is invariant to both scale and location changes of x and y. Correlation is the cosine similarity between centered versions of x and y In a batch of images, attribute vector represents values of particular dimension across all images For e.g if batch size = 512 and embedding dim = 2048, then we have 2048 attribute vectors each of length 512 Now consider another batch of slightly different version of the same images in first batch. Our goal is to Maximize cosine similarity of same attributes in both batches Minimize cosine similarity of different attributes in both batches Here, I used correlation as a proxy loss as both have same output range [-1,1] I divided loss function in two parts Correlation of attribute vectors at same index should be 1 . To make model generalize better I decreased the predicted correlation by margin m. For e.g if correlation is 0.9 we make it 0.6 so model tries to make it higher. Correlation of attribute vectors at different index should be 0 . Since there are N^2-N pairs of different attribute vectors I assigned lower weight compared to positive attribute pair. Code for attribute loss def attribute_loss(emb_1, emb_2, alpha=5e-4, margin=0.3): # per feature-dimension normalisation standardize = lambda x : (x - tf.reduce_mean(x, axis=0)) / tf.math.reduce_std(x, axis=0) emb_1_std = standardize(emb_1) # BxE emb_2_std = standardize(emb_2) # BxE # similarity of pairwise feature-dimension bsize = tf.cast(tf.shape(emb_1)[0], tf.float32) cos_t = tf.matmul(emb_1_std, emb_2_std, transpose_a=True) / bsize # Embed x Embed acos_t = tf.acos(tf.clip_by_value(cos_t, -1.0, 1.0)) # diagonal values rep. expected similarity of same feature-dim same_dim_mask = tf.eye(emb_1.shape[1]) # Embed x Embed # non-diagonal values rep. expected similarity of different features-dim diff_dim_mask = 1.0 - same_dim_mask # Embed x Embed # increase angle betweem same feature-dims : Cos(angle + m)(i==j) cosine_same = tf.cos(acos_t + margin) same_dim_loss = tf.square(same_dim_mask - cosine_same) * same_dim_mask # decrease angle between different feature-dims : Cos(angle - m)(i!=j) # cosine_diff = tf.cos(acos_t - margin) diff_dim_loss = tf.square(same_dim_mask - cos_t) * diff_dim_mask # final weighted loss weighted_loss = tf.reduce_sum(same_dim_loss + diff_dim_loss * alpha) return weighted_loss Step 2 : Reducing the domain gap using pinch of supervision \u00b6 Model trained with attribute loss can learn diverse set of attributes for each image. Although consumer and shop images have inherent biases due to different source of data generation This is very well known as domain gap where two similar objects can perceived differently by model due to pixel level differences I used combination of attribute loss and instance loss to reduce this domain gap with only 3 more epochs for finetuning 1. Attribute loss : In stage one, attribute loss was calculated between two different versions of same image as we did not have any other information Here we calculate it between attributes of same cloth type but one image from consumer and other from shop using label information 2. Instance loss : Attribute loss encourages model to learn features which are consistent in consumer and shop domain Instance loss is normal classification loss which uses these attributes to group similar cloths in tight clusters and increase distance between other groups. I used arcface as an instance loss. 3. Using weights from attribute model to Initialize backbone Initialize weights of newly added classification layer . Arcface tries to reduce cosine similarity of weights vector and instance vector. I used average of all embeddings produced by unsupervised model for each class as the initial weight vector of that class. Code for classifier weights calculation # Get classifier weights def classifier_weights(dataset, model, num_class): # calculate avg embedding for each class as w_init for fc out_layer = params.model.class_layer weights = np.zeros(shape=(num_class, model.outputs[out_layer].shape[-1]), dtype=np.float32) # (n_class, emb_dim) # (n_class,) extra 1 for ignoring zero div class_cnt = np.ones(shape=(num_class,), dtype=np.float32) for data in tqdm(dataset): user, shop, class_id = data all_images = tf.concat([user, shop], axis=0) class_ids = tf.concat([class_id, class_id], axis=0) embeds = model(all_images, training=False)[out_layer].numpy() weights[class_ids] += embeds class_cnt[class_ids] += 1 weights = np.divide(weights, class_cnt[:, np.newaxis]) return weights Experiments \u00b6 Model details \u00b6 Model Resnet50 Batch Size 512 Input resolution 96 Embedding dimension 2048 For training details refer code notebook Results \u00b6 Final Comments \u00b6 Although unsupervised loss has lower accuracy when source domains are different , it performs significantly better in mix domain (Exp 1 Last Block). Which suggests that attribute loss can work very well in task agnostic manner. Both attribute loss and fc7 weights initialization using avg class embedding improves results Supervised training from scratch performed significantly worse when training from scratch for 3 epoch Demo on validation dataset \u00b6 No image was used during training Results are in descending order of recall First column is consumer query image Other columns are retrieved shop images Green box is True Positive , Red box is False positive Model : Unsupervised Learning + Attribute Loss (Exp 1) \u00b6 Model : Attribute Loss + Instance Loss + fc7 weights init (Exp-2) \u00b6","title":"Clothes Retrieval"},{"location":"projects/clothes_retrieval/#clothes-retrieval","text":"Searching for exact cloth image accurately from massive collections of clothes' images based on a query image. For example, here the left most image is the query image and other images are matched images retrieved from a huge collection of clothes using a computer vision algorithm.","title":"Clothes Retrieval"},{"location":"projects/clothes_retrieval/#dataset-details","text":"Dataset used : Deep Fashion 2 Problem statement : Consumer-to-shop Clothes Retrieval Problem description : Matching consumer-taken photos with their shop counterparts","title":"Dataset Details"},{"location":"projects/clothes_retrieval/#code","text":"Google Colab","title":"Code"},{"location":"projects/clothes_retrieval/#building-the-intuition","text":"Sorted in the order of how I reached the solution","title":"Building the intuition"},{"location":"projects/clothes_retrieval/#1-usecase-understanding","text":"Consumer can upload a photo of clothing item System should retrieve similar looking items from shopping catalog.","title":"1. Usecase understanding"},{"location":"projects/clothes_retrieval/#2-dataset-understanding","text":"In the given task, we have two different sources of data . Consumer captured images : These are low quality images captured using phone's camera (front or back). Images have variation in lighting, orientation, occlusion, filters etc. Shop captured images : Shop images include good quality photo shoot quality images. It also includes images from online shopping carts where background is removed etc.","title":"2. Dataset understanding"},{"location":"projects/clothes_retrieval/#3-real-world-challenges","text":"Labeling the dataset is major challenge Shopping image collection including online and offline stores can be huge Thousands of unique clothing items Manually pairing each user captured image to a similar shop image costs lots of human effort When the product is new, we might not have huge collection of user captured images. In that case the system cannot work better for new styles","title":"3. Real world challenges"},{"location":"projects/clothes_retrieval/#solving-the-problem","text":"Considering the scarcity of labeled consumer to shop pairs, I decided build two stage pipeline . First learning task agnostic attributes from raw data then use them to reduce consumer to shop domain gap using fewer labeled images.","title":"Solving the problem"},{"location":"projects/clothes_retrieval/#step-1-learning-directly-from-raw-images-without-labeling","text":"Observations Humans are good at pattern matching . We recognize many things from their attributes . For e.g we recognize a vehicle with wheels, seats, glasses, horn etc. We also use this attributes to distinguish between two different vehicles We learn about these attributes by comparing between many instances unconsciously Inspirations Deep learning model can also learn similar attributes by just observing across different images In many image retrieval problems we consider output of pre-fc layer as embedding We use metric learning approaches such as triplet loss to lean discriminative embedding We can consider each dimension in embedding vector as one attribute This attributes make the embedding vector discriminative Difference from metric learning Metric learning approaches try to increase distance between embeddings as a whole . For that we need positive and negative pairs . Instead we can increase distance between attributes of embedding without labels . Here, We do not try to teach which attribute represents what ? We try to teach that no two attributes should represent same concept . Mathematical Formulation We often calculate similarity of two vectors using cosine similarity measure. Cosine similarity has an interpretation as the cosine of the angle between the two vectors Cosine similarity is not invariant to shifts . If x was shifted to x+1, the cosine similarity would change. Unlike the cosine, the correlation is invariant to both scale and location changes of x and y. Correlation is the cosine similarity between centered versions of x and y In a batch of images, attribute vector represents values of particular dimension across all images For e.g if batch size = 512 and embedding dim = 2048, then we have 2048 attribute vectors each of length 512 Now consider another batch of slightly different version of the same images in first batch. Our goal is to Maximize cosine similarity of same attributes in both batches Minimize cosine similarity of different attributes in both batches Here, I used correlation as a proxy loss as both have same output range [-1,1] I divided loss function in two parts Correlation of attribute vectors at same index should be 1 . To make model generalize better I decreased the predicted correlation by margin m. For e.g if correlation is 0.9 we make it 0.6 so model tries to make it higher. Correlation of attribute vectors at different index should be 0 . Since there are N^2-N pairs of different attribute vectors I assigned lower weight compared to positive attribute pair. Code for attribute loss def attribute_loss(emb_1, emb_2, alpha=5e-4, margin=0.3): # per feature-dimension normalisation standardize = lambda x : (x - tf.reduce_mean(x, axis=0)) / tf.math.reduce_std(x, axis=0) emb_1_std = standardize(emb_1) # BxE emb_2_std = standardize(emb_2) # BxE # similarity of pairwise feature-dimension bsize = tf.cast(tf.shape(emb_1)[0], tf.float32) cos_t = tf.matmul(emb_1_std, emb_2_std, transpose_a=True) / bsize # Embed x Embed acos_t = tf.acos(tf.clip_by_value(cos_t, -1.0, 1.0)) # diagonal values rep. expected similarity of same feature-dim same_dim_mask = tf.eye(emb_1.shape[1]) # Embed x Embed # non-diagonal values rep. expected similarity of different features-dim diff_dim_mask = 1.0 - same_dim_mask # Embed x Embed # increase angle betweem same feature-dims : Cos(angle + m)(i==j) cosine_same = tf.cos(acos_t + margin) same_dim_loss = tf.square(same_dim_mask - cosine_same) * same_dim_mask # decrease angle between different feature-dims : Cos(angle - m)(i!=j) # cosine_diff = tf.cos(acos_t - margin) diff_dim_loss = tf.square(same_dim_mask - cos_t) * diff_dim_mask # final weighted loss weighted_loss = tf.reduce_sum(same_dim_loss + diff_dim_loss * alpha) return weighted_loss","title":"Step 1 : Learning directly from raw images without labeling"},{"location":"projects/clothes_retrieval/#step-2-reducing-the-domain-gap-using-pinch-of-supervision","text":"Model trained with attribute loss can learn diverse set of attributes for each image. Although consumer and shop images have inherent biases due to different source of data generation This is very well known as domain gap where two similar objects can perceived differently by model due to pixel level differences I used combination of attribute loss and instance loss to reduce this domain gap with only 3 more epochs for finetuning 1. Attribute loss : In stage one, attribute loss was calculated between two different versions of same image as we did not have any other information Here we calculate it between attributes of same cloth type but one image from consumer and other from shop using label information 2. Instance loss : Attribute loss encourages model to learn features which are consistent in consumer and shop domain Instance loss is normal classification loss which uses these attributes to group similar cloths in tight clusters and increase distance between other groups. I used arcface as an instance loss. 3. Using weights from attribute model to Initialize backbone Initialize weights of newly added classification layer . Arcface tries to reduce cosine similarity of weights vector and instance vector. I used average of all embeddings produced by unsupervised model for each class as the initial weight vector of that class. Code for classifier weights calculation # Get classifier weights def classifier_weights(dataset, model, num_class): # calculate avg embedding for each class as w_init for fc out_layer = params.model.class_layer weights = np.zeros(shape=(num_class, model.outputs[out_layer].shape[-1]), dtype=np.float32) # (n_class, emb_dim) # (n_class,) extra 1 for ignoring zero div class_cnt = np.ones(shape=(num_class,), dtype=np.float32) for data in tqdm(dataset): user, shop, class_id = data all_images = tf.concat([user, shop], axis=0) class_ids = tf.concat([class_id, class_id], axis=0) embeds = model(all_images, training=False)[out_layer].numpy() weights[class_ids] += embeds class_cnt[class_ids] += 1 weights = np.divide(weights, class_cnt[:, np.newaxis]) return weights","title":"Step 2 : Reducing the domain gap using pinch of supervision"},{"location":"projects/clothes_retrieval/#experiments","text":"","title":"Experiments"},{"location":"projects/clothes_retrieval/#model-details","text":"Model Resnet50 Batch Size 512 Input resolution 96 Embedding dimension 2048 For training details refer code notebook","title":"Model details"},{"location":"projects/clothes_retrieval/#results","text":"","title":"Results"},{"location":"projects/clothes_retrieval/#final-comments","text":"Although unsupervised loss has lower accuracy when source domains are different , it performs significantly better in mix domain (Exp 1 Last Block). Which suggests that attribute loss can work very well in task agnostic manner. Both attribute loss and fc7 weights initialization using avg class embedding improves results Supervised training from scratch performed significantly worse when training from scratch for 3 epoch","title":"Final Comments"},{"location":"projects/clothes_retrieval/#demo-on-validation-dataset","text":"No image was used during training Results are in descending order of recall First column is consumer query image Other columns are retrieved shop images Green box is True Positive , Red box is False positive","title":"Demo on validation dataset"},{"location":"projects/clothes_retrieval/#model-unsupervised-learning-attribute-loss-exp-1","text":"","title":"Model :  Unsupervised Learning + Attribute Loss (Exp 1)"},{"location":"projects/clothes_retrieval/#model-attribute-loss-instance-loss-fc7-weights-init-exp-2","text":"","title":"Model :  Attribute Loss + Instance Loss + fc7 weights init (Exp-2)"},{"location":"projects/deep_face_recognition/","text":"Deep Face Recognition \u00b6 Introduction \u00b6 Deep face recognition is the technique to identify the identity of a person using only facial images. Generally, a Deep Conv Neural Network (DCNN) is used for transforming an image into fix length high dimensional vector. We usually precompute the embeddings for all the people in our database using a few facial images per person and use it for matching it against an embedding of an unknown face image using distance metrics such as cosine similarity. The accuracy of our model depends upon how well it can maximize the distance between two different person's face embeddings and minimize the distance between two face embeddings of the same person. A standard approach is to train our model using triplet loss where for each image we prepare a positive image that belongs to the same person and a negative image that belongs to a different person. Although there is a combinatorial explosion in the number of face triplets especially for large-scale datasets, leading to a significant increase in the number of iteration steps. Technique such as Hard Negative Mining is used very often with triplet loss to generalize it for challenging face images. Though sampling is quite a hard problem. Arcface Loss \u00b6 Instead of sampling triplets, I used the Arcface loss which required no sampling at all. Arcface modifies the logit for the target class before calculating softmax cross-entropy loss, such that the model learns to form very close clusters for embeddings of the same class. Specifically, the dot product between the DCNN feature and the last fully connected layer is equal to the cosine distance after feature and center normalization. Arcface utilizes the arc-cosine function to calculate the angle between the current feature and the target center. Afterward, it introduces an additive angular margin to the target angle, and we get the target logit back again by the cosine function. Then, it re-scales all logits by a fixed feature norm, and the subsequent steps are the same as in the softmax loss. Due to the exact correspondence between the angle and arc in the normalized hypersphere, this method can directly optimize the geodesic distance margin, thus it is called as ArcFace. Datasets \u00b6 Training dataset \u00b6 For the training, I mixed two datasets by removing overlapping identities using a pretrained Resnet100 model. MS-Celeb-1M - Around 5.1M images of 93K identities DeepGlint - Around 6.75M images of 181K identities Test dataset \u00b6 For testing, I used two datasets as below, LFW Benchmark dataset : LFW is a public benchmark for face verification, also known as pair matching. Here we have 50% positive pairs and 50% negative pairs. We calculate PR curve using cosine similarity as a threshold. Although it is just a verification benchmark that only checks whether pair of images are of the same person or not. It is very difficult to extrapolate from performance on verification to performance on 1:N recognition. Many groups are not well represented in LFW. For example, there are very few children, no babies, very few people over the age of 80, and a relatively small proportion of women. In addition, many ethnicities have very minor representation or none at all. I created a custom evaluation dataset balanced using age, gender, ethnicity, image quality, and lighting. Here I tested both the 1:1 face verification and 1:N face recognition performance of my model for three different image quality pairs. High vs High : High-quality query images and High-quality database images. It is used for testing best-case scenarios when both enrolled and query images are captured using a high-quality camera. Low vs High : Low-quality query images and High-quality database images. This tests the real-world scenario where enrolled images are mostly captured using good-quality cameras and in the controlled environment in which query images are often captured using low-quality surveillance cameras. This specifically tests the recall of the model as the chances of rejection are high for a low-quality image. Low vs Low : Low-quality query images and Low-quality database images. This specifically tests the precision of the model when we enroll low-quality images in the database to decrease the rejection rate. Chances of false-match are high when image quality is not good for both query and database images. Data normalization \u00b6 Empirically it is observed that normalizing face images such that the location of eyes, nose, and lips ends are always almost fixed for the training and evaluation images significantly increases the model's accuracy. For this task, I used a small landmark detection model trained with wing loss based on standard CNN with an input resolution of 96x96 px on the face crop extracted by a face detection model. Data augmentation \u00b6 I used two types of data augmentation techniques for better generalization Offline data augmentation for pose diversity It is very hard to collect a wide range of facial poses in the real world for each identity. I used PRNet to generate a 3D depth map from a single-face image and generate different poses for the same as shown below. The final dataset was balanced by poses. Depth estimation Reconstruct face images with different poses. Online data augmentation for lighting and image quality. I used imgaug library which provides a range of image augmentation functions that can be randomly applied to an image during training. Some of the transforms I used were, Fliplr JpegCompression blur AddToHue AddToBrightness Example of online augmentations. Faster inference \u00b6 As I wanted to deploy my model on the edge devices such as raspberry-pi or smartphone SOCs, I used two techniques to increase speed and reduce model size. Knowledge distillation The model I used for deployment was a custom ResNet50. Training a smaller model from scratch on a large amount of data could lead to an underfitting problem. I designed a knowledge distillation routine in which I transferred the learnings of the pretrained ResNet100 model to the ResNet50. The steps were as shown below. Train ResNet100 from scratch on the whole dataset Use ResNet100 to precompute outputs of final embedding and intermediate Resnet blocks for the left and right flips of the image. Copy weights of the last fully connected layer from pretrained ResNet100 to the initial version of ResNet50. Train ResNet50 only on the precomputed dataset and add loss for embedding reconstruction and intermediate features reconstruction loss to the arcface loss. This way the embedding generated using ResNet50 is very similar to ResNet100 and it significantly outperforms vanilla ResNet50 trained from scratch. Full integer quantization using TensorFlow lite. I used TensorFlow lite's post-training quantization module to convert my float32 model to a full int8 model. For the representative dataset, I balanced images by age, gender, pose, ethnicity, and image quality. This reduced model's size by 4x and inference speed by 5x. With these two techniques, I was able to infer my model within a second on the slowest hardware such as raspberry pi 3 (1.2 GHz quad-core ARM Cortex-A53) by keeping similar accuracy of a full-fledged ResNet100 deployment on GPU servers. Deployment \u00b6 I used different frameworks to deploy the same model based on hardware specifications. Some of the frameworks I used are listed below. The challenge was to use only supported ops and maintain accuracy after conversion from TensorFlow to custom frameworks. NVIDIA TensorRT : For GPU-enabled server deployment, I converted my model to TensorRT plan files. This reduced float32 to float16 and significantly improved inference speed. Intel OpenVino : For intel's CPU-based server deployment, I converted my model to OpenVino's intermediate format. This reduced float32 to int8 using inbuilt quantization with a fallback to float32. TensorFlow Lite : For edge devices such as raspberry-pi and smartphones, I used TensorFlow lite to deploy my model. I used C++ to write a modular backend where image reading, preprocessing and post-processing functions were common. While the actual inference class was an abstract class implemented for every hardware using framework-specific methods. It is easy to compile and distribute models in form of a C++ SDK and then to write separate code for each hardware.","title":"Deep Face Recognition"},{"location":"projects/deep_face_recognition/#deep-face-recognition","text":"","title":"Deep Face Recognition"},{"location":"projects/deep_face_recognition/#introduction","text":"Deep face recognition is the technique to identify the identity of a person using only facial images. Generally, a Deep Conv Neural Network (DCNN) is used for transforming an image into fix length high dimensional vector. We usually precompute the embeddings for all the people in our database using a few facial images per person and use it for matching it against an embedding of an unknown face image using distance metrics such as cosine similarity. The accuracy of our model depends upon how well it can maximize the distance between two different person's face embeddings and minimize the distance between two face embeddings of the same person. A standard approach is to train our model using triplet loss where for each image we prepare a positive image that belongs to the same person and a negative image that belongs to a different person. Although there is a combinatorial explosion in the number of face triplets especially for large-scale datasets, leading to a significant increase in the number of iteration steps. Technique such as Hard Negative Mining is used very often with triplet loss to generalize it for challenging face images. Though sampling is quite a hard problem.","title":"Introduction"},{"location":"projects/deep_face_recognition/#arcface-loss","text":"Instead of sampling triplets, I used the Arcface loss which required no sampling at all. Arcface modifies the logit for the target class before calculating softmax cross-entropy loss, such that the model learns to form very close clusters for embeddings of the same class. Specifically, the dot product between the DCNN feature and the last fully connected layer is equal to the cosine distance after feature and center normalization. Arcface utilizes the arc-cosine function to calculate the angle between the current feature and the target center. Afterward, it introduces an additive angular margin to the target angle, and we get the target logit back again by the cosine function. Then, it re-scales all logits by a fixed feature norm, and the subsequent steps are the same as in the softmax loss. Due to the exact correspondence between the angle and arc in the normalized hypersphere, this method can directly optimize the geodesic distance margin, thus it is called as ArcFace.","title":"Arcface Loss"},{"location":"projects/deep_face_recognition/#datasets","text":"","title":"Datasets"},{"location":"projects/deep_face_recognition/#training-dataset","text":"For the training, I mixed two datasets by removing overlapping identities using a pretrained Resnet100 model. MS-Celeb-1M - Around 5.1M images of 93K identities DeepGlint - Around 6.75M images of 181K identities","title":"Training dataset"},{"location":"projects/deep_face_recognition/#test-dataset","text":"For testing, I used two datasets as below, LFW Benchmark dataset : LFW is a public benchmark for face verification, also known as pair matching. Here we have 50% positive pairs and 50% negative pairs. We calculate PR curve using cosine similarity as a threshold. Although it is just a verification benchmark that only checks whether pair of images are of the same person or not. It is very difficult to extrapolate from performance on verification to performance on 1:N recognition. Many groups are not well represented in LFW. For example, there are very few children, no babies, very few people over the age of 80, and a relatively small proportion of women. In addition, many ethnicities have very minor representation or none at all. I created a custom evaluation dataset balanced using age, gender, ethnicity, image quality, and lighting. Here I tested both the 1:1 face verification and 1:N face recognition performance of my model for three different image quality pairs. High vs High : High-quality query images and High-quality database images. It is used for testing best-case scenarios when both enrolled and query images are captured using a high-quality camera. Low vs High : Low-quality query images and High-quality database images. This tests the real-world scenario where enrolled images are mostly captured using good-quality cameras and in the controlled environment in which query images are often captured using low-quality surveillance cameras. This specifically tests the recall of the model as the chances of rejection are high for a low-quality image. Low vs Low : Low-quality query images and Low-quality database images. This specifically tests the precision of the model when we enroll low-quality images in the database to decrease the rejection rate. Chances of false-match are high when image quality is not good for both query and database images.","title":"Test dataset"},{"location":"projects/deep_face_recognition/#data-normalization","text":"Empirically it is observed that normalizing face images such that the location of eyes, nose, and lips ends are always almost fixed for the training and evaluation images significantly increases the model's accuracy. For this task, I used a small landmark detection model trained with wing loss based on standard CNN with an input resolution of 96x96 px on the face crop extracted by a face detection model.","title":"Data normalization"},{"location":"projects/deep_face_recognition/#data-augmentation","text":"I used two types of data augmentation techniques for better generalization Offline data augmentation for pose diversity It is very hard to collect a wide range of facial poses in the real world for each identity. I used PRNet to generate a 3D depth map from a single-face image and generate different poses for the same as shown below. The final dataset was balanced by poses. Depth estimation Reconstruct face images with different poses. Online data augmentation for lighting and image quality. I used imgaug library which provides a range of image augmentation functions that can be randomly applied to an image during training. Some of the transforms I used were, Fliplr JpegCompression blur AddToHue AddToBrightness Example of online augmentations.","title":"Data augmentation"},{"location":"projects/deep_face_recognition/#faster-inference","text":"As I wanted to deploy my model on the edge devices such as raspberry-pi or smartphone SOCs, I used two techniques to increase speed and reduce model size. Knowledge distillation The model I used for deployment was a custom ResNet50. Training a smaller model from scratch on a large amount of data could lead to an underfitting problem. I designed a knowledge distillation routine in which I transferred the learnings of the pretrained ResNet100 model to the ResNet50. The steps were as shown below. Train ResNet100 from scratch on the whole dataset Use ResNet100 to precompute outputs of final embedding and intermediate Resnet blocks for the left and right flips of the image. Copy weights of the last fully connected layer from pretrained ResNet100 to the initial version of ResNet50. Train ResNet50 only on the precomputed dataset and add loss for embedding reconstruction and intermediate features reconstruction loss to the arcface loss. This way the embedding generated using ResNet50 is very similar to ResNet100 and it significantly outperforms vanilla ResNet50 trained from scratch. Full integer quantization using TensorFlow lite. I used TensorFlow lite's post-training quantization module to convert my float32 model to a full int8 model. For the representative dataset, I balanced images by age, gender, pose, ethnicity, and image quality. This reduced model's size by 4x and inference speed by 5x. With these two techniques, I was able to infer my model within a second on the slowest hardware such as raspberry pi 3 (1.2 GHz quad-core ARM Cortex-A53) by keeping similar accuracy of a full-fledged ResNet100 deployment on GPU servers.","title":"Faster inference"},{"location":"projects/deep_face_recognition/#deployment","text":"I used different frameworks to deploy the same model based on hardware specifications. Some of the frameworks I used are listed below. The challenge was to use only supported ops and maintain accuracy after conversion from TensorFlow to custom frameworks. NVIDIA TensorRT : For GPU-enabled server deployment, I converted my model to TensorRT plan files. This reduced float32 to float16 and significantly improved inference speed. Intel OpenVino : For intel's CPU-based server deployment, I converted my model to OpenVino's intermediate format. This reduced float32 to int8 using inbuilt quantization with a fallback to float32. TensorFlow Lite : For edge devices such as raspberry-pi and smartphones, I used TensorFlow lite to deploy my model. I used C++ to write a modular backend where image reading, preprocessing and post-processing functions were common. While the actual inference class was an abstract class implemented for every hardware using framework-specific methods. It is easy to compile and distribute models in form of a C++ SDK and then to write separate code for each hardware.","title":"Deployment"},{"location":"projects/hyperspectral_image_classification/","text":"Hyper Spectral Image Classification \u00b6 Code \u00b6 Hyspeclib Library Code Manual Thesis Introduction \u00b6 Hyperspectral imaging which is also known as imaging spectroscopy, detects radiation of earth surface features in narrow contiguous spectral regions of the electromagnetic spectrum. The Airborne Visible Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) is an airborne hyperspectral sensor of NASA\u2019s Jet Propulsion Laboratory (JPL) with 425 spectral bands ranging from 380 nm to 2510 nm with a bandwidth of 5 nm and spatial resolution of 4-6 m. This study aims at pixel-wise identification and discrimination of crop types using AVIRIS-NG hyperspectral images, with novel Parallel Convolutional Neural Networks architecture. To tackle the challenge posed by a large number of correlated bands, we compare two band selection techniques using Principal Component Analysis (PCA) and back traversal of pre-trained Artificial Neural Network (ANN). Bands selected by PCA method Bands selected by ANN method We also propose an automated technique for augmentation of training dataset with a large number of pixels from unlabelled parts of an image, based on Euclidian distance. Experiments show that bands selected by ANN achieve higher accuracy compare to PCA selected bands with automated data augmentation. Paper \u00b6 Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques @INPROCEEDINGS{8897897, author={Patel, Hetul and Bhagia, Nita and Vyas, Tarjni and Bhattacharya, Bimal and Dave, Kinjal}, booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, title={Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques}, year={2019}, volume={}, number={}, pages={3728-3731}, doi={10.1109/IGARSS.2019.8897897}}","title":"Hyper Spectral Image Classification"},{"location":"projects/hyperspectral_image_classification/#hyper-spectral-image-classification","text":"","title":"Hyper Spectral Image Classification"},{"location":"projects/hyperspectral_image_classification/#code","text":"Hyspeclib Library Code Manual Thesis","title":"Code"},{"location":"projects/hyperspectral_image_classification/#introduction","text":"Hyperspectral imaging which is also known as imaging spectroscopy, detects radiation of earth surface features in narrow contiguous spectral regions of the electromagnetic spectrum. The Airborne Visible Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) is an airborne hyperspectral sensor of NASA\u2019s Jet Propulsion Laboratory (JPL) with 425 spectral bands ranging from 380 nm to 2510 nm with a bandwidth of 5 nm and spatial resolution of 4-6 m. This study aims at pixel-wise identification and discrimination of crop types using AVIRIS-NG hyperspectral images, with novel Parallel Convolutional Neural Networks architecture. To tackle the challenge posed by a large number of correlated bands, we compare two band selection techniques using Principal Component Analysis (PCA) and back traversal of pre-trained Artificial Neural Network (ANN). Bands selected by PCA method Bands selected by ANN method We also propose an automated technique for augmentation of training dataset with a large number of pixels from unlabelled parts of an image, based on Euclidian distance. Experiments show that bands selected by ANN achieve higher accuracy compare to PCA selected bands with automated data augmentation.","title":"Introduction"},{"location":"projects/hyperspectral_image_classification/#paper","text":"Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques @INPROCEEDINGS{8897897, author={Patel, Hetul and Bhagia, Nita and Vyas, Tarjni and Bhattacharya, Bimal and Dave, Kinjal}, booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, title={Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques}, year={2019}, volume={}, number={}, pages={3728-3731}, doi={10.1109/IGARSS.2019.8897897}}","title":"Paper"},{"location":"projects/lecture_notes_saving_app/","text":"Fellow (Lecture notes app) \u00b6 Fellow is an Android app that I built to quickly save my lecture notes into google drive to refer them easily later. Students can quickly click a picture of daily lecture notes for each course. Notes are automatically compiled into a single pdf per course. Some of its feautres are as mentioned below. Code \u00b6 Fellow App Login with google drive account \u00b6 User can login with google account so that all the data is backed-up. Add semester wise courses \u00b6 Students can add list of courses enrolled in current semester so that notes can be easily organized and accessed quickly in pdf form by clicking on the subject name. Customise home screen \u00b6 Students can customize the homescreen by adding the weekly schedule of lectures. The home screen will then only show courses planned for the given day. User can quickly add photos of lecture notes for the given course by clicking on the upload button. Navigate easily \u00b6 Access all the features from the stylish drawer navigation.","title":"Fellow (Lecture notes app)"},{"location":"projects/lecture_notes_saving_app/#fellow-lecture-notes-app","text":"Fellow is an Android app that I built to quickly save my lecture notes into google drive to refer them easily later. Students can quickly click a picture of daily lecture notes for each course. Notes are automatically compiled into a single pdf per course. Some of its feautres are as mentioned below.","title":"Fellow (Lecture notes app)"},{"location":"projects/lecture_notes_saving_app/#code","text":"Fellow App","title":"Code"},{"location":"projects/lecture_notes_saving_app/#login-with-google-drive-account","text":"User can login with google account so that all the data is backed-up.","title":"Login with google drive account"},{"location":"projects/lecture_notes_saving_app/#add-semester-wise-courses","text":"Students can add list of courses enrolled in current semester so that notes can be easily organized and accessed quickly in pdf form by clicking on the subject name.","title":"Add semester wise courses"},{"location":"projects/lecture_notes_saving_app/#customise-home-screen","text":"Students can customize the homescreen by adding the weekly schedule of lectures. The home screen will then only show courses planned for the given day. User can quickly add photos of lecture notes for the given course by clicking on the upload button.","title":"Customise home screen"},{"location":"projects/lecture_notes_saving_app/#navigate-easily","text":"Access all the features from the stylish drawer navigation.","title":"Navigate easily"},{"location":"projects/mind_controlled_home_automation/","text":"Mind Controlled Home Automation \u00b6 Introduction \u00b6 Mind Controlled Home Automation System with self-learning capability \u2013 an approach to enter the era of virtual reality, an era where everything is possible. That\u2019s why we named it as SAMBHAV. The main aim of our project is to control home appliances with human brain acting as an interface. This means that the signals generated by our brain will now control electrical appliances \u2013 termed as Home Automation. For achieving the same, we have used MindWave headset that calculates our concentration(attention) and meditation(relaxation) and even detects the blink. The combination of these human activities will result into switching ON/OFF devices. About Headset \u00b6 A closer look at the headset : Basic functionalities of headset: \u00b6 Power Switch \u2013 To power ON/OFF the headset. If the switch is held past the ON position for 3 seconds, the headset will enter the Bluetooth Pairing Mode and if held past the ON position for 6 seconds, the headset\u2019s pairing memory will be cleared. Ear Clip \u2013 Acts as the reference point and is clipped at the earlobe (lower part of the ear). Sensor Arm \u2013 Core of the headset. It contains the sensor that should be in direct contact with the forehead. Besides sensor, it also includes the on-board chip that processes all of the data. Working \u00b6 The biosensor placed inside the headset measures the frequencies of the electrical signals emitted by the nerve cells. Although neurons are not so good conductor of electricity, yet they generate electrical signals based on the flow of ions across their plasma membranes. Basically, neurons generate a negative potential, that is measured by recording the voltage between the inside and outside of nerve cells. The sensor chip does this work with ear clip acting as the reference point and the electrical signals measured are commonly referred to as brainwaves. The sensor chip then amplifies the brainwave signals and removes the unnecessary noise and muscle movement. These analog signals are then converted into digital signals by the on-board chip and is being processed to the device. The table below shows some of the frequencies that tend to be generated by different types of activity in the brain: Brainwave Type Frequency range Mental states and conditions Delta 0.1Hz to 3Hz Deep, dreamless sleep, non-REM sleep, unconscious Theta 4Hz to 7Hz Intuitive, creative, recall, fantasy, imaginary, dream Alpha 8Hz to 12Hz Relaxed (but not drowsy) tranquil, conscious Low Beta 12Hz to 15Hz Formerly SMR, relaxed yet focused, integrated Midrange Beta 12Hz to 15Hz Thinking, aware of self & surroundings High Beta 21Hz to 30Hz Alertness, agitation Android App for analysis \u00b6 We created an android app for analyzing various signals generated by our brain on pursuing different activities. App is used to display different EEG band power values such as Delta, Theta, Alpha and Gamma etc. At the top of the screen, Graph is displayed based on this values and a sudden change in amplitude is detected on blinking. The Attention and Meditation values are also displayed. This app is connected to mindwave headset via Bluetooth. When user go beyond the meditation value of 85, Flash light of the mobile phone automatically gets turned on. Controlling Light by Mind : Circuit Diagram \u00b6 Main components of the circuit are: \u00b6 Arduino Uno R3 Bluetooth Module HC \u2013 05 4 Channel Relay Module Bulb Power Supply HC-05 Module has 3 pins in use: \u00b6 VCC \u2013 Connected to Digital Pin 5 of Arduino (5 Volt Supply) GND \u2013 Connected to Ground Pin of Arduino Tx \u2013 Transmitter Pin is connected to RX pin of Arduino Relay module has 5 pins in use: \u00b6 VCC \u2013 Connected to VCC of Arduino (5 Volt Supply) GND - Connected to Ground Pin of Arduino IN2 \u2013 As an input pin for relay 2 since we have connected bulb to relay 2. This pin is connected to Digital Pin no. 2 of Arduino. C \u2013 It is common pin of relay connected to Hot line of 250 V Ac supply. NO - Normally open pin is connected to Positive terminal of bulb. Negative terminal of Bulb is connected to Neutral line of power supply. Mind wave Head set is wirelessly connected to Arduino via Bluetooth connection. Setting up Bluetooth module \u00b6 HC-05 by default can be paired to any device. But for our project we specifically need to pair the module with mind wave headset. For that We first turned on our module in COMMAND MODE by holding a push button provided on top of HC-05. To send AT commands to HC-05, we coded the Arduino as shown below and connected the module to it. AT commands for setting up module are listed below: \u00b6 AT+NAME=SAMBHAV: Sets name of Bluetooth device to \u201cSambhav\u201d AT+UART=57600,0,0: Sets Baud rate to 57600 as mindwave sends data at this rate. AT+PSWD=0000: \u201c0000\u201d is default password for pairing up with mindwave head set AT+BIND=MMM,YY,NNNNN: We mentioned the Unique Identifier Number of Our Headset here which we get from device properties of headset. Algorithm \u00b6 Once the connection is established, mind wave sends the data in 32-byte packet. We need to retrieve the data from packet. Based on the attention and meditation value of mind we can send signal to relay for controlling light. Packet is described here. Byte Value Explanation 0 0xAA [SYNC] Synchronisation bit 1 0xAA [SYNC] Synchronisation bit 2 0x08 (payload length) of 8 bytes 3 0x02 POOR_SIGNAL Quality 4 0x20 Some poor signal detected (32/255) 5 0x01 BATTERY Level 6 0x7E Almost full 3V of battery (126/127) 7 0x04 ATTENTION 8 0x12 Attention level of 18% 9 0x05 MEDITATION 10 0x60 Meditation level of 96% 11 0xE3 [CHKSUM] (1's comp inverse of 8-bit Payload sum of 0x1C) Parsing a Packet \u00b6 Keep reading bytes from the stream until a [SYNC] byte (0xAA) is encountered. Read the next byte and ensure it is also a [SYNC] byte If not a [SYNC] byte, return to step 1. Otherwise, continue to step 3. Read the next byte from the stream as the [PLENGTH]. If [PLENGTH] is 170 ([SYNC]), then repeat step 3. If [PLENGTH] is greater than 170, then return to step 1 (PLENGTH TOO LARGE). Otherwise, continue to step 4. Read the next [PLENGTH] bytes of the [PAYLOAD\u2026] from the stream, saving them into a storage area (such as an unsigned char payload [256] array). Sum up each byte as it is read by incrementing a checksum accumulator (checksum += byte). Take the lowest 8 bits of the checksum accumulator and invert them. Here is the C code: checksum &= 0xFF; checksum = ~checksum & 0xFF; Read the next byte from the stream as the [CHKSUM] byte. If the [CHKSUM] does not match your calculated chksum (CHKSUM FAILED). Otherwise, you may now parse the contents of the Payload into DataRows to obtain the Data Values, as described below. In either case, return to step 1. Parsing Data Rows in a Packet Payload \u00b6 Repeat the following steps for parsing a DataRow until all bytes in the payload[] array ([PLENGTH] bytes) have been considered and parsed: Parse and count the number of [EXCODE] (0x55) bytes that may be at the beginning of the current DataRow. Parse the [CODE] byte for the current DataRow. If [CODE] >= 0x80, parse the next byte as the [VLENGTH] byte for the current DataRow. Parse and handle the [VALUE\u2026] byte(s) of the current DataRow, based on the DataRow's [EXCODE] level, [CODE], and [VLENGTH]. If not all bytes have been parsed from the payload[] array, return to step 1. to continue parsing the next DataRow. Get Attention Value from [EXCODE] 0x04 Get Meditation Value from [EXCODE] 0x05 If Attention is greater than threshold, then turn on light else go to step 9. If Meditation is greater than threshold, then turn off light else go to step 1. References \u00b6 Documentation and algorithm for mindwave headset","title":"Mind Controlled Home Automation"},{"location":"projects/mind_controlled_home_automation/#mind-controlled-home-automation","text":"","title":"Mind Controlled Home Automation"},{"location":"projects/mind_controlled_home_automation/#introduction","text":"Mind Controlled Home Automation System with self-learning capability \u2013 an approach to enter the era of virtual reality, an era where everything is possible. That\u2019s why we named it as SAMBHAV. The main aim of our project is to control home appliances with human brain acting as an interface. This means that the signals generated by our brain will now control electrical appliances \u2013 termed as Home Automation. For achieving the same, we have used MindWave headset that calculates our concentration(attention) and meditation(relaxation) and even detects the blink. The combination of these human activities will result into switching ON/OFF devices.","title":"Introduction"},{"location":"projects/mind_controlled_home_automation/#about-headset","text":"A closer look at the headset :","title":"About Headset"},{"location":"projects/mind_controlled_home_automation/#basic-functionalities-of-headset","text":"Power Switch \u2013 To power ON/OFF the headset. If the switch is held past the ON position for 3 seconds, the headset will enter the Bluetooth Pairing Mode and if held past the ON position for 6 seconds, the headset\u2019s pairing memory will be cleared. Ear Clip \u2013 Acts as the reference point and is clipped at the earlobe (lower part of the ear). Sensor Arm \u2013 Core of the headset. It contains the sensor that should be in direct contact with the forehead. Besides sensor, it also includes the on-board chip that processes all of the data.","title":"Basic functionalities of headset:"},{"location":"projects/mind_controlled_home_automation/#working","text":"The biosensor placed inside the headset measures the frequencies of the electrical signals emitted by the nerve cells. Although neurons are not so good conductor of electricity, yet they generate electrical signals based on the flow of ions across their plasma membranes. Basically, neurons generate a negative potential, that is measured by recording the voltage between the inside and outside of nerve cells. The sensor chip does this work with ear clip acting as the reference point and the electrical signals measured are commonly referred to as brainwaves. The sensor chip then amplifies the brainwave signals and removes the unnecessary noise and muscle movement. These analog signals are then converted into digital signals by the on-board chip and is being processed to the device. The table below shows some of the frequencies that tend to be generated by different types of activity in the brain: Brainwave Type Frequency range Mental states and conditions Delta 0.1Hz to 3Hz Deep, dreamless sleep, non-REM sleep, unconscious Theta 4Hz to 7Hz Intuitive, creative, recall, fantasy, imaginary, dream Alpha 8Hz to 12Hz Relaxed (but not drowsy) tranquil, conscious Low Beta 12Hz to 15Hz Formerly SMR, relaxed yet focused, integrated Midrange Beta 12Hz to 15Hz Thinking, aware of self & surroundings High Beta 21Hz to 30Hz Alertness, agitation","title":"Working"},{"location":"projects/mind_controlled_home_automation/#android-app-for-analysis","text":"We created an android app for analyzing various signals generated by our brain on pursuing different activities. App is used to display different EEG band power values such as Delta, Theta, Alpha and Gamma etc. At the top of the screen, Graph is displayed based on this values and a sudden change in amplitude is detected on blinking. The Attention and Meditation values are also displayed. This app is connected to mindwave headset via Bluetooth. When user go beyond the meditation value of 85, Flash light of the mobile phone automatically gets turned on.","title":"Android App for analysis"},{"location":"projects/mind_controlled_home_automation/#controlling-light-by-mind-circuit-diagram","text":"","title":"Controlling Light by Mind : Circuit Diagram"},{"location":"projects/mind_controlled_home_automation/#main-components-of-the-circuit-are","text":"Arduino Uno R3 Bluetooth Module HC \u2013 05 4 Channel Relay Module Bulb Power Supply","title":"Main components of the circuit are:"},{"location":"projects/mind_controlled_home_automation/#hc-05-module-has-3-pins-in-use","text":"VCC \u2013 Connected to Digital Pin 5 of Arduino (5 Volt Supply) GND \u2013 Connected to Ground Pin of Arduino Tx \u2013 Transmitter Pin is connected to RX pin of Arduino","title":"HC-05 Module has 3 pins in use:"},{"location":"projects/mind_controlled_home_automation/#relay-module-has-5-pins-in-use","text":"VCC \u2013 Connected to VCC of Arduino (5 Volt Supply) GND - Connected to Ground Pin of Arduino IN2 \u2013 As an input pin for relay 2 since we have connected bulb to relay 2. This pin is connected to Digital Pin no. 2 of Arduino. C \u2013 It is common pin of relay connected to Hot line of 250 V Ac supply. NO - Normally open pin is connected to Positive terminal of bulb. Negative terminal of Bulb is connected to Neutral line of power supply. Mind wave Head set is wirelessly connected to Arduino via Bluetooth connection.","title":"Relay module has 5 pins in use:"},{"location":"projects/mind_controlled_home_automation/#setting-up-bluetooth-module","text":"HC-05 by default can be paired to any device. But for our project we specifically need to pair the module with mind wave headset. For that We first turned on our module in COMMAND MODE by holding a push button provided on top of HC-05. To send AT commands to HC-05, we coded the Arduino as shown below and connected the module to it.","title":"Setting up Bluetooth module"},{"location":"projects/mind_controlled_home_automation/#at-commands-for-setting-up-module-are-listed-below","text":"AT+NAME=SAMBHAV: Sets name of Bluetooth device to \u201cSambhav\u201d AT+UART=57600,0,0: Sets Baud rate to 57600 as mindwave sends data at this rate. AT+PSWD=0000: \u201c0000\u201d is default password for pairing up with mindwave head set AT+BIND=MMM,YY,NNNNN: We mentioned the Unique Identifier Number of Our Headset here which we get from device properties of headset.","title":"AT commands for setting up module are listed below:"},{"location":"projects/mind_controlled_home_automation/#algorithm","text":"Once the connection is established, mind wave sends the data in 32-byte packet. We need to retrieve the data from packet. Based on the attention and meditation value of mind we can send signal to relay for controlling light. Packet is described here. Byte Value Explanation 0 0xAA [SYNC] Synchronisation bit 1 0xAA [SYNC] Synchronisation bit 2 0x08 (payload length) of 8 bytes 3 0x02 POOR_SIGNAL Quality 4 0x20 Some poor signal detected (32/255) 5 0x01 BATTERY Level 6 0x7E Almost full 3V of battery (126/127) 7 0x04 ATTENTION 8 0x12 Attention level of 18% 9 0x05 MEDITATION 10 0x60 Meditation level of 96% 11 0xE3 [CHKSUM] (1's comp inverse of 8-bit Payload sum of 0x1C)","title":"Algorithm"},{"location":"projects/mind_controlled_home_automation/#parsing-a-packet","text":"Keep reading bytes from the stream until a [SYNC] byte (0xAA) is encountered. Read the next byte and ensure it is also a [SYNC] byte If not a [SYNC] byte, return to step 1. Otherwise, continue to step 3. Read the next byte from the stream as the [PLENGTH]. If [PLENGTH] is 170 ([SYNC]), then repeat step 3. If [PLENGTH] is greater than 170, then return to step 1 (PLENGTH TOO LARGE). Otherwise, continue to step 4. Read the next [PLENGTH] bytes of the [PAYLOAD\u2026] from the stream, saving them into a storage area (such as an unsigned char payload [256] array). Sum up each byte as it is read by incrementing a checksum accumulator (checksum += byte). Take the lowest 8 bits of the checksum accumulator and invert them. Here is the C code: checksum &= 0xFF; checksum = ~checksum & 0xFF; Read the next byte from the stream as the [CHKSUM] byte. If the [CHKSUM] does not match your calculated chksum (CHKSUM FAILED). Otherwise, you may now parse the contents of the Payload into DataRows to obtain the Data Values, as described below. In either case, return to step 1.","title":"Parsing a Packet"},{"location":"projects/mind_controlled_home_automation/#parsing-data-rows-in-a-packet-payload","text":"Repeat the following steps for parsing a DataRow until all bytes in the payload[] array ([PLENGTH] bytes) have been considered and parsed: Parse and count the number of [EXCODE] (0x55) bytes that may be at the beginning of the current DataRow. Parse the [CODE] byte for the current DataRow. If [CODE] >= 0x80, parse the next byte as the [VLENGTH] byte for the current DataRow. Parse and handle the [VALUE\u2026] byte(s) of the current DataRow, based on the DataRow's [EXCODE] level, [CODE], and [VLENGTH]. If not all bytes have been parsed from the payload[] array, return to step 1. to continue parsing the next DataRow. Get Attention Value from [EXCODE] 0x04 Get Meditation Value from [EXCODE] 0x05 If Attention is greater than threshold, then turn on light else go to step 9. If Meditation is greater than threshold, then turn off light else go to step 1.","title":"Parsing Data Rows in a Packet Payload"},{"location":"projects/mind_controlled_home_automation/#references","text":"Documentation and algorithm for mindwave headset","title":"References"},{"location":"projects/mobile_app_for_visually_impaired/","text":"","title":"Mobile app for visually impaired"},{"location":"projects/on_device_classification/","text":"On-Device Object Classification \u00b6 Transfer Learning \u00b6 Transfer learning is the technique for image classification where we use a pre-trained model trained on large-scale datasets (such as ImageNet) and adapt it for our small number of classes by only retraining the fully connected layer of the model from scratch. Generally, training models require powerful CPUs or GPUs. But since the transfer learning technique only requires a small amount of computing to retrain a single layer, we can utilize CPUs/GPUs of modern smartphone devices. That means users can train image classification models on their smartphones on the fly with only as few as 10 images per class. Inspiration \u00b6 I found this interesting blog in Example on-device model personalization with TensorFlow Lite which does something similar. The app uses transfer learning on a quantized MobileNetV2 model pre-trained on ImageNet with the last few layers replaced by a trainable softmax classifier. You can train the last layers to recognize any four new classes. Since the app was entirely written in java, I redesigned the backend of the app such that the core logic is wrapped inside in a C++ SDK. This SDK is portable such that it can be used on any hardware and os which supports c++ program execution. Workflow \u00b6 The below image explains the workflow of how this app works. APIs \u00b6 The C++ library exposes three functions that any app can use for building on-device classification apps. 1. Add an image \u00b6 Use can add an image and label to the training set. Internally we store the output of the bottleneck layer generated from the pretrained model. 2. Train model \u00b6 Randomly shuffles all the embeddings and re-trains the last fully connected layer. 3. Predict an image \u00b6 Generates embedding using a pre-trained model and multiplies it with weights of the finetuned fc layer and outputs the final result. Models \u00b6 There are five different TensorflowLite models to execute the entire flow. 1. bottleneck.tflite (Base model) \u00b6 Pretrained mobilenet model without the last fully connected layer. It is used for extracting learned features from the input image. For mobile CPUs, we use the fully quantized int8 model and for mobile GPUs, we use the fp16 model. 3. initialize.tflite \u00b6 To train our model on custom datasets, the library maintains a weights and biases array in the memory. The initialize.tflite model outputs these two matrices initialized with zeros. 2. train_head.tflite \u00b6 During the training phase, train_head.tflite model takes the output of the base model, current weights, current biases, and the ground truth labels. It then calculates the loss and returns gradients of the weights and biases w.r.t the loss. Those gradients are passed to the optimizer model to apply updates. 4. optimizer.tflite \u00b6 The optimizer model takes current weights and biases and gradients of weights and biases to apply a gradient descent algorithm ( new_weights = old_weights - learning_rate * gradients ) to calculate new weights and biases. We perform this step after each invocation of the train_head.tflite model during the training phase. The library replaces current parameters with the updated parameters in the memory. 5. inference.tflite (Head model) \u00b6 The inference model takes three inputs, the output of the bottleneck model, the trained weights matrix, and the trained bias values. It calculates the final probability using the softmax(w*x + b) formula. Extending to object detection \u00b6 I extended this idea to object detection by using a bottleneck layer of mobilenet-ssd model trained on the COCO dataset and finetuning the bounding box and class prediction layers on the device itself. The primary results were good if there is very change in the test image compared to the training image, but it required more images for the model to produce good results when the test image is taken under different conditions. Though this can be a good future work.","title":"On-Device Object Classification"},{"location":"projects/on_device_classification/#on-device-object-classification","text":"","title":"On-Device Object Classification"},{"location":"projects/on_device_classification/#transfer-learning","text":"Transfer learning is the technique for image classification where we use a pre-trained model trained on large-scale datasets (such as ImageNet) and adapt it for our small number of classes by only retraining the fully connected layer of the model from scratch. Generally, training models require powerful CPUs or GPUs. But since the transfer learning technique only requires a small amount of computing to retrain a single layer, we can utilize CPUs/GPUs of modern smartphone devices. That means users can train image classification models on their smartphones on the fly with only as few as 10 images per class.","title":"Transfer Learning"},{"location":"projects/on_device_classification/#inspiration","text":"I found this interesting blog in Example on-device model personalization with TensorFlow Lite which does something similar. The app uses transfer learning on a quantized MobileNetV2 model pre-trained on ImageNet with the last few layers replaced by a trainable softmax classifier. You can train the last layers to recognize any four new classes. Since the app was entirely written in java, I redesigned the backend of the app such that the core logic is wrapped inside in a C++ SDK. This SDK is portable such that it can be used on any hardware and os which supports c++ program execution.","title":"Inspiration"},{"location":"projects/on_device_classification/#workflow","text":"The below image explains the workflow of how this app works.","title":"Workflow"},{"location":"projects/on_device_classification/#apis","text":"The C++ library exposes three functions that any app can use for building on-device classification apps.","title":"APIs"},{"location":"projects/on_device_classification/#1-add-an-image","text":"Use can add an image and label to the training set. Internally we store the output of the bottleneck layer generated from the pretrained model.","title":"1. Add an image"},{"location":"projects/on_device_classification/#2-train-model","text":"Randomly shuffles all the embeddings and re-trains the last fully connected layer.","title":"2. Train model"},{"location":"projects/on_device_classification/#3-predict-an-image","text":"Generates embedding using a pre-trained model and multiplies it with weights of the finetuned fc layer and outputs the final result.","title":"3. Predict an image"},{"location":"projects/on_device_classification/#models","text":"There are five different TensorflowLite models to execute the entire flow.","title":"Models"},{"location":"projects/on_device_classification/#1-bottlenecktflite-base-model","text":"Pretrained mobilenet model without the last fully connected layer. It is used for extracting learned features from the input image. For mobile CPUs, we use the fully quantized int8 model and for mobile GPUs, we use the fp16 model.","title":"1. bottleneck.tflite (Base model)"},{"location":"projects/on_device_classification/#3-initializetflite","text":"To train our model on custom datasets, the library maintains a weights and biases array in the memory. The initialize.tflite model outputs these two matrices initialized with zeros.","title":"3. initialize.tflite"},{"location":"projects/on_device_classification/#2-train_headtflite","text":"During the training phase, train_head.tflite model takes the output of the base model, current weights, current biases, and the ground truth labels. It then calculates the loss and returns gradients of the weights and biases w.r.t the loss. Those gradients are passed to the optimizer model to apply updates.","title":"2. train_head.tflite"},{"location":"projects/on_device_classification/#4-optimizertflite","text":"The optimizer model takes current weights and biases and gradients of weights and biases to apply a gradient descent algorithm ( new_weights = old_weights - learning_rate * gradients ) to calculate new weights and biases. We perform this step after each invocation of the train_head.tflite model during the training phase. The library replaces current parameters with the updated parameters in the memory.","title":"4. optimizer.tflite"},{"location":"projects/on_device_classification/#5-inferencetflite-head-model","text":"The inference model takes three inputs, the output of the bottleneck model, the trained weights matrix, and the trained bias values. It calculates the final probability using the softmax(w*x + b) formula.","title":"5. inference.tflite (Head model)"},{"location":"projects/on_device_classification/#extending-to-object-detection","text":"I extended this idea to object detection by using a bottleneck layer of mobilenet-ssd model trained on the COCO dataset and finetuning the bounding box and class prediction layers on the device itself. The primary results were good if there is very change in the test image compared to the training image, but it required more images for the model to produce good results when the test image is taken under different conditions. Though this can be a good future work.","title":"Extending to object detection"},{"location":"projects/small_face_detection/","text":"","title":"Small face detection"},{"location":"projects/spry/","text":"Hospital Management System \u00b6 Spry was a hobby project I created for my family doctor. It provides some helpful features for the doctors to manage their patients' data through this web app. Responsive Dashboard \u00b6 List of features \u00b6 1. Add symptoms and treatments \u00b6 2. Track patient's blood pressure history. \u00b6 3. Track paid and unpaid amount. \u00b6 4. Create unique patient profile \u00b6 5. Check patient's medical history. \u00b6 6. Search patients by multiple fields. \u00b6 7. Real time sync with cloud \u00b6","title":"Hospital Management System"},{"location":"projects/spry/#hospital-management-system","text":"Spry was a hobby project I created for my family doctor. It provides some helpful features for the doctors to manage their patients' data through this web app.","title":"Hospital Management System"},{"location":"projects/spry/#responsive-dashboard","text":"","title":"Responsive Dashboard"},{"location":"projects/spry/#list-of-features","text":"","title":"List of features"},{"location":"projects/spry/#1-add-symptoms-and-treatments","text":"","title":"1. Add symptoms and treatments"},{"location":"projects/spry/#2-track-patients-blood-pressure-history","text":"","title":"2. Track patient's blood pressure history."},{"location":"projects/spry/#3-track-paid-and-unpaid-amount","text":"","title":"3. Track paid and unpaid amount."},{"location":"projects/spry/#4-create-unique-patient-profile","text":"","title":"4. Create unique patient profile"},{"location":"projects/spry/#5-check-patients-medical-history","text":"","title":"5. Check patient's medical history."},{"location":"projects/spry/#6-search-patients-by-multiple-fields","text":"","title":"6. Search patients by multiple fields."},{"location":"projects/spry/#7-real-time-sync-with-cloud","text":"","title":"7. Real time sync with cloud"}]}