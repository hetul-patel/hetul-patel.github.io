{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hello! \ud83d\udc4b \u00b6 I build AI that helps someone somewhere everyday.","title":"Hello! \ud83d\udc4b"},{"location":"#hello","text":"I build AI that helps someone somewhere everyday.","title":"Hello! \ud83d\udc4b"},{"location":"contact/","text":"I am, Somewhere out there. Some wonderful place on our planet earth. Wandering and Wondering. You can write to me at, hetulvp@gmail.com Or connect via the digital universe. GitHub LinkedIn Instagram","title":"Contact"},{"location":"education/","text":"Nirma University Aug, 2014 - May, 2018 Institute of technology, Nirma University, Ahmedabad, India. Bachelor of Technology in Computer Science and Engineering CGPA: 8.12 / 10.0 Gujarat Secondary and Higher Secondary Education Board April, 2014 Utkarsh vidyalaya, Vadodara, India. Higher Secondary School Certificate Percentile: 99.87 Gujarat Secondary and Higher Secondary Education Board March, 2012 Bright School, Vadodara, India. Secondary School Certificate Percentile: 99.98","title":"Education"},{"location":"experience/","text":"Machine Learning Engineer - II Feb, 2021 - Present InFoCusp Innovations Private Limited , Ahmedabad, India. Client: Google-X, the Moonshot Factory (Confidential Research Project) Neural Machine Translation Computer vision The goal was to create a single virtualized view of the electricity system through an aerial view imaginary using an image segmentation model. I explored various state-of-the-art image segmentation models such as PSP-NET, ViT, etc., and combined the best of all in a single model to achieve production-grade results. I also developed a novel metric for the evaluation of the model's performance as traditional segmentation metrics failed to provide helpful indications of failure cases. While the new metric helped filter failure cases easily which was then improved through data augmentation techniques. GRAD-CAM and its variant were used to locate features in the image when the model failed to produce the correct output. This analysis helped improve the quality of the training and evaluation dataset. Client: Innovyze, An Autodesk company. At Innovyze, I helped Process Engineers to optimize chemical consumption in the water treatment plant by analyzing historical sensor data and developing predictive models to automate the processes. My primary tasks were Data analysis, Feature engineering, and Modelling. Software Development Engineer - II (AI/ML) May, 2018 - Feb, 2021 Matrix Comsec R&D , Vadodara, India. Matrix ComSec is a leader in Security and Telecom solutions for modern businesses and enterprises with 1M+ customers in 50+ countries. I was responsible for developing and delivering DL algorithms in SDK form. My major contributions are listed below, Face Recognition (FR) and Face Detection (FD) Developed FD algorithm to detect multiple faces in the image with a minimum face size of 30px. Developed FR algorithm with 99.85 % accuracy on the LFW benchmark. Improved existing FR algorithm to identify people wearing a mask on the face with only 5% accuracy drop wrt to full-face model. It helped employees to mark attendance without removing their masks during the coronavirus pandemic Developed a Face Mask Detection model to identify whether employees are wearing masks or not when marking attendance to improve their safety. Developed CNN for a single RGB image-based Passive Face Antispoofing model to prevent fake attendance marking using a mobile phone or printed photo. Delivered all these features in a single FR SDK for RPi3/4, CPU, GPU, and Android devices with a maximum latency of 800 ms on the slowest hardware Automated License Plate Recognition (ALPR) Developed a real-time Licence Plate Detection model which achieved 99%+ recall and 93%+ precision on the internal Indian vehicle test dataset Contributed to the development of the CNN model for Licence Plate Recognition using CTC Loss. We achieved 87% accuracy on challenging the Indian test datasets outperforming other commercial solutions by at least 15% and 98% accuracy on the OpenALPR benchmark Developed semi-supervised data annotation tool which helped us collect a labeled dataset of 30K+ images and incrementally improved the model's accuracy Designed ALPR SDK architecture for improving throughput by batching simultaneous requests into single inference. A myriad of inference engines such as TensorFlow, TensorRT, OpenVINO, TensorflowLite, etc. can be integrated based on hardware without modifying 95% of the codebase. Seven Segment Display Number Recognition To handle a variety of color digital displays, a self-calibration algorithm was designed to run once to extract display-specific properties using KNN and an inference algorithm was designed to use these properties for recognizing digits in real-time using SVM. The algorithm was integrated into existing weighbridge vehicle management software for automating data entry of the vehicle's weight from the digital display through the camera. It reduced manual human intervention by 95% and introduced transparency in the whole process. Deep Learning Research Intern Jan, 2018 - May, 2018 Space Applications Centre (SAC) - ISRO, Ahmedabad, Gujarat, India SAC being the major R&D center of ISRO designs and develops the optical and microwave sensors for the satellites, signal and image processing software, GIS software, and many applications for the Earth Observation (EO) program of ISRO. As a Research Intern, I developed DL techniques for accurate crop classification using Hyper Spectral Satellite images (425 channels per pixel). My primary contributions were as listed below Developed ANN-based dimensionality reduction technique which retained useful features and worked better than PCA. Developed FR algorithm with 99.85 % accuracy on the LFW benchmark. Developed virtual data augmentation technique for overcoming the issue of insufficient ground truth data of different crops for training. Provided detailed comparative analysis of ANN, CNN, and SVM. Designed highly accurate Parallel-CNNs architecture for classifying nearly inseparable classes based on interclass separability analysis. Developed Python library based on my work so that other scientists can use these techniques on a variety of other satellite images. Machine Learning Summer Intern May, 2017 - June, 2017 Wolfsoft Pvt. Ltd., Vadodara, Gujarat, India. During my internship I worked on a food review app similar to Zomato and my contributions were as listed below, Developed a Food Recommendation API based on a Collaborative Filtering algorithm. Developed a Food Search API. Normalized and stored posting list of Indian food names in the Trie data structure for faster and more accurate search. Added spelling correction using the Levenshtein distance algorithm which achieved higher recall compared to SQL's search.","title":"Experience"},{"location":"publications/","text":"Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques @INPROCEEDINGS{8897897, author={Patel, Hetul and Bhagia, Nita and Vyas, Tarjni and Bhattacharya, Bimal and Dave, Kinjal}, booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, title={Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques}, year={2019}, volume={}, number={}, pages={3728-3731}, doi={10.1109/IGARSS.2019.8897897}}","title":"Publications"},{"location":"projects/3d_password/","text":"3D Password \u00b6 Introduction \u00b6 3D password is an advancement in the field of user authentication techniques. 3d password can have large number of possible combinations as integration of textual, graphical and biometric. So it is technically very hard to crack the one. 3d password is setup in any computer screen which includes a virtual environment where user can set different objects as password. Below is the list object which can be used as 3D password, An Iris Scanner A Fingerprint Scanner A Virtual ATM machine that requires real ATM card A text input device which take input Photo album as pictorial password So any real world object can be integrated in the 3D environment and any new authentication technique can also be included if it is invented after. Working of 3d environment \u00b6 Every object in a 3D environment has its unique (x,y,z) coordinates. If the size of the 3D world is M x M x M then the entire virtual world can be represented by [1,2,3......, M] x [1,2,3......, M] x [1,2,3......, M] . User can move in the 3d environment using mouse and keyboard. Also biometric scanners, stylus pen and ATM card reader can be attached for interactions with virtual models of it. The coordinates are stored when any interaction is performed with the object. E.g. user opens a door located at (10,15,60) , He closes the door and enters the office. Take a marker located at (25,20,70) and write \u201cHello\u201d on the white board located at (25,52,80) . This interaction is stored as 3D password as follows: (10,15,60) Object: door Action: Opening the door, (10,15,60) Object: door Action: Closing the door, (25,20,70) Object: marker Action: Take the marker, (25,52,80) Object: white board Action: Writing \u201cH\u201d, (25,52,80) Object: white board Action: Writing \u201cE\u201d, (25,52,80) Object: white board Action: Writing \u201cL\u201d, (25,52,80) Object: white board Action: Writing \u201cL\u201d, (25,52,80) Object: white board Action: Writing \u201cO\u201d. Implementing 3D Password \u00b6 3D password implementation requires the integration of software and hardware. 3D objects can be designed in 3D modelling software such as Adobe Maya, Blender etc. This 3D objects are imported in a 3D world which can be created in gave development software like UNITY 3D. The Serial Input and Output can be driven through USB port. I have implemented small 3D Password using UNITY 3D. The steps are as follows: Step 1: Read all the instructions carefully. \u00b6 Figure 4.1 shows the instruction to be followed to travel in the 3D world. Use Keyboard direction keys to move in linear direction and mouse to move in angular direction. Step 2: Open the Right or Left gate to enter into the Nirma\u2019s Gate. \u00b6 As shown in figure 4.2 You will see the entrance gate of Nirma. User has to go Security guard and use the mouse to click on LEFT or RIGHT button to open the left or right gate respectively. As The password is predefined here I\u2019ll open the LEFT gate as shown in fig 4.3 and enter into the college. Step 3: Go to correct colony and choose correct photo frame. \u00b6 As shown in figure 4.4, You will see 3 identical colonies with same photo frames hanged on the walls. The difference is that each colony has different number of trees near it. We will go first the colony which has 3 trees near it As shown in figure 4.5 there are 4 photo frames hanged on the wall from the left Steve jobs Mahatama Gandhi Karsanbhai Patel Bill Gates I will select the Mahatama Gandhi photo frame by clicking on it. The photo will come forward as selected as shown in figure 4.6 Then I\u2019ll go to the another colony with two trees. And select the Karsanbhai patel\u2019s photo frame as it is my predefined password. Step 4: Click the login button to get the access. \u00b6 After performing the correct sequence of interactions, I will go to the last stage of my password where I click the login button as shown in figure 4.8. Now If all the actions are correctly performed, I will be logged into the Nirma University System as show in figure 4.9. Else it will show me an Invalid password error as shown in figure 4.10.","title":"3D Password"},{"location":"projects/3d_password/#3d-password","text":"","title":"3D Password"},{"location":"projects/3d_password/#introduction","text":"3D password is an advancement in the field of user authentication techniques. 3d password can have large number of possible combinations as integration of textual, graphical and biometric. So it is technically very hard to crack the one. 3d password is setup in any computer screen which includes a virtual environment where user can set different objects as password. Below is the list object which can be used as 3D password, An Iris Scanner A Fingerprint Scanner A Virtual ATM machine that requires real ATM card A text input device which take input Photo album as pictorial password So any real world object can be integrated in the 3D environment and any new authentication technique can also be included if it is invented after.","title":"Introduction"},{"location":"projects/3d_password/#working-of-3d-environment","text":"Every object in a 3D environment has its unique (x,y,z) coordinates. If the size of the 3D world is M x M x M then the entire virtual world can be represented by [1,2,3......, M] x [1,2,3......, M] x [1,2,3......, M] . User can move in the 3d environment using mouse and keyboard. Also biometric scanners, stylus pen and ATM card reader can be attached for interactions with virtual models of it. The coordinates are stored when any interaction is performed with the object. E.g. user opens a door located at (10,15,60) , He closes the door and enters the office. Take a marker located at (25,20,70) and write \u201cHello\u201d on the white board located at (25,52,80) . This interaction is stored as 3D password as follows: (10,15,60) Object: door Action: Opening the door, (10,15,60) Object: door Action: Closing the door, (25,20,70) Object: marker Action: Take the marker, (25,52,80) Object: white board Action: Writing \u201cH\u201d, (25,52,80) Object: white board Action: Writing \u201cE\u201d, (25,52,80) Object: white board Action: Writing \u201cL\u201d, (25,52,80) Object: white board Action: Writing \u201cL\u201d, (25,52,80) Object: white board Action: Writing \u201cO\u201d.","title":"Working of 3d environment"},{"location":"projects/3d_password/#implementing-3d-password","text":"3D password implementation requires the integration of software and hardware. 3D objects can be designed in 3D modelling software such as Adobe Maya, Blender etc. This 3D objects are imported in a 3D world which can be created in gave development software like UNITY 3D. The Serial Input and Output can be driven through USB port. I have implemented small 3D Password using UNITY 3D. The steps are as follows:","title":"Implementing 3D Password"},{"location":"projects/3d_password/#step-1-read-all-the-instructions-carefully","text":"Figure 4.1 shows the instruction to be followed to travel in the 3D world. Use Keyboard direction keys to move in linear direction and mouse to move in angular direction.","title":"Step 1: Read all the instructions carefully."},{"location":"projects/3d_password/#step-2-open-the-right-or-left-gate-to-enter-into-the-nirmas-gate","text":"As shown in figure 4.2 You will see the entrance gate of Nirma. User has to go Security guard and use the mouse to click on LEFT or RIGHT button to open the left or right gate respectively. As The password is predefined here I\u2019ll open the LEFT gate as shown in fig 4.3 and enter into the college.","title":"Step 2: Open the Right or Left gate to enter into the Nirma\u2019s Gate."},{"location":"projects/3d_password/#step-3-go-to-correct-colony-and-choose-correct-photo-frame","text":"As shown in figure 4.4, You will see 3 identical colonies with same photo frames hanged on the walls. The difference is that each colony has different number of trees near it. We will go first the colony which has 3 trees near it As shown in figure 4.5 there are 4 photo frames hanged on the wall from the left Steve jobs Mahatama Gandhi Karsanbhai Patel Bill Gates I will select the Mahatama Gandhi photo frame by clicking on it. The photo will come forward as selected as shown in figure 4.6 Then I\u2019ll go to the another colony with two trees. And select the Karsanbhai patel\u2019s photo frame as it is my predefined password.","title":"Step 3: Go to correct colony and choose correct photo frame."},{"location":"projects/3d_password/#step-4-click-the-login-button-to-get-the-access","text":"After performing the correct sequence of interactions, I will go to the last stage of my password where I click the login button as shown in figure 4.8. Now If all the actions are correctly performed, I will be logged into the Nirma University System as show in figure 4.9. Else it will show me an Invalid password error as shown in figure 4.10.","title":"Step 4: Click the login button to get the access."},{"location":"projects/alpr/","text":"","title":"Alpr"},{"location":"projects/clothes_retrieval/","text":"Clothes Retrieval \u00b6 Searching for exact cloth image accurately from massive collections of clothes' images based on a query image. For example, here the left most image is the query image and other images are matched images retrieved from a huge collection of clothes using a computer vision algorithm. Dataset Details \u00b6 Dataset used : Deep Fashion 2 Problem statement : Consumer-to-shop Clothes Retrieval Problem description : Matching consumer-taken photos with their shop counterparts Code \u00b6 Google Colab Building the intuition \u00b6 Sorted in the order of how I reached the solution 1. Usecase understanding \u00b6 Consumer can upload a photo of clothing item System should retrieve similar looking items from shopping catalog. 2. Dataset understanding \u00b6 In the given task, we have two different sources of data . Consumer captured images : These are low quality images captured using phone's camera (front or back). Images have variation in lighting, orientation, occlusion, filters etc. Shop captured images : Shop images include good quality photo shoot quality images. It also includes images from online shopping carts where background is removed etc. 3. Real world challenges \u00b6 Labeling the dataset is major challenge Shopping image collection including online and offline stores can be huge Thousands of unique clothing items Manually pairing each user captured image to a similar shop image costs lots of human effort When the product is new, we might not have huge collection of user captured images. In that case the system cannot work better for new styles Solving the problem \u00b6 Considering the scarcity of labeled consumer to shop pairs, I decided build two stage pipeline . First learning task agnostic attributes from raw data then use them to reduce consumer to shop domain gap using fewer labeled images. Step 1 : Learning directly from raw images without labeling \u00b6 Observations Humans are good at pattern matching . We recognize many things from their attributes . For e.g we recognize a vehicle with wheels, seats, glasses, horn etc. We also use this attributes to distinguish between two different vehicles We learn about these attributes by comparing between many instances unconsciously Inspirations Deep learning model can also learn similar attributes by just observing across different images In many image retrieval problems we consider output of pre-fc layer as embedding We use metric learning approaches such as triplet loss to lean discriminative embedding We can consider each dimension in embedding vector as one attribute This attributes make the embedding vector discriminative Difference from metric learning Metric learning approaches try to increase distance between embeddings as a whole . For that we need positive and negative pairs . Instead we can increase distance between attributes of embedding without labels . Here, We do not try to teach which attribute represents what ? We try to teach that no two attributes should represent same concept . Mathematical Formulation We often calculate similarity of two vectors using cosine similarity measure. Cosine similarity has an interpretation as the cosine of the angle between the two vectors Cosine similarity is not invariant to shifts . If x was shifted to x+1, the cosine similarity would change. Unlike the cosine, the correlation is invariant to both scale and location changes of x and y. Correlation is the cosine similarity between centered versions of x and y In a batch of images, attribute vector represents values of particular dimension across all images For e.g if batch size = 512 and embedding dim = 2048, then we have 2048 attribute vectors each of length 512 Now consider another batch of slightly different version of the same images in first batch. Our goal is to Maximize cosine similarity of same attributes in both batches Minimize cosine similarity of different attributes in both batches Here, I used correlation as a proxy loss as both have same output range [-1,1] I divided loss function in two parts Correlation of attribute vectors at same index should be 1 . To make model generalize better I decreased the predicted correlation by margin m. For e.g if correlation is 0.9 we make it 0.6 so model tries to make it higher. Correlation of attribute vectors at different index should be 0 . Since there are N^2-N pairs of different attribute vectors I assigned lower weight compared to positive attribute pair. Code for attribute loss def attribute_loss(emb_1, emb_2, alpha=5e-4, margin=0.3): # per feature-dimension normalisation standardize = lambda x : (x - tf.reduce_mean(x, axis=0)) / tf.math.reduce_std(x, axis=0) emb_1_std = standardize(emb_1) # BxE emb_2_std = standardize(emb_2) # BxE # similarity of pairwise feature-dimension bsize = tf.cast(tf.shape(emb_1)[0], tf.float32) cos_t = tf.matmul(emb_1_std, emb_2_std, transpose_a=True) / bsize # Embed x Embed acos_t = tf.acos(tf.clip_by_value(cos_t, -1.0, 1.0)) # diagonal values rep. expected similarity of same feature-dim same_dim_mask = tf.eye(emb_1.shape[1]) # Embed x Embed # non-diagonal values rep. expected similarity of different features-dim diff_dim_mask = 1.0 - same_dim_mask # Embed x Embed # increase angle betweem same feature-dims : Cos(angle + m)(i==j) cosine_same = tf.cos(acos_t + margin) same_dim_loss = tf.square(same_dim_mask - cosine_same) * same_dim_mask # decrease angle between different feature-dims : Cos(angle - m)(i!=j) # cosine_diff = tf.cos(acos_t - margin) diff_dim_loss = tf.square(same_dim_mask - cos_t) * diff_dim_mask # final weighted loss weighted_loss = tf.reduce_sum(same_dim_loss + diff_dim_loss * alpha) return weighted_loss Step 2 : Reducing the domain gap using pinch of supervision \u00b6 Model trained with attribute loss can learn diverse set of attributes for each image. Although consumer and shop images have inherent biases due to different source of data generation This is very well known as domain gap where two similar objects can perceived differently by model due to pixel level differences I used combination of attribute loss and instance loss to reduce this domain gap with only 3 more epochs for finetuning 1. Attribute loss : In stage one, attribute loss was calculated between two different versions of same image as we did not have any other information Here we calculate it between attributes of same cloth type but one image from consumer and other from shop using label information 2. Instance loss : Attribute loss encourages model to learn features which are consistent in consumer and shop domain Instance loss is normal classification loss which uses these attributes to group similar cloths in tight clusters and increase distance between other groups. I used arcface as an instance loss. 3. Using weights from attribute model to Initialize backbone Initialize weights of newly added classification layer . Arcface tries to reduce cosine similarity of weights vector and instance vector. I used average of all embeddings produced by unsupervised model for each class as the initial weight vector of that class. Code for classifier weights calculation # Get classifier weights def classifier_weights(dataset, model, num_class): # calculate avg embedding for each class as w_init for fc out_layer = params.model.class_layer weights = np.zeros(shape=(num_class, model.outputs[out_layer].shape[-1]), dtype=np.float32) # (n_class, emb_dim) # (n_class,) extra 1 for ignoring zero div class_cnt = np.ones(shape=(num_class,), dtype=np.float32) for data in tqdm(dataset): user, shop, class_id = data all_images = tf.concat([user, shop], axis=0) class_ids = tf.concat([class_id, class_id], axis=0) embeds = model(all_images, training=False)[out_layer].numpy() weights[class_ids] += embeds class_cnt[class_ids] += 1 weights = np.divide(weights, class_cnt[:, np.newaxis]) return weights Experiments \u00b6 Model details \u00b6 Model Resnet50 Batch Size 512 Input resolution 96 Embedding dimension 2048 For training details refer code notebook Results \u00b6 Final Comments \u00b6 Although unsupervised loss has lower accuracy when source domains are different , it performs significantly better in mix domain (Exp 1 Last Block). Which suggests that attribute loss can work very well in task agnostic manner. Both attribute loss and fc7 weights initialization using avg class embedding improves results Supervised training from scratch performed significantly worse when training from scratch for 3 epoch Demo on validation dataset \u00b6 No image was used during training Results are in descending order of recall First column is consumer query image Other columns are retrieved shop images Green box is True Positive , Red box is False positive Model : Unsupervised Learning + Attribute Loss (Exp 1) \u00b6 Model : Attribute Loss + Instance Loss + fc7 weights init (Exp-2) \u00b6","title":"Clothes Retrieval"},{"location":"projects/clothes_retrieval/#clothes-retrieval","text":"Searching for exact cloth image accurately from massive collections of clothes' images based on a query image. For example, here the left most image is the query image and other images are matched images retrieved from a huge collection of clothes using a computer vision algorithm.","title":"Clothes Retrieval"},{"location":"projects/clothes_retrieval/#dataset-details","text":"Dataset used : Deep Fashion 2 Problem statement : Consumer-to-shop Clothes Retrieval Problem description : Matching consumer-taken photos with their shop counterparts","title":"Dataset Details"},{"location":"projects/clothes_retrieval/#code","text":"Google Colab","title":"Code"},{"location":"projects/clothes_retrieval/#building-the-intuition","text":"Sorted in the order of how I reached the solution","title":"Building the intuition"},{"location":"projects/clothes_retrieval/#1-usecase-understanding","text":"Consumer can upload a photo of clothing item System should retrieve similar looking items from shopping catalog.","title":"1. Usecase understanding"},{"location":"projects/clothes_retrieval/#2-dataset-understanding","text":"In the given task, we have two different sources of data . Consumer captured images : These are low quality images captured using phone's camera (front or back). Images have variation in lighting, orientation, occlusion, filters etc. Shop captured images : Shop images include good quality photo shoot quality images. It also includes images from online shopping carts where background is removed etc.","title":"2. Dataset understanding"},{"location":"projects/clothes_retrieval/#3-real-world-challenges","text":"Labeling the dataset is major challenge Shopping image collection including online and offline stores can be huge Thousands of unique clothing items Manually pairing each user captured image to a similar shop image costs lots of human effort When the product is new, we might not have huge collection of user captured images. In that case the system cannot work better for new styles","title":"3. Real world challenges"},{"location":"projects/clothes_retrieval/#solving-the-problem","text":"Considering the scarcity of labeled consumer to shop pairs, I decided build two stage pipeline . First learning task agnostic attributes from raw data then use them to reduce consumer to shop domain gap using fewer labeled images.","title":"Solving the problem"},{"location":"projects/clothes_retrieval/#step-1-learning-directly-from-raw-images-without-labeling","text":"Observations Humans are good at pattern matching . We recognize many things from their attributes . For e.g we recognize a vehicle with wheels, seats, glasses, horn etc. We also use this attributes to distinguish between two different vehicles We learn about these attributes by comparing between many instances unconsciously Inspirations Deep learning model can also learn similar attributes by just observing across different images In many image retrieval problems we consider output of pre-fc layer as embedding We use metric learning approaches such as triplet loss to lean discriminative embedding We can consider each dimension in embedding vector as one attribute This attributes make the embedding vector discriminative Difference from metric learning Metric learning approaches try to increase distance between embeddings as a whole . For that we need positive and negative pairs . Instead we can increase distance between attributes of embedding without labels . Here, We do not try to teach which attribute represents what ? We try to teach that no two attributes should represent same concept . Mathematical Formulation We often calculate similarity of two vectors using cosine similarity measure. Cosine similarity has an interpretation as the cosine of the angle between the two vectors Cosine similarity is not invariant to shifts . If x was shifted to x+1, the cosine similarity would change. Unlike the cosine, the correlation is invariant to both scale and location changes of x and y. Correlation is the cosine similarity between centered versions of x and y In a batch of images, attribute vector represents values of particular dimension across all images For e.g if batch size = 512 and embedding dim = 2048, then we have 2048 attribute vectors each of length 512 Now consider another batch of slightly different version of the same images in first batch. Our goal is to Maximize cosine similarity of same attributes in both batches Minimize cosine similarity of different attributes in both batches Here, I used correlation as a proxy loss as both have same output range [-1,1] I divided loss function in two parts Correlation of attribute vectors at same index should be 1 . To make model generalize better I decreased the predicted correlation by margin m. For e.g if correlation is 0.9 we make it 0.6 so model tries to make it higher. Correlation of attribute vectors at different index should be 0 . Since there are N^2-N pairs of different attribute vectors I assigned lower weight compared to positive attribute pair. Code for attribute loss def attribute_loss(emb_1, emb_2, alpha=5e-4, margin=0.3): # per feature-dimension normalisation standardize = lambda x : (x - tf.reduce_mean(x, axis=0)) / tf.math.reduce_std(x, axis=0) emb_1_std = standardize(emb_1) # BxE emb_2_std = standardize(emb_2) # BxE # similarity of pairwise feature-dimension bsize = tf.cast(tf.shape(emb_1)[0], tf.float32) cos_t = tf.matmul(emb_1_std, emb_2_std, transpose_a=True) / bsize # Embed x Embed acos_t = tf.acos(tf.clip_by_value(cos_t, -1.0, 1.0)) # diagonal values rep. expected similarity of same feature-dim same_dim_mask = tf.eye(emb_1.shape[1]) # Embed x Embed # non-diagonal values rep. expected similarity of different features-dim diff_dim_mask = 1.0 - same_dim_mask # Embed x Embed # increase angle betweem same feature-dims : Cos(angle + m)(i==j) cosine_same = tf.cos(acos_t + margin) same_dim_loss = tf.square(same_dim_mask - cosine_same) * same_dim_mask # decrease angle between different feature-dims : Cos(angle - m)(i!=j) # cosine_diff = tf.cos(acos_t - margin) diff_dim_loss = tf.square(same_dim_mask - cos_t) * diff_dim_mask # final weighted loss weighted_loss = tf.reduce_sum(same_dim_loss + diff_dim_loss * alpha) return weighted_loss","title":"Step 1 : Learning directly from raw images without labeling"},{"location":"projects/clothes_retrieval/#step-2-reducing-the-domain-gap-using-pinch-of-supervision","text":"Model trained with attribute loss can learn diverse set of attributes for each image. Although consumer and shop images have inherent biases due to different source of data generation This is very well known as domain gap where two similar objects can perceived differently by model due to pixel level differences I used combination of attribute loss and instance loss to reduce this domain gap with only 3 more epochs for finetuning 1. Attribute loss : In stage one, attribute loss was calculated between two different versions of same image as we did not have any other information Here we calculate it between attributes of same cloth type but one image from consumer and other from shop using label information 2. Instance loss : Attribute loss encourages model to learn features which are consistent in consumer and shop domain Instance loss is normal classification loss which uses these attributes to group similar cloths in tight clusters and increase distance between other groups. I used arcface as an instance loss. 3. Using weights from attribute model to Initialize backbone Initialize weights of newly added classification layer . Arcface tries to reduce cosine similarity of weights vector and instance vector. I used average of all embeddings produced by unsupervised model for each class as the initial weight vector of that class. Code for classifier weights calculation # Get classifier weights def classifier_weights(dataset, model, num_class): # calculate avg embedding for each class as w_init for fc out_layer = params.model.class_layer weights = np.zeros(shape=(num_class, model.outputs[out_layer].shape[-1]), dtype=np.float32) # (n_class, emb_dim) # (n_class,) extra 1 for ignoring zero div class_cnt = np.ones(shape=(num_class,), dtype=np.float32) for data in tqdm(dataset): user, shop, class_id = data all_images = tf.concat([user, shop], axis=0) class_ids = tf.concat([class_id, class_id], axis=0) embeds = model(all_images, training=False)[out_layer].numpy() weights[class_ids] += embeds class_cnt[class_ids] += 1 weights = np.divide(weights, class_cnt[:, np.newaxis]) return weights","title":"Step 2 : Reducing the domain gap using pinch of supervision"},{"location":"projects/clothes_retrieval/#experiments","text":"","title":"Experiments"},{"location":"projects/clothes_retrieval/#model-details","text":"Model Resnet50 Batch Size 512 Input resolution 96 Embedding dimension 2048 For training details refer code notebook","title":"Model details"},{"location":"projects/clothes_retrieval/#results","text":"","title":"Results"},{"location":"projects/clothes_retrieval/#final-comments","text":"Although unsupervised loss has lower accuracy when source domains are different , it performs significantly better in mix domain (Exp 1 Last Block). Which suggests that attribute loss can work very well in task agnostic manner. Both attribute loss and fc7 weights initialization using avg class embedding improves results Supervised training from scratch performed significantly worse when training from scratch for 3 epoch","title":"Final Comments"},{"location":"projects/clothes_retrieval/#demo-on-validation-dataset","text":"No image was used during training Results are in descending order of recall First column is consumer query image Other columns are retrieved shop images Green box is True Positive , Red box is False positive","title":"Demo on validation dataset"},{"location":"projects/clothes_retrieval/#model-unsupervised-learning-attribute-loss-exp-1","text":"","title":"Model :  Unsupervised Learning + Attribute Loss (Exp 1)"},{"location":"projects/clothes_retrieval/#model-attribute-loss-instance-loss-fc7-weights-init-exp-2","text":"","title":"Model :  Attribute Loss + Instance Loss + fc7 weights init (Exp-2)"},{"location":"projects/deep_face_recognition/","text":"","title":"Deep face recognition"},{"location":"projects/hyperspectral_image_classification/","text":"Hyper Spectral Image Classification \u00b6 Code \u00b6 Hyspeclib Library Code Manual Thesis Introduction \u00b6 Hyperspectral imaging which is also known as imaging spectroscopy, detects radiation of earth surface features in narrow contiguous spectral regions of the electromagnetic spectrum. The Airborne Visible Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) is an airborne hyperspectral sensor of NASA\u2019s Jet Propulsion Laboratory (JPL) with 425 spectral bands ranging from 380 nm to 2510 nm with a bandwidth of 5 nm and spatial resolution of 4-6 m. This study aims at pixel-wise identification and discrimination of crop types using AVIRIS-NG hyperspectral images, with novel Parallel Convolutional Neural Networks architecture. To tackle the challenge posed by a large number of correlated bands, we compare two band selection techniques using Principal Component Analysis (PCA) and back traversal of pre-trained Artificial Neural Network (ANN). Bands selected by PCA method Bands selected by ANN method We also propose an automated technique for augmentation of training dataset with a large number of pixels from unlabelled parts of an image, based on Euclidian distance. Experiments show that bands selected by ANN achieve higher accuracy compare to PCA selected bands with automated data augmentation. Paper \u00b6 Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques @INPROCEEDINGS{8897897, author={Patel, Hetul and Bhagia, Nita and Vyas, Tarjni and Bhattacharya, Bimal and Dave, Kinjal}, booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, title={Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques}, year={2019}, volume={}, number={}, pages={3728-3731}, doi={10.1109/IGARSS.2019.8897897}}","title":"Hyper Spectral Image Classification"},{"location":"projects/hyperspectral_image_classification/#hyper-spectral-image-classification","text":"","title":"Hyper Spectral Image Classification"},{"location":"projects/hyperspectral_image_classification/#code","text":"Hyspeclib Library Code Manual Thesis","title":"Code"},{"location":"projects/hyperspectral_image_classification/#introduction","text":"Hyperspectral imaging which is also known as imaging spectroscopy, detects radiation of earth surface features in narrow contiguous spectral regions of the electromagnetic spectrum. The Airborne Visible Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) is an airborne hyperspectral sensor of NASA\u2019s Jet Propulsion Laboratory (JPL) with 425 spectral bands ranging from 380 nm to 2510 nm with a bandwidth of 5 nm and spatial resolution of 4-6 m. This study aims at pixel-wise identification and discrimination of crop types using AVIRIS-NG hyperspectral images, with novel Parallel Convolutional Neural Networks architecture. To tackle the challenge posed by a large number of correlated bands, we compare two band selection techniques using Principal Component Analysis (PCA) and back traversal of pre-trained Artificial Neural Network (ANN). Bands selected by PCA method Bands selected by ANN method We also propose an automated technique for augmentation of training dataset with a large number of pixels from unlabelled parts of an image, based on Euclidian distance. Experiments show that bands selected by ANN achieve higher accuracy compare to PCA selected bands with automated data augmentation.","title":"Introduction"},{"location":"projects/hyperspectral_image_classification/#paper","text":"Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques @INPROCEEDINGS{8897897, author={Patel, Hetul and Bhagia, Nita and Vyas, Tarjni and Bhattacharya, Bimal and Dave, Kinjal}, booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, title={Crop Identification and Discrimination Using AVIRIS-NG Hyperspectral Data Based on Deep Learning Techniques}, year={2019}, volume={}, number={}, pages={3728-3731}, doi={10.1109/IGARSS.2019.8897897}}","title":"Paper"},{"location":"projects/incremental_learning/","text":"","title":"Incremental learning"},{"location":"projects/lecture_notes_saving_app/","text":"Fellow (Lecture notes app) \u00b6 Fellow is an Android app that I built to quickly save my lecture notes into google drive to refer them easily later. Students can quickly click a picture of daily lecture notes for each course. Notes are automatically compiled into a single pdf per course. Some of its feautres are as mentioned below. Code \u00b6 Fellow App Login with google drive account \u00b6 User can login with google account so that all the data is backed-up. Add semester wise courses \u00b6 Students can add list of courses enrolled in current semester so that notes can be easily organized and accessed quickly in pdf form by clicking on the subject name. Customise home screen \u00b6 Students can customize the homescreen by adding the weekly schedule of lectures. The home screen will then only show courses planned for the given day. User can quickly add photos of lecture notes for the given course by clicking on the upload button. Navigate easily \u00b6 Access all the features from the stylish drawer navigation.","title":"Fellow (Lecture notes app)"},{"location":"projects/lecture_notes_saving_app/#fellow-lecture-notes-app","text":"Fellow is an Android app that I built to quickly save my lecture notes into google drive to refer them easily later. Students can quickly click a picture of daily lecture notes for each course. Notes are automatically compiled into a single pdf per course. Some of its feautres are as mentioned below.","title":"Fellow (Lecture notes app)"},{"location":"projects/lecture_notes_saving_app/#code","text":"Fellow App","title":"Code"},{"location":"projects/lecture_notes_saving_app/#login-with-google-drive-account","text":"User can login with google account so that all the data is backed-up.","title":"Login with google drive account"},{"location":"projects/lecture_notes_saving_app/#add-semester-wise-courses","text":"Students can add list of courses enrolled in current semester so that notes can be easily organized and accessed quickly in pdf form by clicking on the subject name.","title":"Add semester wise courses"},{"location":"projects/lecture_notes_saving_app/#customise-home-screen","text":"Students can customize the homescreen by adding the weekly schedule of lectures. The home screen will then only show courses planned for the given day. User can quickly add photos of lecture notes for the given course by clicking on the upload button.","title":"Customise home screen"},{"location":"projects/lecture_notes_saving_app/#navigate-easily","text":"Access all the features from the stylish drawer navigation.","title":"Navigate easily"},{"location":"projects/mind_controlled_home_automation/","text":"Mind Controlled Home Automation \u00b6 Introduction \u00b6 Mind Controlled Home Automation System with self-learning capability \u2013 an approach to enter the era of virtual reality, an era where everything is possible. That\u2019s why we named it as SAMBHAV. The main aim of our project is to control home appliances with human brain acting as an interface. This means that the signals generated by our brain will now control electrical appliances \u2013 termed as Home Automation. For achieving the same, we have used MindWave headset that calculates our concentration(attention) and meditation(relaxation) and even detects the blink. The combination of these human activities will result into switching ON/OFF devices. About Headset \u00b6 A closer look at the headset : Basic functionalities of headset: \u00b6 Power Switch \u2013 To power ON/OFF the headset. If the switch is held past the ON position for 3 seconds, the headset will enter the Bluetooth Pairing Mode and if held past the ON position for 6 seconds, the headset\u2019s pairing memory will be cleared. Ear Clip \u2013 Acts as the reference point and is clipped at the earlobe (lower part of the ear). Sensor Arm \u2013 Core of the headset. It contains the sensor that should be in direct contact with the forehead. Besides sensor, it also includes the on-board chip that processes all of the data. Working \u00b6 The biosensor placed inside the headset measures the frequencies of the electrical signals emitted by the nerve cells. Although neurons are not so good conductor of electricity, yet they generate electrical signals based on the flow of ions across their plasma membranes. Basically, neurons generate a negative potential, that is measured by recording the voltage between the inside and outside of nerve cells. The sensor chip does this work with ear clip acting as the reference point and the electrical signals measured are commonly referred to as brainwaves. The sensor chip then amplifies the brainwave signals and removes the unnecessary noise and muscle movement. These analog signals are then converted into digital signals by the on-board chip and is being processed to the device. The table below shows some of the frequencies that tend to be generated by different types of activity in the brain: Brainwave Type Frequency range Mental states and conditions Delta 0.1Hz to 3Hz Deep, dreamless sleep, non-REM sleep, unconscious Theta 4Hz to 7Hz Intuitive, creative, recall, fantasy, imaginary, dream Alpha 8Hz to 12Hz Relaxed (but not drowsy) tranquil, conscious Low Beta 12Hz to 15Hz Formerly SMR, relaxed yet focused, integrated Midrange Beta 12Hz to 15Hz Thinking, aware of self & surroundings High Beta 21Hz to 30Hz Alertness, agitation Android App for analysis \u00b6 We created an android app for analyzing various signals generated by our brain on pursuing different activities. App is used to display different EEG band power values such as Delta, Theta, Alpha and Gamma etc. At the top of the screen, Graph is displayed based on this values and a sudden change in amplitude is detected on blinking. The Attention and Meditation values are also displayed. This app is connected to mindwave headset via Bluetooth. When user go beyond the meditation value of 85, Flash light of the mobile phone automatically gets turned on. Controlling Light by Mind : Circuit Diagram \u00b6 Main components of the circuit are: \u00b6 Arduino Uno R3 Bluetooth Module HC \u2013 05 4 Channel Relay Module Bulb Power Supply HC-05 Module has 3 pins in use: \u00b6 VCC \u2013 Connected to Digital Pin 5 of Arduino (5 Volt Supply) GND \u2013 Connected to Ground Pin of Arduino Tx \u2013 Transmitter Pin is connected to RX pin of Arduino Relay module has 5 pins in use: \u00b6 VCC \u2013 Connected to VCC of Arduino (5 Volt Supply) GND - Connected to Ground Pin of Arduino IN2 \u2013 As an input pin for relay 2 since we have connected bulb to relay 2. This pin is connected to Digital Pin no. 2 of Arduino. C \u2013 It is common pin of relay connected to Hot line of 250 V Ac supply. NO - Normally open pin is connected to Positive terminal of bulb. Negative terminal of Bulb is connected to Neutral line of power supply. Mind wave Head set is wirelessly connected to Arduino via Bluetooth connection. Setting up Bluetooth module \u00b6 HC-05 by default can be paired to any device. But for our project we specifically need to pair the module with mind wave headset. For that We first turned on our module in COMMAND MODE by holding a push button provided on top of HC-05. To send AT commands to HC-05, we coded the Arduino as shown below and connected the module to it. AT commands for setting up module are listed below: \u00b6 AT+NAME=SAMBHAV: Sets name of Bluetooth device to \u201cSambhav\u201d AT+UART=57600,0,0: Sets Baud rate to 57600 as mindwave sends data at this rate. AT+PSWD=0000: \u201c0000\u201d is default password for pairing up with mindwave head set AT+BIND=MMM,YY,NNNNN: We mentioned the Unique Identifier Number of Our Headset here which we get from device properties of headset. Algorithm \u00b6 Once the connection is established, mind wave sends the data in 32-byte packet. We need to retrieve the data from packet. Based on the attention and meditation value of mind we can send signal to relay for controlling light. Packet is described here. Byte Value Explanation 0 0xAA [SYNC] Synchronisation bit 1 0xAA [SYNC] Synchronisation bit 2 0x08 (payload length) of 8 bytes 3 0x02 POOR_SIGNAL Quality 4 0x20 Some poor signal detected (32/255) 5 0x01 BATTERY Level 6 0x7E Almost full 3V of battery (126/127) 7 0x04 ATTENTION 8 0x12 Attention level of 18% 9 0x05 MEDITATION 10 0x60 Meditation level of 96% 11 0xE3 [CHKSUM] (1's comp inverse of 8-bit Payload sum of 0x1C) Parsing a Packet \u00b6 Keep reading bytes from the stream until a [SYNC] byte (0xAA) is encountered. Read the next byte and ensure it is also a [SYNC] byte If not a [SYNC] byte, return to step 1. Otherwise, continue to step 3. Read the next byte from the stream as the [PLENGTH]. If [PLENGTH] is 170 ([SYNC]), then repeat step 3. If [PLENGTH] is greater than 170, then return to step 1 (PLENGTH TOO LARGE). Otherwise, continue to step 4. Read the next [PLENGTH] bytes of the [PAYLOAD\u2026] from the stream, saving them into a storage area (such as an unsigned char payload [256] array). Sum up each byte as it is read by incrementing a checksum accumulator (checksum += byte). Take the lowest 8 bits of the checksum accumulator and invert them. Here is the C code: checksum &= 0xFF; checksum = ~checksum & 0xFF; Read the next byte from the stream as the [CHKSUM] byte. If the [CHKSUM] does not match your calculated chksum (CHKSUM FAILED). Otherwise, you may now parse the contents of the Payload into DataRows to obtain the Data Values, as described below. In either case, return to step 1. Parsing Data Rows in a Packet Payload \u00b6 Repeat the following steps for parsing a DataRow until all bytes in the payload[] array ([PLENGTH] bytes) have been considered and parsed: Parse and count the number of [EXCODE] (0x55) bytes that may be at the beginning of the current DataRow. Parse the [CODE] byte for the current DataRow. If [CODE] >= 0x80, parse the next byte as the [VLENGTH] byte for the current DataRow. Parse and handle the [VALUE\u2026] byte(s) of the current DataRow, based on the DataRow's [EXCODE] level, [CODE], and [VLENGTH]. If not all bytes have been parsed from the payload[] array, return to step 1. to continue parsing the next DataRow. Get Attention Value from [EXCODE] 0x04 Get Meditation Value from [EXCODE] 0x05 If Attention is greater than threshold, then turn on light else go to step 9. If Meditation is greater than threshold, then turn off light else go to step 1. References \u00b6 Documentation and algorithm for mindwave headset","title":"Mind Controlled Home Automation"},{"location":"projects/mind_controlled_home_automation/#mind-controlled-home-automation","text":"","title":"Mind Controlled Home Automation"},{"location":"projects/mind_controlled_home_automation/#introduction","text":"Mind Controlled Home Automation System with self-learning capability \u2013 an approach to enter the era of virtual reality, an era where everything is possible. That\u2019s why we named it as SAMBHAV. The main aim of our project is to control home appliances with human brain acting as an interface. This means that the signals generated by our brain will now control electrical appliances \u2013 termed as Home Automation. For achieving the same, we have used MindWave headset that calculates our concentration(attention) and meditation(relaxation) and even detects the blink. The combination of these human activities will result into switching ON/OFF devices.","title":"Introduction"},{"location":"projects/mind_controlled_home_automation/#about-headset","text":"A closer look at the headset :","title":"About Headset"},{"location":"projects/mind_controlled_home_automation/#basic-functionalities-of-headset","text":"Power Switch \u2013 To power ON/OFF the headset. If the switch is held past the ON position for 3 seconds, the headset will enter the Bluetooth Pairing Mode and if held past the ON position for 6 seconds, the headset\u2019s pairing memory will be cleared. Ear Clip \u2013 Acts as the reference point and is clipped at the earlobe (lower part of the ear). Sensor Arm \u2013 Core of the headset. It contains the sensor that should be in direct contact with the forehead. Besides sensor, it also includes the on-board chip that processes all of the data.","title":"Basic functionalities of headset:"},{"location":"projects/mind_controlled_home_automation/#working","text":"The biosensor placed inside the headset measures the frequencies of the electrical signals emitted by the nerve cells. Although neurons are not so good conductor of electricity, yet they generate electrical signals based on the flow of ions across their plasma membranes. Basically, neurons generate a negative potential, that is measured by recording the voltage between the inside and outside of nerve cells. The sensor chip does this work with ear clip acting as the reference point and the electrical signals measured are commonly referred to as brainwaves. The sensor chip then amplifies the brainwave signals and removes the unnecessary noise and muscle movement. These analog signals are then converted into digital signals by the on-board chip and is being processed to the device. The table below shows some of the frequencies that tend to be generated by different types of activity in the brain: Brainwave Type Frequency range Mental states and conditions Delta 0.1Hz to 3Hz Deep, dreamless sleep, non-REM sleep, unconscious Theta 4Hz to 7Hz Intuitive, creative, recall, fantasy, imaginary, dream Alpha 8Hz to 12Hz Relaxed (but not drowsy) tranquil, conscious Low Beta 12Hz to 15Hz Formerly SMR, relaxed yet focused, integrated Midrange Beta 12Hz to 15Hz Thinking, aware of self & surroundings High Beta 21Hz to 30Hz Alertness, agitation","title":"Working"},{"location":"projects/mind_controlled_home_automation/#android-app-for-analysis","text":"We created an android app for analyzing various signals generated by our brain on pursuing different activities. App is used to display different EEG band power values such as Delta, Theta, Alpha and Gamma etc. At the top of the screen, Graph is displayed based on this values and a sudden change in amplitude is detected on blinking. The Attention and Meditation values are also displayed. This app is connected to mindwave headset via Bluetooth. When user go beyond the meditation value of 85, Flash light of the mobile phone automatically gets turned on.","title":"Android App for analysis"},{"location":"projects/mind_controlled_home_automation/#controlling-light-by-mind-circuit-diagram","text":"","title":"Controlling Light by Mind : Circuit Diagram"},{"location":"projects/mind_controlled_home_automation/#main-components-of-the-circuit-are","text":"Arduino Uno R3 Bluetooth Module HC \u2013 05 4 Channel Relay Module Bulb Power Supply","title":"Main components of the circuit are:"},{"location":"projects/mind_controlled_home_automation/#hc-05-module-has-3-pins-in-use","text":"VCC \u2013 Connected to Digital Pin 5 of Arduino (5 Volt Supply) GND \u2013 Connected to Ground Pin of Arduino Tx \u2013 Transmitter Pin is connected to RX pin of Arduino","title":"HC-05 Module has 3 pins in use:"},{"location":"projects/mind_controlled_home_automation/#relay-module-has-5-pins-in-use","text":"VCC \u2013 Connected to VCC of Arduino (5 Volt Supply) GND - Connected to Ground Pin of Arduino IN2 \u2013 As an input pin for relay 2 since we have connected bulb to relay 2. This pin is connected to Digital Pin no. 2 of Arduino. C \u2013 It is common pin of relay connected to Hot line of 250 V Ac supply. NO - Normally open pin is connected to Positive terminal of bulb. Negative terminal of Bulb is connected to Neutral line of power supply. Mind wave Head set is wirelessly connected to Arduino via Bluetooth connection.","title":"Relay module has 5 pins in use:"},{"location":"projects/mind_controlled_home_automation/#setting-up-bluetooth-module","text":"HC-05 by default can be paired to any device. But for our project we specifically need to pair the module with mind wave headset. For that We first turned on our module in COMMAND MODE by holding a push button provided on top of HC-05. To send AT commands to HC-05, we coded the Arduino as shown below and connected the module to it.","title":"Setting up Bluetooth module"},{"location":"projects/mind_controlled_home_automation/#at-commands-for-setting-up-module-are-listed-below","text":"AT+NAME=SAMBHAV: Sets name of Bluetooth device to \u201cSambhav\u201d AT+UART=57600,0,0: Sets Baud rate to 57600 as mindwave sends data at this rate. AT+PSWD=0000: \u201c0000\u201d is default password for pairing up with mindwave head set AT+BIND=MMM,YY,NNNNN: We mentioned the Unique Identifier Number of Our Headset here which we get from device properties of headset.","title":"AT commands for setting up module are listed below:"},{"location":"projects/mind_controlled_home_automation/#algorithm","text":"Once the connection is established, mind wave sends the data in 32-byte packet. We need to retrieve the data from packet. Based on the attention and meditation value of mind we can send signal to relay for controlling light. Packet is described here. Byte Value Explanation 0 0xAA [SYNC] Synchronisation bit 1 0xAA [SYNC] Synchronisation bit 2 0x08 (payload length) of 8 bytes 3 0x02 POOR_SIGNAL Quality 4 0x20 Some poor signal detected (32/255) 5 0x01 BATTERY Level 6 0x7E Almost full 3V of battery (126/127) 7 0x04 ATTENTION 8 0x12 Attention level of 18% 9 0x05 MEDITATION 10 0x60 Meditation level of 96% 11 0xE3 [CHKSUM] (1's comp inverse of 8-bit Payload sum of 0x1C)","title":"Algorithm"},{"location":"projects/mind_controlled_home_automation/#parsing-a-packet","text":"Keep reading bytes from the stream until a [SYNC] byte (0xAA) is encountered. Read the next byte and ensure it is also a [SYNC] byte If not a [SYNC] byte, return to step 1. Otherwise, continue to step 3. Read the next byte from the stream as the [PLENGTH]. If [PLENGTH] is 170 ([SYNC]), then repeat step 3. If [PLENGTH] is greater than 170, then return to step 1 (PLENGTH TOO LARGE). Otherwise, continue to step 4. Read the next [PLENGTH] bytes of the [PAYLOAD\u2026] from the stream, saving them into a storage area (such as an unsigned char payload [256] array). Sum up each byte as it is read by incrementing a checksum accumulator (checksum += byte). Take the lowest 8 bits of the checksum accumulator and invert them. Here is the C code: checksum &= 0xFF; checksum = ~checksum & 0xFF; Read the next byte from the stream as the [CHKSUM] byte. If the [CHKSUM] does not match your calculated chksum (CHKSUM FAILED). Otherwise, you may now parse the contents of the Payload into DataRows to obtain the Data Values, as described below. In either case, return to step 1.","title":"Parsing a Packet"},{"location":"projects/mind_controlled_home_automation/#parsing-data-rows-in-a-packet-payload","text":"Repeat the following steps for parsing a DataRow until all bytes in the payload[] array ([PLENGTH] bytes) have been considered and parsed: Parse and count the number of [EXCODE] (0x55) bytes that may be at the beginning of the current DataRow. Parse the [CODE] byte for the current DataRow. If [CODE] >= 0x80, parse the next byte as the [VLENGTH] byte for the current DataRow. Parse and handle the [VALUE\u2026] byte(s) of the current DataRow, based on the DataRow's [EXCODE] level, [CODE], and [VLENGTH]. If not all bytes have been parsed from the payload[] array, return to step 1. to continue parsing the next DataRow. Get Attention Value from [EXCODE] 0x04 Get Meditation Value from [EXCODE] 0x05 If Attention is greater than threshold, then turn on light else go to step 9. If Meditation is greater than threshold, then turn off light else go to step 1.","title":"Parsing Data Rows in a Packet Payload"},{"location":"projects/mind_controlled_home_automation/#references","text":"Documentation and algorithm for mindwave headset","title":"References"},{"location":"projects/mobile_app_for_visually_impaired/","text":"","title":"Mobile app for visually impaired"},{"location":"projects/on_device_classification/","text":"","title":"On device classification"},{"location":"projects/small_face_detection/","text":"","title":"Small face detection"},{"location":"projects/spry/","text":"Hospital Management System \u00b6 Spry was a hobby project I created for my family doctor. It provides some helpful features for the doctors to manage their patients' data through this web app. Responsive Dashboard \u00b6 List of features \u00b6 1. Add symptoms and treatments \u00b6 2. Track patient's blood pressure history. \u00b6 3. Track paid and unpaid amount. \u00b6 4. Create unique patient profile \u00b6 5. Check patient's medical history. \u00b6 6. Search patients by multiple fields. \u00b6 7. Real time sync with cloud \u00b6","title":"Hospital Management System"},{"location":"projects/spry/#hospital-management-system","text":"Spry was a hobby project I created for my family doctor. It provides some helpful features for the doctors to manage their patients' data through this web app.","title":"Hospital Management System"},{"location":"projects/spry/#responsive-dashboard","text":"","title":"Responsive Dashboard"},{"location":"projects/spry/#list-of-features","text":"","title":"List of features"},{"location":"projects/spry/#1-add-symptoms-and-treatments","text":"","title":"1. Add symptoms and treatments"},{"location":"projects/spry/#2-track-patients-blood-pressure-history","text":"","title":"2. Track patient's blood pressure history."},{"location":"projects/spry/#3-track-paid-and-unpaid-amount","text":"","title":"3. Track paid and unpaid amount."},{"location":"projects/spry/#4-create-unique-patient-profile","text":"","title":"4. Create unique patient profile"},{"location":"projects/spry/#5-check-patients-medical-history","text":"","title":"5. Check patient's medical history."},{"location":"projects/spry/#6-search-patients-by-multiple-fields","text":"","title":"6. Search patients by multiple fields."},{"location":"projects/spry/#7-real-time-sync-with-cloud","text":"","title":"7. Real time sync with cloud"}]}